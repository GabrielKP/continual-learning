{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnant-urt22Y"
   },
   "source": [
    "# AGL Autoencoder\n",
    "\n",
    "Based on https://github.com/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo collection\n",
    "\n",
    "DynaMoE:\n",
    "* Exploring vs exploiting, decreasing over time\n",
    "* New expert only learns from examples it was actually chosen for\n",
    "* consolidation of dynamoe and training gating code into one codeblock\n",
    "* visualize in graph switches of DynaMoE status\n",
    "\n",
    "General:\n",
    "- EWC\n",
    "- Replay\n",
    "- Run on 3 tasks\n",
    "- Test interleaved training: A -> C -> A, only test on A - what is performance of A?\n",
    "- Let's use a CNN and pictures for the input!\n",
    "\n",
    "Interleaved training:\n",
    "- set up so network can switch within single dataloader\n",
    "- optimize for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xKTgS0wMOZbb"
   },
   "outputs": [],
   "source": [
    "BASIS = \"../\" #\"drive/MyDrive/Colab_data/\"\n",
    "MODELFOLDER = BASIS + \"/models/\"\n",
    "PLOTSFOLDER = BASIS + \"/plots/\"\n",
    "MODELAUTOSAVE = MODELFOLDER + \"/autosave/\"\n",
    "PLOTSAUTOSAVE = PLOTSFOLDER + \"/autosave/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8BeTXi1OaMs",
    "outputId": "ca356d96-806e-46bb-9036-0f3c2d8c8139"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0NKkMOK5QRVp"
   },
   "outputs": [],
   "source": [
    "#!cp drive/MyDrive/Colab_data/data.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yjgT9Azct22a"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvhH1uQsvnV7"
   },
   "source": [
    "## Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HV39YqA5vnV8"
   },
   "outputs": [],
   "source": [
    "class GrammarGen():\n",
    "    \"\"\"\n",
    "    Generates Grammar sequences from grammars, and offers other functionalities\n",
    "    Grammars are dictionaries:\n",
    "    - always have START\n",
    "    - all paths lead eventually to END\n",
    "    - Entries starting with the same letter\n",
    "      have same output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grammar=None):\n",
    "        if grammar is None:\n",
    "            self.grammar = data.g0()\n",
    "        else:\n",
    "            self.grammar = grammar\n",
    "\n",
    "        # find how many letters in grammar\n",
    "        self.len = len(set([token[0] for token in self.grammar if (token != 'START' and token != 'END')]))\n",
    "\n",
    "        # variable to check how many sequences have been generated for the grammaticality test\n",
    "        self.grammCheckMaxLen = -1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def generate(self, n):\n",
    "        \"\"\"Generates n tokens\"\"\"\n",
    "        ret = []\n",
    "        count = 0\n",
    "        hashtrack = set()\n",
    "        while count < n:\n",
    "            token = []\n",
    "            current = 'START'\n",
    "            while current != 'END':\n",
    "                # Append current\n",
    "                if current != 'START':\n",
    "                    token.append(current[0])\n",
    "                # Choose next\n",
    "                r = random.randint(0, len(self.grammar[current]) - 1)\n",
    "                current = self.grammar[current][r]\n",
    "            # Check if seq is already inside\n",
    "            tokenhash = ''.join([str(x) for x in token])\n",
    "            if tokenhash not in hashtrack:\n",
    "                hashtrack.add(tokenhash)\n",
    "                ret.append((token, ))\n",
    "                count += 1\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def generateAllGrammatical(self, maxlen=float('inf')):\n",
    "        \"\"\"Generates all grammatical sequences until length maxlen\"\"\"\n",
    "        def genAllHelp(seq, current):\n",
    "            if current == 'END':\n",
    "                return [seq]\n",
    "            if len(seq) >= maxlen:\n",
    "                return []\n",
    "            # Append Current\n",
    "            if current != 'START':\n",
    "                seq.append(current[0])\n",
    "            # Generate next possibilities\n",
    "            options = range(len(self.grammar[current]))\n",
    "            ret = [(genAllHelp(copy.copy(seq), self.grammar[current][i]))\n",
    "                   for i in options]\n",
    "            return itertools.chain(*ret)\n",
    "        return set([tuple(seq) for seq in genAllHelp([], 'START')])\n",
    "\n",
    "    def isGrammatical(self, seqs):\n",
    "        \"\"\"Check for grammaticality of sequences in seqs\"\"\"\n",
    "        maxlen = max([len(seq) for seq in seqs])\n",
    "        if self.grammCheckMaxLen < maxlen:\n",
    "            self.allGrammatical = self.generateAllGrammatical(maxlen)\n",
    "            self.grammCheckMaxLen = maxlen\n",
    "\n",
    "        return [tuple(seq) in self.allGrammatical for seq in seqs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKt9wtL6t22s"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Deyfd3OByLSC"
   },
   "source": [
    "\n",
    "\n",
    "### Cosine Loss, Init_weights, count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "6X9xZXFQvnWF"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class CosineLoss():\n",
    "    def __init__(self, vocabsize, ignore_index, reduction=\"mean\"):\n",
    "        self.vocabsize = vocabsize\n",
    "        self.ignore_index = ignore_index\n",
    "        self.eye = torch.eye(self.vocabsize).to(device)\n",
    "        self.cosSim = nn.CosineSimilarity(dim=1)\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def __call__(self, outputs, labels):\n",
    "        maxlen = outputs.shape[0]\n",
    "        bs = outputs.shape[1]\n",
    "\n",
    "        # Deal with positions which should be ignored and make them the same as label\n",
    "        if self.ignore_index is not None:\n",
    "            ignore_positions = (labels == self.ignore_index).to(device)\n",
    "            outputs[ignore_positions] = self.eye[self.ignore_index]\n",
    "\n",
    "        # Convert labels to onehot\n",
    "        labels_onehot = torch.empty((maxlen, bs, self.vocabsize)).to(device)\n",
    "        for idx in range(maxlen):\n",
    "            labels_onehot[idx,:,:] = self.eye[labels[idx,:]]\n",
    "\n",
    "        # Put labels and output in correct form for torch cosSim function\n",
    "        batch_first_labels = labels_onehot.permute(1,0,2)\n",
    "        processed_labels = batch_first_labels.reshape(-1, maxlen * self.vocabsize)\n",
    "        batch_first_outputs = outputs.permute(1,0,2)\n",
    "        processed_outputs = batch_first_outputs.reshape(-1, maxlen * self.vocabsize)\n",
    "\n",
    "        # use cosSim function\n",
    "        # Cos: 1 same, (angle of 0 degrees), -1 different (180 degrees)\n",
    "        res = (1 - self.cosSim(processed_labels, processed_outputs))\n",
    "\n",
    "        # Same interface as native los functions: reductions for the output\n",
    "        if self.reduction == \"none\":\n",
    "            return res\n",
    "        elif self.reduction == \"mean\":\n",
    "            return res.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return res.sum()\n",
    "        else:\n",
    "            print(\"error\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3d2C9Rvt22w"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Fut2CtQrt22w"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src, src_len = batch\n",
    "        trg = src\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, src_len, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        if isinstance(criterion, CosineLoss):\n",
    "            loss = criterion(output, trg)\n",
    "        else:\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VIT6a8uqt22w"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for seq, seq_len in dataloader:\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0) #turn off teacher forcing\n",
    "            \n",
    "            #seq = [seq_len, batch_size]\n",
    "            #output = [seq_len, batch_size, output_dim]\n",
    "\n",
    "            if isinstance(criterion, CosineLoss):\n",
    "                loss = criterion(output, seq)\n",
    "            else:\n",
    "                output_dim = output.shape[-1]\n",
    "                \n",
    "                output = output[1:].view(-1, output_dim)\n",
    "                trg = seq[1:].view(-1)\n",
    "                \n",
    "                #trg = [(trg len - 1) * batch size]\n",
    "                #output = [(trg len - 1) * batch size, output dim]\n",
    "                \n",
    "                loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uOtABRKLvnWI"
   },
   "outputs": [],
   "source": [
    "def evaluate_extra(model, dataloader, loss_func):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for seqs, seqs_len in dataloader:\n",
    "\n",
    "            outputs = model(seqs, seqs_len, seqs, 0)\n",
    "\n",
    "            loss = loss_func(outputs, seqs) / seqs.shape[1]\n",
    "\n",
    "            loss_total += loss.item()\n",
    "        \n",
    "#        if loss_func == allOrNoneLoss:\n",
    "#            return loss_total\n",
    "\n",
    "        return loss_total / len(dataloader)\n",
    "\n",
    "def cutEndToken(seq):\n",
    "    ret = []\n",
    "    for stim in seq:\n",
    "        if stim == END_TOKEN:\n",
    "            break\n",
    "        ret.append(stim)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def allOrNoneLoss(output, trg):\n",
    "    bs = output.shape[1]\n",
    "    ret = 0\n",
    "    pred = output.argmax(-1)[1:]\n",
    "    trg = trg[1:]\n",
    "    for b in range(bs):\n",
    "        p = cutEndToken(pred[:,b].tolist())\n",
    "        t = cutEndToken(trg[:,b].tolist())\n",
    "        ret += not p == t\n",
    "    return torch.tensor(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lW3r-pjXt22x"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUY7o5eGt22x"
   },
   "source": [
    "During Training in addition to collecting the train/validation loss, collect the amount of entirely correct predicted sequences on the train and test gr/ugr set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval_all_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all_tasks(model, total_loss_array, total_hits_array, n_tasks_total, idx, criterion):\n",
    "    for other_id in range(n_tasks_total):\n",
    "        total_loss_array[other_id,0,idx] = evaluate(model,\n",
    "                                                    train_dls[other_id],\n",
    "                                                    criterion)\n",
    "        total_loss_array[other_id,1,idx] = evaluate(model,\n",
    "                                                    test_dls[other_id],\n",
    "                                                    criterion)\n",
    "        total_loss_array[other_id,2,idx] = evaluate(model,\n",
    "                                                    test_ugr_dls[other_id],\n",
    "                                                    criterion)\n",
    "        total_hits_array[other_id,0,idx] = evaluate_extra(model,\n",
    "                                                          train_dls[other_id],\n",
    "                                                          allOrNoneLoss)\n",
    "        total_hits_array[other_id,1,idx] = evaluate_extra(model,\n",
    "                                                          test_dls[other_id],\n",
    "                                                          allOrNoneLoss)\n",
    "        total_hits_array[other_id,2,idx] = evaluate_extra(model,\n",
    "                                                          test_ugr_dls[other_id],\n",
    "                                                          allOrNoneLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JAVqiZ3kvnWJ"
   },
   "outputs": [],
   "source": [
    "def fit(\n",
    "    n_tasks_total,\n",
    "    model,\n",
    "    task_id,\n",
    "    n_task_epochs,\n",
    "    step_size_evaluation,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    clip=1,\n",
    "    repetition=None\n",
    "):\n",
    "        \n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((n_tasks_total, 3, n_task_epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((n_tasks_total, 3, n_task_epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    for epoch in range(n_task_epochs):\n",
    "        # First Epoch log performance BEFORE training\n",
    "        if epoch == 0:\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, 0, criterion)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train(model, train_dls[task_id], optimizer, criterion, clip)\n",
    "        valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        #if valid_loss < best_valid_loss:\n",
    "        #    best_valid_loss = valid_loss\n",
    "        #    torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        # Log performance AFTER training\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0 and epoch != 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, idx, criterion)\n",
    "\n",
    "        \n",
    "        if repetition is not None:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | R{repetition} T{task_id}')\n",
    "        else:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | T{task_id}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbtZwmf8egUu"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9S4PXbSe06C",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "P57X-q51t22n"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "                \n",
    "        #need to explicitly put lengths on cpu!\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'))\n",
    "                \n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "                                 \n",
    "        #packed_outputs is a packed sequence containing all hidden states\n",
    "        #hidden is now from the final non-padded element in the batch\n",
    "            \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
    "            \n",
    "        #outputs is now a non-packed sequence, all hidden states obtained\n",
    "        #  when the input is a pad token are all zeros\n",
    "            \n",
    "        #outputs = [src len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "le1kHQOIt22o"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "P_t5icjNt22o"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "  \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        #attention = [batch size, src len]\n",
    "        \n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v371q1Tkt22q"
   },
   "source": [
    "### Decoder\n",
    "\n",
    "The decoder only needs a few small changes. It needs to accept a mask over the source sentence and pass this to the attention module. As we want to view the values of attention during inference, we also return the attention tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ECjR4jZot22q"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #mask = [batch size, src len]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "                \n",
    "        #a = [batch size, src len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        # assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toSHWJmEt22q"
   },
   "source": [
    "### Seq2Seq\n",
    "\n",
    "The overarching seq2seq model also needs a few changes for packed padded sequences, masking and inference. \n",
    "\n",
    "We need to tell it what the indexes are for the pad token and also pass the source sentence lengths as input to the `forward` method.\n",
    "\n",
    "We use the pad token index to create the masks, by creating a mask tensor that is 1 wherever the source sentence is not equal to the pad token. This is all done within the `create_mask` function.\n",
    "\n",
    "The sequence lengths as needed to pass to the encoder to use packed padded sequences.\n",
    "\n",
    "The attention at each time-step is stored in the `attentions` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OkcTBfr-t22s"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "                    \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        mask = self.create_mask(src)\n",
    "\n",
    "        #mask = [batch size, src len]\n",
    "                \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
    "            #  and mask\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        # outputs=[trg_len, batch_size, seq_len]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLVj3zdDt22d"
   },
   "source": [
    "## Data\n",
    "\n",
    "First, get the training and test sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TH6UqaslvnV_"
   },
   "source": [
    "### Sequencedataset\n",
    "Define a Dataset for Sequences:\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "rBmNkT78t22d"
   },
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seqs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (int): amount of sequences generated\n",
    "        \"\"\"\n",
    "        self.seqs = seqs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5x7ocJD0vnWB"
   },
   "source": [
    "### collate_batch\n",
    "Define collate_batch for the Dataloader: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "\n",
    "Sequences are padded and their non-padded lengths are returned.\n",
    "Since pack_padded_sequences requires sequences to be sorted, they are sorted too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "tOBuBfPPt22e"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    seq_lens = []\n",
    "    processed_seqs = []\n",
    "    # Sort in descending order\n",
    "    batch.sort(reverse=True, key=(lambda x: len(x)))\n",
    "    # append start and end token\n",
    "    for seq in batch:\n",
    "        seq = [START_TOKEN] + seq + [END_TOKEN]\n",
    "        seq_lens.append(len(seq))\n",
    "        processed_seqs.append(torch.tensor(seq))\n",
    "    # pad\n",
    "    padded_seqs = pad_sequence(processed_seqs)\n",
    "    seq_lens = torch.tensor(seq_lens)\n",
    "    return padded_seqs.to(device), seq_lens.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiskh9dalsvW"
   },
   "source": [
    "### Data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4UgG2QIFlsg3"
   },
   "outputs": [],
   "source": [
    "N_TASKS = 2\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "PAD_TOKEN = 0\n",
    "START_TOKEN = 1\n",
    "END_TOKEN = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2cXtZIVe5_q"
   },
   "source": [
    "### Task loading\n",
    "For reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "HREfj6IuvnWJ"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHhSjYo6vnWK"
   },
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "jWwIlfPcvnWL"
   },
   "outputs": [],
   "source": [
    "train_seqs = data.g0_train()\n",
    "valid_seqs = data.g0_train()\n",
    "test_seqs = data.g0_test_gr()\n",
    "test_ugr_seqs = data.g0_test_ugr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5I5lDz8vnWL"
   },
   "source": [
    "Sort for better perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ocjMQF-RvnWL"
   },
   "outputs": [],
   "source": [
    "train_seqs.sort(key=(lambda x: len(x)))\n",
    "valid_seqs.sort(key=(lambda x: len(x)))\n",
    "test_seqs.sort(key=(lambda x: len(x)))\n",
    "test_ugr_seqs.sort(key=(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "zMSM652MvnWL"
   },
   "outputs": [],
   "source": [
    "def buildVocab(letterset):\n",
    "    vocab = {'<pad>': PAD_TOKEN, '<sos>': START_TOKEN, '<eos>': END_TOKEN}\n",
    "    counter = len(vocab)\n",
    "    for letter in letterset:\n",
    "        vocab[letter] = counter\n",
    "        counter +=1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "yBCWnEM1vnWM"
   },
   "outputs": [],
   "source": [
    "letters = set()\n",
    "for seq in train_seqs:\n",
    "    [letters.add(letter) for letter in seq]\n",
    "letters = list(letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPjDWlVPvnWM"
   },
   "source": [
    "Make all tasks, creates an additional task with all sequences from all tasks mashed together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "YtuaqeKNvnWM"
   },
   "outputs": [],
   "source": [
    "train_dls = []\n",
    "valid_dls = []\n",
    "test_dls = []\n",
    "test_ugr_dls = []\n",
    "vocabs = []\n",
    "rvocabs = []\n",
    "let2idxs = []\n",
    "idx2lets = []\n",
    "\n",
    "usedpermutations = set()\n",
    "train_convs = []\n",
    "valid_convs = []\n",
    "test_convs = []\n",
    "test_ugr_convs = []\n",
    "for t in range(N_TASKS + 1):\n",
    "    # Create normal tasks\n",
    "    if t != N_TASKS:\n",
    "        temp_letters = copy.copy(letters)\n",
    "\n",
    "        while str(temp_letters) in usedpermutations:\n",
    "            random.shuffle(temp_letters)\n",
    " \n",
    "        usedpermutations.add(str(temp_letters))\n",
    "\n",
    "        # Vocab\n",
    "        vocabs.append(buildVocab(temp_letters))\n",
    "        rvocabs.append({v: k for k, v in vocabs[-1].items()})\n",
    "\n",
    "        # Conversion Functions\n",
    "        let2idxs.append(lambda seq: [vocabs[-1][let] for let in seq])\n",
    "        idx2lets.append(lambda seq: [rvocabs[-1][let] for let in seq])\n",
    "\n",
    "        # Convert to indices\n",
    "        train_conv = [let2idxs[-1](seq) for seq in train_seqs]\n",
    "        valid_conv = [let2idxs[-1](seq) for seq in valid_seqs]\n",
    "        test_conv = [let2idxs[-1](seq) for seq in test_seqs]\n",
    "        test_ugr_conv = [let2idxs[-1](seq) for seq in test_ugr_seqs]\n",
    "\n",
    "        # Add conv seq to sequence collection\n",
    "        train_convs.extend(train_conv)\n",
    "        valid_convs.extend(valid_conv)\n",
    "        test_convs.extend(test_conv)\n",
    "        test_ugr_convs.extend(test_ugr_conv)\n",
    "\n",
    "    # Create joint task\n",
    "    else:\n",
    "        train_conv = train_convs\n",
    "        valid_conv = valid_convs\n",
    "        test_conv = test_convs\n",
    "        test_ugr_conv = test_ugr_convs\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = SequenceDataset(train_conv)\n",
    "    valid_ds = SequenceDataset(valid_conv)\n",
    "    test_ds = SequenceDataset(test_conv)\n",
    "    test_ugr_ds = SequenceDataset(test_ugr_conv)\n",
    "    \n",
    "    # Dataloader\n",
    "    train_dls.append(\n",
    "        DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=True, collate_fn=collate_batch))\n",
    "    valid_dls.append(\n",
    "        DataLoader(valid_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_dls.append(\n",
    "        DataLoader(test_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_ugr_dls.append(\n",
    "        DataLoader(test_ugr_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-R9Nw9XB8Iw"
   },
   "source": [
    "### Task loading v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "E2Sn2FU9CAlD"
   },
   "outputs": [],
   "source": [
    "SWAP1 = 2\n",
    "SWAP2 = 4\n",
    "train_dls = []\n",
    "valid_dls = []\n",
    "test_dls = []\n",
    "test_ugr_dls = []\n",
    "vocabs = []\n",
    "rvocabs = []\n",
    "let2idx = lambda task_id, seq: [vocabs[task_id][let] for let in seq]\n",
    "idx2let = lambda task_id, seq: [rvocabs[task_id][let] for let in seq]\n",
    "\n",
    "usedpermutations = set()\n",
    "listUsedPermutations = []\n",
    "train_convs = []\n",
    "valid_convs = []\n",
    "test_convs = []\n",
    "test_ugr_convs = []\n",
    "alternate = 0\n",
    "for t in range(N_TASKS + 1):\n",
    "    # Create normal tasks\n",
    "    if t != N_TASKS:\n",
    "        if alternate == 0:\n",
    "            temp_letters = copy.copy(letters)\n",
    "\n",
    "            while str(temp_letters) in usedpermutations:\n",
    "                random.shuffle(temp_letters)\n",
    "    \n",
    "            usedpermutations.add(str(temp_letters))\n",
    "            listUsedPermutations.append(temp_letters)\n",
    "\n",
    "            alternate = 1\n",
    "        else:\n",
    "            temp_letters = listUsedPermutations[-1]\n",
    "\n",
    "            #Swap 5th and 6th indice\n",
    "            temp_letters[SWAP1], temp_letters[SWAP2] = temp_letters[SWAP2], temp_letters[SWAP1]\n",
    "\n",
    "            usedpermutations.add(str(temp_letters))\n",
    "\n",
    "            alternate = 0\n",
    "\n",
    "        # Vocab\n",
    "        vocabs.append(buildVocab(temp_letters))\n",
    "        rvocabs.append({v: k for k, v in vocabs[-1].items()})\n",
    "\n",
    "        # Convert to indices\n",
    "        train_conv = [let2idx(t, seq) for seq in train_seqs]\n",
    "        valid_conv = [let2idx(t, seq) for seq in valid_seqs]\n",
    "        test_conv = [let2idx(t, seq) for seq in test_seqs]\n",
    "        test_ugr_conv = [let2idx(t, seq) for seq in test_ugr_seqs]\n",
    "\n",
    "        # Add conv seq to sequence collection\n",
    "        train_convs.extend(train_conv)\n",
    "        valid_convs.extend(valid_conv)\n",
    "        test_convs.extend(test_conv)\n",
    "        test_ugr_convs.extend(test_ugr_conv)\n",
    "\n",
    "    # Create joint task\n",
    "    else:\n",
    "        train_conv = train_convs\n",
    "        valid_conv = valid_convs\n",
    "        test_conv = test_convs\n",
    "        test_ugr_conv = test_ugr_convs\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = SequenceDataset(train_conv)\n",
    "    valid_ds = SequenceDataset(valid_conv)\n",
    "    test_ds = SequenceDataset(test_conv)\n",
    "    test_ugr_ds = SequenceDataset(test_ugr_conv)\n",
    "    \n",
    "    # Dataloader\n",
    "    train_dls.append(\n",
    "        DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=True, collate_fn=collate_batch))\n",
    "    valid_dls.append(\n",
    "        DataLoader(valid_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_dls.append(\n",
    "        DataLoader(test_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_ugr_dls.append(\n",
    "        DataLoader(test_ugr_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVaRvD2svnWN",
    "outputId": "ddaa8495-e224-4457-af03-78c62225b990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, 'F': 3, 'C': 4, 'D': 5, 'G': 6, 'A': 7}\n",
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, 'F': 3, 'C': 4, 'A': 5, 'G': 6, 'D': 7}\n",
      "\n",
      "First Batch of Task 0:\n",
      "tensor([[1],\n",
      "        [7],\n",
      "        [4],\n",
      "        [3],\n",
      "        [2]])\n",
      "\n",
      "First Batch of Task 1:\n",
      "tensor([[1],\n",
      "        [5],\n",
      "        [4],\n",
      "        [3],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "print(vocabs[0])\n",
    "print(vocabs[1])\n",
    "for i in range(len(valid_dls) - 1):\n",
    "    for seqs, _ in valid_dls[i]:\n",
    "        print(f\"\\nFirst Batch of Task {i}:\")\n",
    "        print(seqs)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFM4eGVOs2lZ",
    "outputId": "ef66cff7-2e19-4a9e-8b0e-9fb932381c00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(task_id, seq)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzTfiG5qPpgS",
    "outputId": "be70246f-611c-4bd8-98e7-d46f7b1dceca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 4, 3] - [5, 4, 3]\n",
      "[7, 4, 6, 3] - [5, 4, 6, 3]\n",
      "[7, 5, 4, 3] - [5, 7, 4, 3]\n",
      "[7, 4, 3, 4, 6] - [5, 4, 3, 4, 6]\n",
      "[7, 5, 4, 3, 4] - [5, 7, 4, 3, 4]\n",
      "[7, 4, 6, 3, 4, 6] - [5, 4, 6, 3, 4, 6]\n",
      "[7, 5, 4, 3, 4, 6] - [5, 7, 4, 3, 4, 6]\n",
      "[7, 5, 4, 6, 3, 4, 6] - [5, 7, 4, 6, 3, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "convTrainSeqs = lambda task_id: [let2idx(task_id, seq) for seq in train_seqs]\n",
    "convTestSeqs = lambda task_id: [let2idx(task_id, seq) for seq in test_ugr_seqs]\n",
    "tr1 = convTrainSeqs(0)\n",
    "tr2 = convTrainSeqs(1)\n",
    "te1 = convTestSeqs(0)\n",
    "te2 = convTestSeqs(1)\n",
    "\n",
    "for seq in tr1:\n",
    "    if seq in tr2:\n",
    "        print(se)\n",
    "        print(\"hi\")\n",
    "\n",
    "for se in tr2:\n",
    "    if se in te1:\n",
    "        print(se)\n",
    "        print(\"hi\")\n",
    "\n",
    "\n",
    "for i in range(len(tr1)):\n",
    "    print(f\"{tr1[i]} - {tr2[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpwRQ7xLvnWR"
   },
   "source": [
    "## Plotting & Evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhO-beIdl9SC"
   },
   "source": [
    "### plotTranser, plotResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "y6vb8He6vnWR"
   },
   "outputs": [],
   "source": [
    "def plotTransfer(data, title):\n",
    "    # data = [n_methods, n_tasks, combinedepochs]\n",
    "    n_methods, n_tasks, n_combinedepochs = data.shape\n",
    "    fig, axs = plt.subplots(n_tasks, 1)\n",
    "    colors = ['blue','green','orange','red','yellow','violett']\n",
    "    \n",
    "    xvals = range(0, n_combinedepochs * STEP_SIZE_EVALUATION, STEP_SIZE_EVALUATION)\n",
    "    \n",
    "    for task_idx in range(n_tasks):\n",
    "        for method_idx in range(n_methods):\n",
    "            axs[task_idx].plot(\n",
    "                xvals,\n",
    "                data[method_idx, task_idx],\n",
    "                color=colors[method_idx]\n",
    "            )\n",
    "            axs[task_idx].set_ylim(0,1.1)\n",
    "            if task_idx != n_tasks - 1:\n",
    "                axs[task_idx].tick_params(\n",
    "                    axis='x',\n",
    "                    which='both',\n",
    "                    labelbottom=False\n",
    "                )\n",
    "        x_lines = range(0, n_combinedepochs * STEP_SIZE_EVALUATION, N_EPOCHS)\n",
    "        for xpos in x_lines:\n",
    "            axs[task_idx].axvline(xpos, color=\"grey\")\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "def plotResults(hist_loss, hist_hits, plotLoss=False):\n",
    "    if torch.is_tensor(hist_loss):\n",
    "        hist_loss = hist_loss.numpy()\n",
    "    if torch.is_tensor(hist_hits):\n",
    "        hist_hits = hist_hits.numpy()\n",
    "    if plotLoss:\n",
    "        plotTransfer( np.expand_dims(hist_loss[:,0,:], 0), \"Train Loss\")\n",
    "        plotTransfer( np.expand_dims(hist_loss[:,1,:], 0), \"Test Gr Loss\")\n",
    "        plotTransfer( np.expand_dims(hist_loss[:,2,:], 0), \"Test Ugr Loss\")\n",
    "    plotTransfer( np.expand_dims(hist_hits[:,0,:], 0), \"Train Hits\")\n",
    "    plotTransfer( np.expand_dims(hist_hits[:,1,:], 0), \"Test Gr Hits\")\n",
    "    plotTransfer( np.expand_dims(hist_hits[:,2,:], 0), \"Test Ugr Hits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotAverages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAverages(hist_all, schedule, step_size_evaluation, datasets=(0,1,2), figsize=(15,9), title=\"\", save=False, path=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets: tuple (int)\n",
    "        0 for train, 1 for test gr, 2 for test ugr\n",
    "    title: string\n",
    "        plot title\n",
    "    \"\"\"\n",
    "    # Handle case where only one dataset is displayed\n",
    "    if isinstance(datasets, int):\n",
    "        hist_all = np.expand_dims(hist_all[:,:,datasets], axis=-2)\n",
    "    elif len(datasets) == 1:\n",
    "        hist_all = np.expand_dims(hist_all[:,:,datasets[0]], axis=-2)\n",
    "    else:\n",
    "        hist_all = hist_all[:,:,datasets]\n",
    "    datasets = np.array(datasets)\n",
    "\n",
    "    # Consistent plot colors for train test gr and ugr\n",
    "    colors = np.array(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "    chosen_colors = colors[datasets]\n",
    "    \n",
    "    # hist_all = [repetitions, n_trainings, n_datasets, n_evalsteps]\n",
    "    n_repetitions, n_trainings, n_datasets, n_evalsteps_total = hist_all.shape\n",
    "    \n",
    "    # Invert Data\n",
    "    hist_all = 1 - hist_all\n",
    "    # averages\n",
    "    hist_all_avg = np.average(hist_all, axis=0)\n",
    "    #hist_all_avg_concat = np.concatenate(hist_all_avg, axis=2)\n",
    "    # standard derivations\n",
    "    hist_all_std = np.std(hist_all, axis=0)\n",
    "    #hist_all_std_concat = np.concatenate(hist_all_std, axis=2)\n",
    "    hist_all_min = np.fmax(hist_all_avg - hist_all_std, 0)\n",
    "    hist_all_max = np.fmin(hist_all_avg + hist_all_std, 1)\n",
    "    \n",
    "    n_tasks = len(set([x[0] for x in schedule]))\n",
    "    n_totalepochs = sum([x[1] for x in schedule])\n",
    "    list_epochs_passed = [0] + list(np.cumsum([x[1] for x in schedule]))\n",
    "    list_evalsteps_passed = [0] + list(np.cumsum([x[1]//step_size_evaluation for x in schedule]))\n",
    "    \n",
    "    fig, axs = plt.subplots(n_tasks, 1, figsize=figsize)\n",
    "    \n",
    "    for dataset_id in range(n_datasets):\n",
    "        for task_id, n_epochs in schedule:\n",
    "            # Right y limits\n",
    "            axs[task_id].set_ylim(-0.05, 1.05)\n",
    "            \n",
    "            # Hide parts of the graph without training\n",
    "            xvals = range(list_epochs_passed[task_id], n_totalepochs, step_size_evaluation)\n",
    "\n",
    "            # Average\n",
    "            axs[task_id].plot(xvals,\n",
    "                              hist_all_avg[task_id,dataset_id,list_evalsteps_passed[task_id]:],\n",
    "                              color=chosen_colors[dataset_id])\n",
    "            # Standard deviation\n",
    "            axs[task_id].fill_between(xvals,\n",
    "                                      hist_all_min[task_id,dataset_id,list_evalsteps_passed[task_id]:],\n",
    "                                      hist_all_max[task_id,dataset_id,list_evalsteps_passed[task_id]:],\n",
    "                                      color=chosen_colors[dataset_id],\n",
    "                                      alpha=0.2)\n",
    "\n",
    "            # prevent ticks in upper subplots\n",
    "            if task_id != n_tasks - 1:\n",
    "                axs[task_id].tick_params(axis=\"x\", \n",
    "                                         which=\"both\",\n",
    "                                         labelbottom=False)\n",
    "            # Epoch label\n",
    "            else:\n",
    "                axs[task_id].set_xlabel(\"Epochs\")\n",
    "\n",
    "            # Task Label\n",
    "            if task_id == 0:\n",
    "                axs[task_id].set_ylabel(f\"Replication percentage average N={n_repetitions}\")\n",
    "\n",
    "            # Vertical lines\n",
    "            if dataset_id == n_datasets - 1:\n",
    "                axs[task_id].vlines(list_epochs_passed, 0, 1,\n",
    "                                    transform=axs[task_id].get_xaxis_transform(),\n",
    "                                    colors=\"grey\")\n",
    "            \n",
    "    legends = np.array([\"Train\", \"Test Grammatical\", \"Test Ungrammatical\"])\n",
    "    plt.legend(legends[datasets])\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(path, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotSpecificTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSpecificTask(hist_all, train_id, task_id, figsize=(13,7), title=\"\", save=False):\n",
    "    # hist_all = [repetitions, n_train_runs, n_tasks, n_datasets, n_evalsteps]\n",
    "    n_repetitions, _, _, n_datasets, n_evalsteps = hist_all.shape\n",
    "    # Invert data\n",
    "    hist_all = 1 - hist_all\n",
    "    titles = (\"Train\", \"Test Grammatical\", \"Test Ungrammatical\")\n",
    "    # average\n",
    "    hist_all_avg = np.average(hist_all, axis=0)\n",
    "    hist_specific = hist_all_avg[train_id, task_id]\n",
    "    \n",
    "    # standard deviation\n",
    "    hist_all_std = np.std(hist_all, axis=0)\n",
    "    hist_std = hist_all_std[train_id, task_id]\n",
    "    hist_max = np.fmin(hist_specific + hist_std, 1)\n",
    "    hist_min = np.fmax(hist_specific - hist_std, 0)\n",
    "    \n",
    "    xvals = range(0, n_evalsteps * STEP_SIZE_EVALUATION, STEP_SIZE_EVALUATION)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for dataset_id in range(n_datasets):\n",
    "        plt.plot(xvals, hist_specific[dataset_id])\n",
    "        plt.fill_between(xvals, hist_min[dataset_id], hist_max[dataset_id], alpha=0.2)\n",
    "    \n",
    "    plt.ylim(-0.05,1.05)\n",
    "    plt.legend(titles)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(f\"Replication percentage average N={n_repetitions}\")\n",
    "    plt.xlabel(\"Epochs Trained\")\n",
    "    if save:\n",
    "        plt.savefig(PLOTSAVE, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMGZLw18mBw9"
   },
   "source": [
    "### visual_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "kQWqbao6vnWR"
   },
   "outputs": [],
   "source": [
    "def visual_eval(model, test_dl, cutEndToken=True):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    errors = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(test_dl):\n",
    "\n",
    "            src, src_len = batch\n",
    "            trg = src\n",
    "\n",
    "            output = model(src, src_len, trg, 0, verbose=True) #turn off teacher forcing\n",
    "            show_batch(output, trg, cutEndToken)\n",
    "\n",
    "\n",
    "def show_batch(output, trg, cutEndToken=True):\n",
    "    bs = output.shape[1]\n",
    "    pred = output.argmax(axis=-1)[1:]\n",
    "    trg = trg[1:]\n",
    "    if cutEndToken: \n",
    "        for b in range(bs):\n",
    "            p = cutEndToken(pred[:,b].tolist())\n",
    "            t = cutEndToken(trg[:,b].tolist())\n",
    "            status = \"same\" if p == t else \"different\"\n",
    "            print(f\"pred = {p} - {status} \\ntrg  = {t}\\n-\")\n",
    "    else:\n",
    "        for b in range(bs):\n",
    "            p = pred[:,b].tolist()\n",
    "            t = trg[:,b].tolist()\n",
    "            print(f\"pred = {p} \\ntrg  = {t}\\n-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6xnLRCNmDu1"
   },
   "source": [
    "### accuracy, accuracyAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "nPn1QguwvnWR"
   },
   "outputs": [],
   "source": [
    "def accuracy(model, iterator=None):\n",
    "    if iterator is None:\n",
    "        iterator = test_dls\n",
    "    for task_id in range(N_TASKS + 1):\n",
    "        gr_not_hits = evaluate_extra(model, iterator[task_id], allOrNoneLoss)\n",
    "        ugr_not_hits = evaluate_extra(model, test_ugr_dls[task_id], allOrNoneLoss)\n",
    "        gr_hits = 1 - gr_not_hits\n",
    "        ugr_hits = 1 - ugr_not_hits\n",
    "        total_acc = (gr_hits + ugr_not_hits) / 2\n",
    "        print(f\"Task {task_id}: Acc {total_acc:2.2}% | Gr acc {gr_hits:2.2} | Ugr acc {ugr_not_hits:2.2}\")\n",
    "        \n",
    "def accuracyAll(models):\n",
    "    for model_id in range(len(models)):\n",
    "        print(f\"\\nModel {model_id}\")\n",
    "        accuracy(models[model_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xoKXlA1vnWQ"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "jc0kqVE-vnWQ"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = max(vocabs[0].values()) + 1\n",
    "OUTPUT_DIM = max(vocabs[0].values()) + 1\n",
    "ENC_EMB_DIM = 19\n",
    "DEC_EMB_DIM = 19\n",
    "ENC_HID_DIM = 9\n",
    "DEC_HID_DIM = 9\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "LEARNING_RATE = 0.01\n",
    "LEARNING_RATE_GATING = 0.01\n",
    "SRC_PAD_IDX = PAD_TOKEN\n",
    "TRG_PAD_IDX = PAD_TOKEN\n",
    "PREFIX = \"pres\"\n",
    "CLIP = 1\n",
    "STEP_SIZE_EVALUATION = 10\n",
    "\n",
    "TEST_ALL_TASKS = 1\n",
    "N_EPOCHS = 200\n",
    "N_EPOCHS_TOTAL = 1200\n",
    "# N_EPOCHS_TOTAL = N_EPOCHS * (N_TASKS + TEST_ALL_TASKS)\n",
    "\n",
    "N_REPETITIONS = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(\n",
    "    experiment_name,\n",
    "    n_repetitions,\n",
    "    schedule,       # List of tuples: [(task_id, n_task_epochs),...]\n",
    "    init_func,      # Returns tuple: model, pass_on_variable which is given to repeat_func\n",
    "    repeat_func,    # Returns triple: hist_loss, hist_hits, model\n",
    "    step_size_evaluation=10\n",
    "):\n",
    "    n_epochs_total = sum([x[1] for x in schedule])\n",
    "    n_tasks_total = len(set([x[0] for x in schedule]))\n",
    "    n_evaluations = sum([x[1] // step_size_evaluation for x in schedule])\n",
    "    n_datasets = 3\n",
    "    \n",
    "    hist_all_losses = np.empty((n_repetitions, n_tasks_total, n_datasets,\n",
    "                                n_evaluations))\n",
    "    hist_all_hitsss = np.empty((n_repetitions, n_tasks_total, n_datasets,\n",
    "                                n_evaluations))\n",
    "\n",
    "    for repetition in range(n_repetitions):\n",
    "        print(f\"@@@@@@@@@ Repetition {repetition:3} @@@@@@@@@\")\n",
    "        if repetition == 0:\n",
    "            models = []\n",
    "        \n",
    "        # Call task specific init function\n",
    "        pass_on_variables = init_func()\n",
    "        \n",
    "        n_evalsteps_passed = 0\n",
    "        for i, (task_id, n_task_epochs) in enumerate(schedule):\n",
    "            tag = f\"{experiment_name}.s{i}.t{task_id}.e{n_task_epochs}\"\n",
    "            print(f\"\\nSCHEDULE: {tag}\")\n",
    "            \n",
    "            # Call task specific repeat function\n",
    "            hist_loss, hist_hits, model = repeat_func(n_tasks_total,\n",
    "                                                      n_task_epochs,\n",
    "                                                      task_id,\n",
    "                                                      step_size_evaluation,\n",
    "                                                      repetition,\n",
    "                                                      pass_on_variables)\n",
    "            \n",
    "            start_idx = n_evalsteps_passed\n",
    "            end_idx = n_evalsteps_passed + n_task_epochs // step_size_evaluation\n",
    "            hist_all_losses[repetition,:,:,start_idx:end_idx] = hist_loss\n",
    "            hist_all_hitsss[repetition,:,:,start_idx:end_idx] = hist_hits\n",
    "            \n",
    "            if repetition == 0 and model is not None:\n",
    "                models.append(copy.deepcopy(model))\n",
    "                \n",
    "            n_evalsteps_passed = end_idx\n",
    "    \n",
    "    return hist_all_losses, hist_all_hitsss, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schedule(n_epochs_total, n_total_tasks, n_task_epochs):\n",
    "    # Creates a training schedule:\n",
    "    schedule = []\n",
    "    epoch_counter = n_epochs_total\n",
    "\n",
    "    for task_id in itertools.cycle(range(n_total_tasks)):\n",
    "\n",
    "        if epoch_counter - n_task_epochs <=0:\n",
    "            schedule.append((task_id, epoch_counter))\n",
    "            break\n",
    "        else:\n",
    "            schedule.append((task_id, n_task_epochs))\n",
    "            epoch_counter -= n_task_epochs\n",
    "    return schedule\n",
    "\n",
    "def schedule_repetitions(x):\n",
    "    \"\"\"x amount of repetitions each task gets\"\"\"\n",
    "    assert x != 0\n",
    "    n_task_epochs = N_EPOCHS_TOTAL // ((N_TASKS + TEST_ALL_TASKS) * x)\n",
    "    return create_schedule(N_EPOCHS_TOTAL, N_TASKS + TEST_ALL_TASKS, n_task_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg_not_interleaved = create_schedule(150,3,50)\n",
    "dbg_medium_interleaved = create_schedule(150,3,25)\n",
    "dbg_strong_interleaved = create_schedule(150,3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_interleaved = schedule_repetitions(1)\n",
    "medium_interleaved = schedule_repetitions(4)\n",
    "strong_interleaved = schedule_repetitions(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline A: Individual Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_individual():\n",
    "    return None\n",
    "\n",
    "def repeat_individual(n_tasks_total, n_task_epochs, task_id, step_size_evaluation, repetition, pass_on_variables):\n",
    "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "    model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "\n",
    "    print(model.apply(init_weights))\n",
    "    print(f'The model has {count_parameters(model)} trainable parameters')\n",
    "\n",
    "    hist_loss, hist_hits= fit(\n",
    "        n_tasks_total,\n",
    "        model,\n",
    "        task_id,\n",
    "        n_task_epochs,\n",
    "        step_size_evaluation,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        CLIP,\n",
    "        repetition=repetition      \n",
    "    )\n",
    "    return hist_loss, hist_hits, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4QnSSsYav-F"
   },
   "source": [
    "### Experiment Individual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEDULE = not_interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "I9vjyK31vnWT"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@ Repetition   0 @@@@@@@@@\n",
      "\n",
      "SCHEDULE: Individual-1.s0.t0.e400\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 19)\n",
      "    (rnn): GRU(19, 9, bidirectional=True)\n",
      "    (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "      (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 19)\n",
      "    (rnn): GRU(37, 9)\n",
      "    (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "The model has 4028 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.642 | Train PPL:   1.900\n",
      "\t Val. Loss: 0.505 |  Val. PPL:   1.657\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.558\n",
      "\t Val. Loss: 0.414 |  Val. PPL:   1.513\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.516\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.436 | Train PPL:   1.547\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.500\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.406 | Train PPL:   1.500\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.418 | Train PPL:   1.519\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.473\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.489\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.370 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.499\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.414 |  Val. PPL:   1.514\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.414 |  Val. PPL:   1.513\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.308 |  Val. PPL:   1.361\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.467\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.401\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.340\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.460\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.252\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.209\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-c5bc0cc5c0f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_repetitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m hist_all_losses_A, hist_all_hitsss_A, models_A = experiment(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"Individual-1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mn_repetitions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mSCHEDULE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-4fb69d5f2fa2>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(experiment_name, n_repetitions, schedule, init_func, repeat_func, step_size_evaluation)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Call task specific repeat function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             hist_loss, hist_hits, model = repeat_func(n_tasks_total,\n\u001b[0m\u001b[1;32m     34\u001b[0m                                                       \u001b[0mn_task_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                                                       \u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-558ed593cd84>\u001b[0m in \u001b[0;36mrepeat_individual\u001b[0;34m(n_tasks_total, n_task_epochs, task_id, step_size_evaluation, repetition, pass_on_variables)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'The model has {count_parameters(model)} trainable parameters'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     hist_loss, hist_hits= fit(\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mn_tasks_total\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-abf148d09830>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(n_tasks_total, model, task_id, n_task_epochs, step_size_evaluation, optimizer, criterion, clip, repetition)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-35e15fd6387a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_repetitions = 1\n",
    "hist_all_losses_A, hist_all_hitsss_A, models_A = experiment(\n",
    "    \"Individual-1\",\n",
    "    n_repetitions,\n",
    "    SCHEDULE,\n",
    "    init_individual,\n",
    "    repeat_individual,\n",
    "    STEP_SIZE_EVALUATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAverages(hist_all_hitsss_A, SCHEDULE, STEP_SIZE_EVALUATION, figsize=(12,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GlY2ObLqvnWV",
    "outputId": "6476a3b7-512d-4bc0-9dd2-dc47319cbef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.81% | Gr acc 0.62 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yD_rpqtSvnWV"
   },
   "source": [
    "## Baseline B: Keep Training same model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_standard():\n",
    "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "    model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "    print(model.apply(init_weights))\n",
    "    return model\n",
    "\n",
    "def repeat_standard(n_tasks_total, n_task_epochs, task_id, step_size_evaluation, repetition, pass_on_variables):\n",
    "    model = pass_on_variables\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "\n",
    "    hist_loss, hist_hits= fit(\n",
    "        n_tasks_total,\n",
    "        model,\n",
    "        task_id,\n",
    "        n_task_epochs,\n",
    "        step_size_evaluation,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        CLIP,\n",
    "        repetition=repetition      \n",
    "    )\n",
    "    return hist_loss, hist_hits, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqEL860-a9t9"
   },
   "source": [
    "### Experiment Keep Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "TRGPK1CjvnWV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEDULE = not_interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9zXm7oSvnWW",
    "outputId": "b2d8e43a-6773-4c10-e7d7-15fe7a662fb2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@ Repetition   0 @@@@@@@@@\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 19)\n",
      "    (rnn): GRU(19, 9, bidirectional=True)\n",
      "    (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "      (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 19)\n",
      "    (rnn): GRU(37, 9)\n",
      "    (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: B-standard-.s0.t0.e400\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.662 | Train PPL:   1.938\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.458 | Train PPL:   1.581\n",
      "\t Val. Loss: 0.414 |  Val. PPL:   1.512\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.430 | Train PPL:   1.537\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.429 |  Val. PPL:   1.535\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.390 | Train PPL:   1.476\n",
      "\t Val. Loss: 0.466 |  Val. PPL:   1.594\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.390 | Train PPL:   1.477\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.515\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.466\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.491\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.485\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.437\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.400\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.401\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.349\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.450\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.312\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.463\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.226\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.237\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.182\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 101 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 102 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 103 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 104 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 105 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 106 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 107 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 108 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.143\n",
      "Epoch: 109 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 110 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 111 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 112 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 113 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 114 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 115 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 116 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 117 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 118 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 119 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 120 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 121 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 122 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 123 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 124 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 125 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 126 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 127 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 128 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 129 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 130 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 131 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 132 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 133 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 134 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 135 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 136 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 137 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 138 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 139 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 140 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 141 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 142 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 143 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 144 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 145 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 146 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 147 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 148 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 149 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 150 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 151 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 152 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 153 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 154 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 155 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 156 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 157 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 158 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 159 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 160 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 161 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 162 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 163 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 164 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 165 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 166 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 167 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 168 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 169 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 170 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 171 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 172 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 173 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 174 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 175 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 176 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 177 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 178 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 179 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 180 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 182 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 183 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 184 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 185 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 186 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 187 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 188 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 189 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 190 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 191 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 194 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 195 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 196 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 197 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 198 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 199 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 200 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 201 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 202 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 203 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 204 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 205 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 206 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 207 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 208 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 209 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 210 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 211 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 212 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 213 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 214 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 215 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 216 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 217 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 218 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 219 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 220 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 221 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 222 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 223 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 224 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 225 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 226 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 227 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 228 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 229 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 230 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 231 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 232 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 233 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 234 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 235 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 236 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 237 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 238 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 239 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 240 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 241 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 242 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 243 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 244 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 245 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 246 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 247 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 248 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 249 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 250 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 251 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 252 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 253 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 254 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 255 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 256 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 257 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 258 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 259 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 260 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 261 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 262 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 263 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 264 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 265 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 266 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 267 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 268 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 269 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 270 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 271 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 272 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 273 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 274 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 275 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 276 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 277 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 278 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 279 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 280 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 281 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 282 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 283 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 284 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 285 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 286 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 287 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 288 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 289 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 290 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 291 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 292 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 293 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 294 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 295 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 296 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 297 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 298 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 299 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 300 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 301 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 302 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 303 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 305 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 308 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 315 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 316 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 318 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 321 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 326 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 328 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 330 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 331 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 332 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 333 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 335 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 336 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 337 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 340 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 341 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 342 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 343 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 344 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 345 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 347 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 348 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 349 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 350 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 352 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 353 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 354 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 355 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 356 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 358 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 360 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 362 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 363 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 369 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 374 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 375 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 376 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 379 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 380 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 381 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 385 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 387 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 389 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 390 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 391 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 392 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 393 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 395 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 399 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "\n",
      "SCHEDULE: B-standard-.s1.t1.e400\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.248\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.209\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.125\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 101 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 102 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 103 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 104 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 105 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 106 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 107 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 108 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 109 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 110 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 111 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 112 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 113 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 114 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 115 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 116 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 117 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 118 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 119 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 120 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 121 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 122 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 123 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 124 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 125 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 126 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 127 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 128 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 129 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 130 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 131 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 132 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 133 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 134 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 135 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 136 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 137 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 138 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 139 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 140 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 141 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 142 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 143 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 144 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 145 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 146 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 147 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 148 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 150 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 152 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 154 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 157 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 160 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 166 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 167 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 168 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 169 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 171 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 191 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 194 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 195 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 196 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 201 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 202 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 203 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 204 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 205 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 206 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 207 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 208 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 209 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 210 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 211 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 212 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 213 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 214 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 215 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 216 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 217 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 218 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 219 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 220 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 221 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 222 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 223 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 224 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 225 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 226 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 227 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 228 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 229 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 230 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 231 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 232 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 233 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 234 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 235 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 236 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 237 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 238 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 239 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 240 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 241 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 242 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 243 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 244 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 245 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 246 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 247 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 248 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 249 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 250 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 251 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 252 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 253 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 254 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 255 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 256 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 257 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 258 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 259 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 260 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 261 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 262 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 263 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 264 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 265 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 266 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 267 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 268 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 269 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 270 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 271 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 272 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 273 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 274 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 275 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 276 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 277 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 278 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 279 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 280 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 281 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 282 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 283 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 284 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 285 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 286 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 287 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 288 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 289 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 290 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 291 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 292 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 293 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 294 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 295 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 296 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 297 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 298 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 299 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 300 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 301 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 302 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 303 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 304 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 305 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 306 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 307 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 308 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 309 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 310 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 311 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 313 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 314 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 315 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 316 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 317 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 318 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 319 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 320 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 321 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 322 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 323 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 324 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 325 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 326 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 327 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 328 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 329 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 330 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 331 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 332 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 333 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 334 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 335 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 336 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 337 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 339 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 340 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 341 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 342 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 343 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 344 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 347 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 348 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 349 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 350 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 352 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 353 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 354 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 355 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 356 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 360 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 361 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 362 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 363 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 364 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 366 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 369 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 370 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 371 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 374 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 375 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 376 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 377 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 379 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 382 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 383 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 385 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 388 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 389 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 390 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 391 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 392 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 393 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 394 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 395 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 397 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 398 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 399 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 400 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "\n",
      "SCHEDULE: B-standard-.s2.t2.e400\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.168\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 101 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 102 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 103 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 104 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 105 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 106 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 107 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 108 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 109 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 110 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 111 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 112 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 113 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 114 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 115 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 116 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 117 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 118 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 119 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 120 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 121 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 122 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 123 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 124 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 125 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 126 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 127 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 128 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 129 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 130 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 131 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 132 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 133 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 134 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 135 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 136 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 137 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 138 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 139 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 140 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 141 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 143 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 144 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 145 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 146 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 147 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 148 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 149 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 150 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 152 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 153 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 155 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 158 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 160 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 161 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 162 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 164 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 167 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 168 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 183 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 184 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 186 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 201 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 202 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 203 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 204 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 205 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 206 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 207 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 208 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 209 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 210 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 211 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 212 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 213 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 214 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 215 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 216 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 217 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 218 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 219 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 220 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 221 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 222 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 223 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 224 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 225 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 226 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 227 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 228 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 229 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 230 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 231 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 232 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 233 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 234 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 235 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 236 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 237 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 238 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 239 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 240 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 241 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 242 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 243 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 244 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 245 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 246 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 247 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 248 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 249 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 250 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 251 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 252 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 253 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 254 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 255 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 256 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 257 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 258 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 259 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 260 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 261 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 262 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 263 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 264 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 265 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 266 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 267 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 268 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 269 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 270 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 271 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 272 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 273 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 274 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 275 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 276 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 277 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 278 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 279 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 280 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 281 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 282 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 283 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 284 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 285 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 286 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 287 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 288 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 289 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 290 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 291 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 292 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 293 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 294 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 295 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 296 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 297 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 298 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 299 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 300 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 301 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 302 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 303 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 304 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 305 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 306 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 307 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 308 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 309 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 310 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 311 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 312 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 313 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 314 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 315 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 316 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 317 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 318 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 319 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 320 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 321 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 322 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 323 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 324 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 325 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 326 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 328 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 329 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 330 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 331 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 332 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 333 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 334 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 335 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 336 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 337 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 340 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 341 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 342 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 343 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 344 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 347 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 348 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 349 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 350 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 352 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 353 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 354 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 355 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 356 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 360 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 362 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 363 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 367 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 368 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 369 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 373 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 374 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 375 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 376 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 377 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 378 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 379 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 385 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 390 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 392 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 393 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 394 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 395 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 396 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 397 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 399 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 400 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = 1\n",
    "hist_all_losses_B, hist_all_hitsss_B, models_B = experiment(\n",
    "    \"B-standard-\",\n",
    "    n_repetitions,\n",
    "    SCHEDULE,\n",
    "    init_standard,\n",
    "    repeat_standard,\n",
    "    STEP_SIZE_EVALUATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_B, SCHEDULE, STEP_SIZE_EVALUATION, figsize=(12,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotSpecificTask(hist_all_hitsss_B, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZD9r3RykvnWX",
    "outputId": "e5330dc5-23ab-4038-af99-0fb47c16ec07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 1.0% | Gr acc 1.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 1.0% | Gr acc 1.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.94% | Gr acc 0.88 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interleaved Keep Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "TRGPK1CjvnWV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEDULE = medium_interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9zXm7oSvnWW",
    "outputId": "b2d8e43a-6773-4c10-e7d7-15fe7a662fb2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@ Repetition   0 @@@@@@@@@\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 19)\n",
      "    (rnn): GRU(19, 9, bidirectional=True)\n",
      "    (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "      (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 19)\n",
      "    (rnn): GRU(37, 9)\n",
      "    (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: B2-standard-.s0.t0.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.701 | Train PPL:   2.015\n",
      "\t Val. Loss: 0.550 |  Val. PPL:   1.733\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.564\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.572\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.482\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.499\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.393 | Train PPL:   1.482\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.452 | Train PPL:   1.571\n",
      "\t Val. Loss: 0.404 |  Val. PPL:   1.498\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.517\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.427\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.508\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.429 |  Val. PPL:   1.535\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.530\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.417 |  Val. PPL:   1.518\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.194\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.188\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.164\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.168\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.145\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "\n",
      "SCHEDULE: B2-standard-.s1.t1.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.475 | Train PPL:   1.608\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.511\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.363\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.356\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.248\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.175\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.115\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "\n",
      "SCHEDULE: B2-standard-.s2.t2.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.285\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.195\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.150\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "\n",
      "SCHEDULE: B2-standard-.s3.t0.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "\n",
      "SCHEDULE: B2-standard-.s4.t1.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "SCHEDULE: B2-standard-.s5.t2.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "SCHEDULE: B2-standard-.s6.t0.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "\n",
      "SCHEDULE: B2-standard-.s7.t1.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "\n",
      "SCHEDULE: B2-standard-.s8.t2.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "SCHEDULE: B2-standard-.s9.t0.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "\n",
      "SCHEDULE: B2-standard-.s10.t1.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "\n",
      "SCHEDULE: B2-standard-.s11.t2.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = 1\n",
    "hist_all_losses_B2, hist_all_hitsss_B2, models_B2 = experiment(\n",
    "    \"B2-standard-\",\n",
    "    n_repetitions,\n",
    "    SCHEDULE,\n",
    "    init_standard,\n",
    "    repeat_standard,\n",
    "    STEP_SIZE_EVALUATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_B2, SCHEDULE, STEP_SIZE_EVALUATION, figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUleAoTVvnWY"
   },
   "source": [
    "## Baseline C: Freeze Parameters (not updated)\n",
    "\n",
    "1. Define functions\n",
    "2. Train model, freeze core weights in between tasks\n",
    "3. Look at performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbcfqNFlbGjA"
   },
   "source": [
    "### onTaskUpdate, applyOnParameters, freezeParameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "EiFSSGphvnWY"
   },
   "outputs": [],
   "source": [
    "def applyOnParameters(model, conditions, apply_function):\n",
    "    for name, param in model.named_parameters():\n",
    "        # Check every condition\n",
    "        for condition in conditions:\n",
    "            # check every keyword\n",
    "            allincluded = True\n",
    "            for keyword in condition:\n",
    "                if keyword not in name:\n",
    "                    allincluded = False\n",
    "                    break\n",
    "            if allincluded:\n",
    "                apply_function(param)\n",
    "\n",
    "def freezeParameters(model, conditions):\n",
    "    def freeze(param):\n",
    "        param.requires_grad = False\n",
    "    applyOnParameters(model, conditions, freeze)\n",
    "\n",
    "def unfreezeParameters(model, conditions):\n",
    "    def unfreeze(param):\n",
    "        param.requires_grad = True\n",
    "    applyOnParameters(model, conditions, unfreeze)\n",
    "\n",
    "def showModelParameters(model, requires_grad=False):\n",
    "    for name, param in model.named_parameters():\n",
    "        if requires_grad:\n",
    "            if param.requires_grad:\n",
    "                print(name)\n",
    "        else:\n",
    "            print(name)\n",
    "            \n",
    "def onTaskUpdate(model):\n",
    "    # Freeze core weights\n",
    "    freezeParameters(model, ((\"\"),))    # Freeze everything\n",
    "    unfreezeParameters(model, ((\"encoder\",\"embedding\"), (\"decoder\",\"fc_out\"), (\"attention\",))) # Unfreeze relevant stuff\n",
    "    \n",
    "    # Reinitialize\n",
    "    to_constant = lambda param: nn.init.constant_(param.data, 0)\n",
    "    applyOnParameters(model, ((\"decoder\",\"fc_out\",\"bias\"),(\"attn\",\"bias\")), to_constant)\n",
    "    to_normal = lambda param: nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "    applyOnParameters(model, ((\"encoder\",\"embedding\"),(\"decoder\",\"fc_out\",\"weight\"),(\"attention\",\"weight\")), to_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJlVK1CCvnWZ"
   },
   "source": [
    "### Experiment Freeze Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "9_UJzijZvnWZ"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H83P-2BXvnWZ",
    "outputId": "8c4cbe78-90a0-46bc-cc0f-8a9328d47640",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-C0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.613 | Train PPL:   1.845\n",
      "\t Val. Loss: 0.481 |  Val. PPL:   1.618\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.603\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.533\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.558\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.426 | Train PPL:   1.531\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.503\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.469\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.380 | Train PPL:   1.463\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.454\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.490\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.489\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.349\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.326\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.307\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.184\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.134\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.122\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "tr-AE-30-10-0.01-C1\n",
      "The model has 1048 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.642 | Train PPL:   1.900\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.519\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.401 | Train PPL:   1.493\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.416\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.257\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.269\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.285\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.237\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.244\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.262\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.150\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.172\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.147\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "tr-AE-30-10-0.01-C2\n",
      "The model has 1048 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.502 | Train PPL:   1.652\n",
      "\t Val. Loss: 0.428 |  Val. PPL:   1.534\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.491\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.502\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.463\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.440\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.370 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.397\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.400\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.407\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.381\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.358\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.348\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.342\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.348\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.352\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.321\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.303\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.313\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.292\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.278\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.276\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = N_REPETITIONS\n",
    "hist_all_losses_C = np.empty((n_repetitions, N_TASKS + TEST_ALL_TASKS,\n",
    "                              N_TASKS + TEST_ALL_TASKS, 3,\n",
    "                              N_EPOCHS // STEP_SIZE_EVALUATION))\n",
    "hist_all_hitsss_C = np.empty((n_repetitions, N_TASKS + TEST_ALL_TASKS,\n",
    "                              N_TASKS + TEST_ALL_TASKS, 3,\n",
    "                              N_EPOCHS // STEP_SIZE_EVALUATION))\n",
    "for repetition in range(n_repetitions):\n",
    "    print(f\"\\n\\n\\n\\n\\n\\n------ REPETITION {repetition:3} ------\")\n",
    "    # To have single copy\n",
    "    if repetition == n_repetitions - 1:\n",
    "        models_F = []\n",
    "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "    model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "    print(model.apply(init_weights))\n",
    "\n",
    "    for n_task in range(N_TASKS + TEST_ALL_TASKS):\n",
    "        SUFFIX = f\"C{n_task}\"\n",
    "        title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "        LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "        SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "        PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "        criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "\n",
    "        print(title)\n",
    "        print(f'The model has {count_parameters(model)} trainable parameters')\n",
    "\n",
    "        hist_loss_temp, hist_hits_temp = fit(model, n_task, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "        hist_all_losses_C[repetition,n_task] = hist_hits_temp\n",
    "        hist_all_hitsss_C[repetition,n_task] = hist_hits_temp\n",
    "        if repetition == n_repetitions - 1:\n",
    "            models_C.append(copy.deepcopy(model))\n",
    "\n",
    "        # Freeze, reinitialize\n",
    "        onTaskUpdate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "x_07dWu8vnWa",
    "outputId": "22e6837e-a1ee-44e7-cc6c-1ccfd78122e3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss_C = torch.cat(hist_losses_C, dim=2)\n",
    "hist_hits_C = torch.cat(hist_hitsss_C, dim=2)\n",
    "\n",
    "plotResults(hist_loss_C, hist_hits_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6K7g1tH3vnWa",
    "outputId": "25d75c5a-f339-4071-8aad-0f4df6f05ac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkGxECJqvnWb"
   },
   "source": [
    "## Transfer D: EWC (unfunctional)\n",
    "\n",
    "1. Create Fisher functions\n",
    "2. Train model on tasks\n",
    "3. Compare results\n",
    "\n",
    "Based on: https://github.com/ContinualAI/colab/blob/master/notebooks/intro_to_continual_learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNS4W04AvnWc"
   },
   "source": [
    "Compute optimal parameters and fisher information after training on tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AI4QzSeavnWc"
   },
   "outputs": [],
   "source": [
    "def onTaskUpdate_ewc(model, task_id, train_dl, criterion):\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #accumulate Gradient\n",
    "    for it in range(100):\n",
    "        for seq, seq_len in train_dl:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0)\n",
    "\n",
    "            if criterion == F.cross_entropy:\n",
    "              output_dim = output.shape[-1]\n",
    "                \n",
    "              output = output[1:].view(-1, output_dim)\n",
    "\n",
    "              trg = seq[1:].view(-1)\n",
    "\n",
    "              loss = criterion(output, trg)\n",
    "            else:\n",
    "              loss = criterion(output, seq)\n",
    "            #print(loss)\n",
    "\n",
    "            loss.backward()\n",
    "        \n",
    "    fishers.append({})\n",
    "    optParams.append({})\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        fishers[task_id][name] = param.grad.data.clone().pow(2)\n",
    "        optParams[task_id][name] = param.data.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJu-KM7mvnWc"
   },
   "source": [
    "Adapt evaluation and training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRW-TlhwvnWd"
   },
   "outputs": [],
   "source": [
    "def train_ewc(model, task_id, dataloader, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seq, seq_len in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(seq, seq_len, seq)\n",
    "        loss = criterion(output, seq)\n",
    "        \n",
    "        if task_id > 0:\n",
    "            print(\"-\\n\", loss)\n",
    "        \n",
    "        # EWC Training penalty\n",
    "        for other_task_id in range(task_id):\n",
    "            for name, param in model.named_parameters():\n",
    "                fisher = fishers[other_task_id][name]\n",
    "                optParam = optParams[other_task_id][name]\n",
    "                #print(ewc_lambda)\n",
    "                loss += (ewc_lambda / 2) * (fisher * (optParam - param).pow(2)).sum()\n",
    "                #print((fisher * (optParam - param).pow(2)).sum())\n",
    "                #print((optParam - param).pow(2).sum())\n",
    "                #loss += ewc_lambda * (optParam - param).pow(2).sum()\n",
    "        \n",
    "        if task_id > 0:\n",
    "            print(loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnQbtl1yvnWe"
   },
   "outputs": [],
   "source": [
    "def evaluate_ewc(model, task_id, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for seq, seq_len in dataloader:\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0) #turn off teacher forcing\n",
    "\n",
    "            loss = criterion(output, seq).type(torch.float)\n",
    "            \n",
    "            # EWC Training penalty\n",
    "            for other_task_id in range(task_id):\n",
    "                for name, param in model.named_parameters():\n",
    "                    fisher = fishers[other_task_id][name]\n",
    "                    optParam = optParams[other_task_id][name]\n",
    "                    loss += (ewc_lambda / 2) * (fisher * (optParam - param).pow(2)).sum()\n",
    "                    \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgGMwiHcvnWe"
   },
   "outputs": [],
   "source": [
    "def fit_ewc(model, task_id, epochs, step_size_evaluation, clip ):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train_ewc(model, task_id, train_dls[task_id], optimizer, criterion, clip)\n",
    "        valid_loss = evaluate_ewc(model, task_id, valid_dls[task_id], criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            for other_id in range(task_id + 1):\n",
    "                total_loss[other_id,0,idx] = evaluate_ewc(model, task_id, train_dls[other_id], criterion)\n",
    "                total_loss[other_id,1,idx] = evaluate_ewc(model, task_id, test_dls[other_id], criterion)\n",
    "                total_loss[other_id,2,idx] = evaluate_ewc(model, task_id, test_ugr_dls[other_id], criterion)\n",
    "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
    "\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NI--0ZFvnWe"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bB0VOa5lvnWf",
    "outputId": "cfd946be-3342-47d9-a885-90fb65f024f9",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      " tensor(0.0976, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1117, grad_fn=<AddBackward0>)\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "-\n",
      " tensor(0.1121, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1263, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1110, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1253, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1112, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1254, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1282, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1422, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1154, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0998, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0900, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1036, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1173, grad_fn=<AddBackward0>)\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.1189, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1327, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1009, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1146, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1829, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1102, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1021, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1156, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0869, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0972, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1105, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2079, grad_fn=<AddBackward0>)\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "-\n",
      " tensor(0.1126, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1258, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1095, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1227, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1115, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0962, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1094, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0914, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0993, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1126, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0826, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0959, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0960, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1092, grad_fn=<AddBackward0>)\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.1142, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1275, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1053, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1186, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1186, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1145, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1275, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0941, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1071, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0897, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, grad_fn=<AddBackward0>)\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.1250, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1379, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1136, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1264, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1391, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1517, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0943, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1074, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1200, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0891, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1385, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1511, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1211, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1339, grad_fn=<AddBackward0>)\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.2965, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3096, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2932, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3065, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1084, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1220, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1228, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1366, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1162, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1303, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0907, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1050, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0888, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1032, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1152, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1298, grad_fn=<AddBackward0>)\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.1047, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1194, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1184, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1130, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1306, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1457, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1167, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1319, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1919, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0993, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1144, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2151, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2300, grad_fn=<AddBackward0>)\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "-\n",
      " tensor(0.1167, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1315, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0968, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1146, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0972, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2175, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2325, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1107, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1256, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1040, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1189, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3172, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3321, grad_fn=<AddBackward0>)\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1099, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1111, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1259, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0967, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1115, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2133, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2282, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2158, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2308, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1072, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1222, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1113, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1265, grad_fn=<AddBackward0>)\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.1131, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1285, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1082, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1239, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1157, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1317, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1101, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1262, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1144, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1308, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1075, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1240, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3278, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3444, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1183, grad_fn=<AddBackward0>)\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.1036, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1205, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1088, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1260, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1044, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1217, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1150, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0968, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1146, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0967, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1145, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1379, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1557, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1043, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1222, grad_fn=<AddBackward0>)\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.1368, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1546, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1200, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1378, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1921, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2099, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1193, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1166, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1245, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1419, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2155, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2326, grad_fn=<AddBackward0>)\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.1816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1076, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1239, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2703, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1076, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1237, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1081, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1196, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1361, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1426, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1592, grad_fn=<AddBackward0>)\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "-\n",
      " tensor(0.1610, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1778, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1098, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0992, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1158, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0974, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1140, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0984, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1150, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1053, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1218, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1308, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1472, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1120, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1283, grad_fn=<AddBackward0>)\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "-\n",
      " tensor(0.1032, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1194, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1125, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1286, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1008, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1169, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0989, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1150, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1199, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1360, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0922, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1083, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.1150, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1310, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1318, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1477, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1253, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1411, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1187, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1345, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1036, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0868, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3831, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2618, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2772, grad_fn=<AddBackward0>)\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "-\n",
      " tensor(0.1353, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1506, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1048, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1200, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1105, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1253, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1043, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1191, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1284, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1431, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1006, grad_fn=<AddBackward0>)\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "-\n",
      " tensor(0.0956, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1059, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1008, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1154, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1065, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1212, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0981, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1314, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1461, grad_fn=<AddBackward0>)\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "-\n",
      " tensor(0.0887, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1034, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1130, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0965, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1111, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1154, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1300, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1524, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1669, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1488, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1632, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0978, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1024, grad_fn=<AddBackward0>)\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "-\n",
      " tensor(0.0902, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1044, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1057, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1198, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1153, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0842, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1014, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1156, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.1235, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1379, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1241, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1386, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1003, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1148, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0917, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1062, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1026, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1170, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, grad_fn=<AddBackward0>)\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1044, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1190, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1011, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1157, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0989, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0945, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1008, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1152, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1031, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1173, grad_fn=<AddBackward0>)\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1060, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0894, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1095, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1236, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0952, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0923, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0864, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0768, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0908, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0964, grad_fn=<AddBackward0>)\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.0827, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2350, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2487, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1029, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1165, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0868, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0933, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1069, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0945, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1082, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1073, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1210, grad_fn=<AddBackward0>)\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0879, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1090, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1168, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1307, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0939, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2639, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2778, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0963, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0872, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1008, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0912, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1069, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1003, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0972, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1105, grad_fn=<AddBackward0>)\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.1286, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1419, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0922, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0939, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1074, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0920, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1011, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1070, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1207, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1001, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0809, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0742, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0730, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0862, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0999, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1130, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1004, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0968, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1096, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0957, grad_fn=<AddBackward0>)\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1008, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1020, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0869, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1013, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0847, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1042, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1041, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1166, grad_fn=<AddBackward0>)\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1093, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1220, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0877, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0952, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0980, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0928, grad_fn=<AddBackward0>)\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1033, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0791, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1061, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1187, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0715, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0839, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0949, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.1074, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1198, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0941, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0899, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1022, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0809, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0842, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1026, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1144, grad_fn=<AddBackward0>)\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0754, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0881, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1083, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0886, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0873, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0710, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0826, grad_fn=<AddBackward0>)\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0777, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0892, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0952, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1073, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1187, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0943, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0810, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0925, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1040, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1155, grad_fn=<AddBackward0>)\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0726, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0841, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0843, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0963, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1083, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1197, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0895, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0697, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0811, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0921, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1053, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1167, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1094, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0844, grad_fn=<AddBackward0>)\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0868, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0983, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0814, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0795, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0952, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0760, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0772, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0885, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0744, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0856, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0768, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0971, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1047, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1040, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1152, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1023, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0906, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0902, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0717, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0832, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1096, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1211, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1019, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1133, grad_fn=<AddBackward0>)\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0852, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0747, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0861, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1067, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1180, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1066, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1178, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1889, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2002, grad_fn=<AddBackward0>)\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1011, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1045, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0760, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0900, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1014, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1006, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0741, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0858, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0810, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1020, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1139, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3459, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3578, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0911, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1031, grad_fn=<AddBackward0>)\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0941, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1389, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1514, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1031, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0919, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1049, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0938, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1070, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1029, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1211, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1343, grad_fn=<AddBackward0>)\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0864, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0996, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0811, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0903, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1042, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1178, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1060, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1197, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1110, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1248, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0854, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0991, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0973, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1117, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0903, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1089, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1234, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0964, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0846, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, grad_fn=<AddBackward0>)\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "-\n",
      " tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0923, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1073, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1219, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0916, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1090, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1237, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0950, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1096, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1013, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1158, grad_fn=<AddBackward0>)\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1006, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1089, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0802, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1117, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1258, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1077, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0919, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0800, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.1103, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1239, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1122, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1255, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0914, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0858, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0743, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2989, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.1353, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1478, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0978, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1103, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1041, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1167, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0957, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1162, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0969, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1079, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1202, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0886, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1024, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1091, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1215, grad_fn=<AddBackward0>)\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0894, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0892, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1179, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1299, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0810, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0920, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0928, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0730, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0814, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0852, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0909, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1004, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1007, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1125, grad_fn=<AddBackward0>)\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.1029, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0970, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1072, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0794, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0706, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0830, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1879, grad_fn=<AddBackward0>)\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.1426, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1551, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1002, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1128, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0854, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0983, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2096, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2227, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1103, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1241, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1071, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1212, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "-\n",
      " tensor(0.1100, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1244, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1126, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1272, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1221, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1364, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0864, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1007, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0992, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1323, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1464, grad_fn=<AddBackward0>)\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1513, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1658, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0955, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1164, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1316, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1193, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1349, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0942, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1100, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0916, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1075, grad_fn=<AddBackward0>)\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1223, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1384, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1135, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1207, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1367, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0839, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0810, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1294, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1449, grad_fn=<AddBackward0>)\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.1100, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1254, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1564, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1716, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1096, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1249, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1190, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1463, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1618, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "-\n",
      " tensor(0.2395, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2554, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1126, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1288, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1059, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1223, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1023, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1191, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1237, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1408, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0939, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1112, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3442, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3616, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1160, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1332, grad_fn=<AddBackward0>)\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "-\n",
      " tensor(0.0997, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1170, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1093, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1265, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1032, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0903, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1077, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1125, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1131, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1156, grad_fn=<AddBackward0>)\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.1188, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1360, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1102, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1095, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1297, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1470, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1308, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1482, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0963, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1139, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0885, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1062, grad_fn=<AddBackward0>)\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.1059, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1238, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1127, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1308, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1089, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1269, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0986, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1166, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0959, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1072, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0968, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1074, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0811, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1008, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1184, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0964, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0967, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1030, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1199, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0855, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1022, grad_fn=<AddBackward0>)\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.1086, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1251, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1132, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0943, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1103, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0891, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1050, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1046, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "-\n",
      " tensor(0.0924, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0915, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1062, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0921, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0768, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0922, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1196, grad_fn=<AddBackward0>)\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0976, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0933, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0831, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0971, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0723, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1087, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1229, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0991, grad_fn=<AddBackward0>)\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0925, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1017, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1157, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0914, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0855, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0990, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1115, grad_fn=<AddBackward0>)\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0970, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0956, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1088, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1084, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1216, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1149, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1282, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0703, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0838, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1044, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1180, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0844, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0980, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0897, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1031, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0978, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0823, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0955, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0914, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1044, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1012, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0914, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0961, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1094, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1126, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1261, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0809, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0716, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0853, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1061, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1196, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0835, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0968, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0864, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0993, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0987, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1114, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1061, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0739, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0864, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0727, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0872, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1014, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0823, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0976, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0909, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1059, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1183, grad_fn=<AddBackward0>)\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0969, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0734, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0957, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1083, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0818, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0908, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0886, grad_fn=<AddBackward0>)\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0998, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1121, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0767, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0996, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1048, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1167, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0689, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0808, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0717, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0836, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1066, grad_fn=<AddBackward0>)\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0900, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0861, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0980, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0917, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0721, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0838, grad_fn=<AddBackward0>)\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0747, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0864, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0798, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0723, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0841, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0774, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0892, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0973, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0727, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0733, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0851, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0872, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0990, grad_fn=<AddBackward0>)\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0747, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0862, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0723, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0838, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0808, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0823, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0938, grad_fn=<AddBackward0>)\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0851, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0877, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0735, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0978, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1005, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0794, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1027, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1142, grad_fn=<AddBackward0>)\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.1020, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1135, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1129, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0777, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, grad_fn=<AddBackward0>)\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0681, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0795, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1129, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0922, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0724, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0837, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0885, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0895, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0963, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1075, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0877, grad_fn=<AddBackward0>)\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0695, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0807, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0903, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0770, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1002, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1114, grad_fn=<AddBackward0>)\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1039, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1151, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0814, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0843, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0770, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0882, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0739, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0858, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0969, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0843, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0932, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0898, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0847, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1722, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1832, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0933, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0862, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0907, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0925, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0899, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1012, grad_fn=<AddBackward0>)\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0954, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1066, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0795, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0907, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0895, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0970, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0713, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0824, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0839, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0949, grad_fn=<AddBackward0>)\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0849, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0806, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0876, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0803, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0705, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0817, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0725, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0837, grad_fn=<AddBackward0>)\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0694, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0806, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1095, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0667, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0779, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0793, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0903, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0861, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0985, grad_fn=<AddBackward0>)\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0960, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1069, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1044, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0865, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0737, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0846, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0960, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0954, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0909, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0837, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0999, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0713, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0821, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0879, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0840, grad_fn=<AddBackward0>)\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-\n",
      " tensor(0.0746, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0854, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0711, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0820, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0693, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0803, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0875, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0802, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2095, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2208, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0900, grad_fn=<AddBackward0>)\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0735, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0848, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0727, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0842, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0844, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0942, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1058, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0724, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0841, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1405, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1522, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0994, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1144, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1259, grad_fn=<AddBackward0>)\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1058, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1241, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1366, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0870, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1205, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1337, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1026, grad_fn=<AddBackward0>)\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.1189, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1335, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1026, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1173, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1029, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0925, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1013, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1050, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1202, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1113, grad_fn=<AddBackward0>)\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1023, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1027, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1184, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0916, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0804, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1024, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0977, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1092, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1249, grad_fn=<AddBackward0>)\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0827, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1253, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0847, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1000, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0876, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0879, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1028, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0798, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1181, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1323, grad_fn=<AddBackward0>)\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0915, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1124, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1263, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0946, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0861, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0987, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1124, grad_fn=<AddBackward0>)\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0770, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0907, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0861, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0954, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0749, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0884, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0739, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0909, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1042, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0868, grad_fn=<AddBackward0>)\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1033, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1165, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0705, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0835, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0950, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1080, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0981, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0806, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0855, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0903, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0989, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0943, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0688, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0811, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0743, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0866, grad_fn=<AddBackward0>)\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0742, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0864, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0735, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0855, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0954, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0696, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0815, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0734, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0852, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0709, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0827, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0685, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0802, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0749, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0797, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0887, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1005, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0858, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1011, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1127, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0803, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0875, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0672, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0787, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1089, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0693, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0808, grad_fn=<AddBackward0>)\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0698, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0813, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0853, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0745, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0924, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0864, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0846, grad_fn=<AddBackward0>)\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0869, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0709, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0823, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0986, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, grad_fn=<AddBackward0>)\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1171, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0872, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0712, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0825, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0859, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0711, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0822, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0926, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, grad_fn=<AddBackward0>)\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0749, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0902, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0804, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0989, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0960, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1075, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0957, grad_fn=<AddBackward0>)\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0925, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0698, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0811, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0708, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0821, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0954, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0898, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0954, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0842, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0888, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0774, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0703, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0817, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0997, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1109, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0680, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0791, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0911, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1026, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0708, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0818, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0684, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0795, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0721, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0832, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0772, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0838, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0839, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0719, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0828, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0993, grad_fn=<AddBackward0>)\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "-\n",
      " tensor(0.0941, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1052, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0919, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0767, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0878, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0846, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0795, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0815, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0924, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0842, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-\n",
      " tensor(0.0982, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1090, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1000, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0686, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0795, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0779, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0796, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0978, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0757, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0869, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0861, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0706, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0817, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0831, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0739, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1092, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1232, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1343, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1888, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0815, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0809, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1070, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1075, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1195, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1035, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0922, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0741, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1155, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1284, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0908, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1033, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1112, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1070, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1207, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0779, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0917, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1059, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1017, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, grad_fn=<AddBackward0>)\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1079, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1021, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1163, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1189, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1330, grad_fn=<AddBackward0>)\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0791, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0901, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1068, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1208, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1050, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1006, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0987, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0912, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2331, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2473, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0940, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1081, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1076, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0815, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2180, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2321, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0978, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.4974, grad_fn=<MeanBackward0>)\n",
      "tensor(0.5117, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0949, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1094, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1097, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1244, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0986, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0932, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1085, grad_fn=<AddBackward0>)\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.0900, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0956, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1117, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2344, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2505, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0985, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0912, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1074, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1071, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1232, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0849, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1010, grad_fn=<AddBackward0>)\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1374, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1534, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1042, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0827, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0900, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1060, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0996, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0912, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1225, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1385, grad_fn=<AddBackward0>)\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.0946, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1105, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0753, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0953, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1108, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0914, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1087, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1237, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1008, grad_fn=<AddBackward0>)\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0933, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0855, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0773, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0919, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1226, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1370, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0889, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1031, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1835, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1977, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0770, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0911, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1003, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1145, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0793, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0718, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0861, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1029, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1172, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0806, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0907, grad_fn=<AddBackward0>)\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0907, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0827, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0974, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1042, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1190, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0960, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0909, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0921, grad_fn=<AddBackward0>)\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0735, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2361, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2506, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0994, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0737, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0881, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0964, grad_fn=<AddBackward0>)\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0960, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0922, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0841, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0989, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1036, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1183, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1341, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1487, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1080, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "-\n",
      " tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1135, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0815, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0916, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0901, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0983, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0993, grad_fn=<AddBackward0>)\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0965, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0911, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1050, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2765, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0839, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.2020, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2165, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0873, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0722, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0865, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0965, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1109, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1102, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1247, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1925, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2069, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0800, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1097, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1094, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1244, grad_fn=<AddBackward0>)\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.1433, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1584, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2994, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0964, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1123, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1152, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1219, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1483, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1649, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0793, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0793, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1048, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1061, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1233, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0993, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1164, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1053, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1128, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1773, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1943, grad_fn=<AddBackward0>)\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.1054, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1222, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1150, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1318, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1008, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2344, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2513, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0839, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1011, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1442, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1615, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1753, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1928, grad_fn=<AddBackward0>)\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1099, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1161, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1715, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1366, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1547, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0927, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1076, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1262, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1081, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1269, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1081, grad_fn=<AddBackward0>)\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "-\n",
      " tensor(0.1030, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1220, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1114, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2137, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2329, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1077, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1045, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1217, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1408, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1136, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1325, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "-\n",
      " tensor(0.1275, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1460, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0977, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1161, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1192, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1374, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1022, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1052, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1235, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0798, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1175, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3141, grad_fn=<AddBackward0>)\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1119, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0925, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1111, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1040, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1226, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1155, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1340, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0941, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1127, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1038, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1224, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1397, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1585, grad_fn=<AddBackward0>)\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.0933, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1140, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1332, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0846, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1293, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1350, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1543, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0934, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1126, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0950, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1141, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0994, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1183, grad_fn=<AddBackward0>)\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0911, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1086, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1277, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0919, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0925, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1116, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0949, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1139, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1085, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2191, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2381, grad_fn=<AddBackward0>)\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "-\n",
      " tensor(0.0908, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1098, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1042, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1232, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1200, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1389, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2116, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2304, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0894, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1080, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0901, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1085, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0926, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.1044, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1229, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0982, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1169, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1173, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1360, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0987, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1928, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2113, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0857, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "-\n",
      " tensor(0.0988, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1170, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1373, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1554, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2714, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1112, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0921, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1409, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1601, grad_fn=<AddBackward0>)\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "-\n",
      " tensor(0.2625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2818, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1331, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1528, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0873, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1071, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1248, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1450, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1141, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1125, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1335, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1366, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1578, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1444, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1656, grad_fn=<AddBackward0>)\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "-\n",
      " tensor(0.1001, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1214, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1100, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1313, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2758, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1275, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1488, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1002, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1216, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1498, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1712, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1203, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1418, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1530, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1746, grad_fn=<AddBackward0>)\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "-\n",
      " tensor(0.1233, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1450, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1077, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1294, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1570, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1786, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0985, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1201, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1202, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1416, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2150, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2362, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0922, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1133, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0913, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1125, grad_fn=<AddBackward0>)\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "-\n",
      " tensor(0.0997, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1211, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1943, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2157, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1023, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1236, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1188, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1250, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1462, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3139, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3351, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2178, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2391, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1224, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1436, grad_fn=<AddBackward0>)\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "-\n",
      " tensor(0.1698, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1399, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1611, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1151, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1207, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1424, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1704, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1922, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1366, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1583, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1018, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1237, grad_fn=<AddBackward0>)\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "-\n",
      " tensor(0.1396, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1616, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1235, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1359, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1581, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1652, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1875, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0973, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1197, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0870, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1104, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1327, grad_fn=<AddBackward0>)\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1188, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1155, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1378, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1143, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1365, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1081, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1161, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1066, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1213, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1099, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1199, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1092, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0924, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1084, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0976, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1121, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1313, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0886, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0753, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1193, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1383, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0961, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1149, grad_fn=<AddBackward0>)\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1065, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0835, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0754, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1026, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0889, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1070, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, grad_fn=<AddBackward0>)\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0841, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1020, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1017, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0716, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0857, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1028, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "-\n",
      " tensor(0.1001, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1171, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0797, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0952, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0814, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0980, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1158, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1322, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0886, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1049, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0889, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1152, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1034, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0886, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0734, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0760, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0897, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1049, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1086, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0908, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1279, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1427, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2341, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2489, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0700, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0848, grad_fn=<AddBackward0>)\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0993, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1143, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1026, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0873, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1032, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1150, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1307, grad_fn=<AddBackward0>)\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1005, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0858, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1013, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1007, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1162, grad_fn=<AddBackward0>)\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1033, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1029, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1252, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0844, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0996, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0724, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0873, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0841, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0989, grad_fn=<AddBackward0>)\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0946, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0754, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0674, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0821, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1012, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0924, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1070, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0981, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1128, grad_fn=<AddBackward0>)\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0908, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0921, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0847, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1080, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1226, grad_fn=<AddBackward0>)\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0767, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0701, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0844, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0901, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1050, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1191, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1123, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, grad_fn=<AddBackward0>)\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1005, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0806, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0711, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0849, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0740, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0877, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0804, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0941, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0713, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0849, grad_fn=<AddBackward0>)\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0794, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0672, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0806, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0900, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0886, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0962, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1097, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0721, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0857, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0854, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0991, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0800, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0977, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0741, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0878, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0742, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1215, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1351, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0724, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1013, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0661, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0796, grad_fn=<AddBackward0>)\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0717, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0853, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0825, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1002, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1223, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1358, grad_fn=<AddBackward0>)\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0652, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0787, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1444, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1579, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0933, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1068, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0669, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0803, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0968, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1029, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1164, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1256, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1392, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0744, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0881, grad_fn=<AddBackward0>)\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0800, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1178, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1322, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1000, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0870, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0955, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0773, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0928, grad_fn=<AddBackward0>)\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.1264, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1421, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1022, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1181, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0926, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1087, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0824, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0987, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1180, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0886, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0949, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1119, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1005, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1176, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1270, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1322, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1493, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0921, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1090, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0959, grad_fn=<AddBackward0>)\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0885, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0971, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1023, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0977, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0919, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, grad_fn=<AddBackward0>)\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0911, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0777, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0904, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1028, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.1577, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1725, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0726, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0873, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0733, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0854, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0887, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1030, grad_fn=<AddBackward0>)\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0804, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0745, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0797, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n"
     ]
    }
   ],
   "source": [
    "fishers = []\n",
    "optParams = []\n",
    "ewc_lambda = 10\n",
    "\n",
    "models_D = []\n",
    "hist_losses_D = []\n",
    "hist_hitsss_D = []\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "print(model.apply(init_weights))\n",
    "\n",
    "for task_id in range(N_TASKS + TEST_ALL_TASKS):\n",
    "    SUFFIX = f\"D{task_id}\"\n",
    "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "    LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "    \n",
    "    hist_loss_temp, hist_hits_temp = fit_ewc(model, task_id, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "    hist_losses_D.append(hist_loss_temp)\n",
    "    hist_hitsss_D.append(hist_hits_temp)\n",
    "    models_D.append(copy.deepcopy(model))\n",
    "    onTaskUpdate_ewc(model, task_id, train_dls[task_id], F.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYEBTL-bo_Tr"
   },
   "outputs": [],
   "source": [
    "hist_loss_D = torch.cat(hist_losses_D, dim=2)\n",
    "hist_hits_D = torch.cat(hist_hitsss_D, dim=2)\n",
    "\n",
    "plotResults(hist_loss_D, hist_hits_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QY_YzfCoVkI"
   },
   "source": [
    "L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TVqno83vnWh"
   },
   "outputs": [],
   "source": [
    "hist_loss_D = torch.cat(hist_losses_D, dim=2)\n",
    "hist_hits_D = torch.cat(hist_hitsss_D, dim=2)\n",
    "\n",
    "plotResults(hist_loss_D, hist_hits_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0k0Kp66cvnWh"
   },
   "outputs": [],
   "source": [
    "accuracyAll(models_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17U8oYfQTlma"
   },
   "outputs": [],
   "source": [
    "torch.max(fishers[0]['encoder.embedding.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DnwXlOTXsbM"
   },
   "source": [
    "## Transfer D2: L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iatypwWYytb"
   },
   "source": [
    "### onTaskUpdate_l2reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wzf8QezfX4rt"
   },
   "outputs": [],
   "source": [
    "def onTaskUpdate_l2reg(model, task_id, train_dl, criterion):\n",
    "    # Save optimal parameters for each task\n",
    "    optParams.append({})\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        optParams[task_id][name] = param.data.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-GTTipSYufm"
   },
   "source": [
    "### train_l2reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfX9ClLaYNpv"
   },
   "outputs": [],
   "source": [
    "def train_l2reg(model, task_id, dataloader, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seq, seq_len in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(seq, seq_len, seq)\n",
    "        loss = criterion(output, seq)\n",
    "        \n",
    "        # L2 Training penalty\n",
    "        for other_task_id in range(task_id):\n",
    "            for name, param in model.named_parameters():\n",
    "                optParam = optParams[other_task_id][name]\n",
    "                loss += LAMBDA_L2REG * (optParam - param).pow(2).sum()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGgyR2yZYDfS"
   },
   "source": [
    "### eval_l2reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RsqsxlvY86I"
   },
   "outputs": [],
   "source": [
    "def evaluate_l2reg(model, task_id, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for seq, seq_len in dataloader:\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0) #turn off teacher forcing\n",
    "\n",
    "            loss = criterion(output, seq).type(torch.float)\n",
    "            \n",
    "            # L2 Training penalty\n",
    "            for other_task_id in range(task_id):\n",
    "                for name, param in model.named_parameters():\n",
    "                    optParam = optParams[other_task_id][name]\n",
    "                    loss += LAMBDA_L2REG * (optParam - param).pow(2).sum()\n",
    "                    \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfwySP1EZNc_"
   },
   "source": [
    "### fit_l2reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIQ9dV7WZPX6"
   },
   "outputs": [],
   "source": [
    "def fit_l2reg(model, task_id, epochs, step_size_evaluation, clip ):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train_l2reg(model, task_id, train_dls[task_id], optimizer, criterion, clip)\n",
    "        valid_loss = evaluate_l2reg(model, task_id, valid_dls[task_id], criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            for other_id in range(task_id + 1):\n",
    "                total_loss[other_id,0,idx] = evaluate_l2reg(model, task_id, train_dls[other_id], criterion)\n",
    "                total_loss[other_id,1,idx] = evaluate_l2reg(model, task_id, test_dls[other_id], criterion)\n",
    "                total_loss[other_id,2,idx] = evaluate_l2reg(model, task_id, test_ugr_dls[other_id], criterion)\n",
    "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
    "\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWITsrZDZz1z"
   },
   "source": [
    "### Experiment L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bki7JTypZT9E"
   },
   "outputs": [],
   "source": [
    "LAMBDA_L2REG = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08h_J3oUZ5-x"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7MObEkiZ6pa",
    "outputId": "ab9a9f0e-bb75-472d-ea4b-7dbb17755167",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.585 | Train PPL:   1.796\n",
      "\t Val. Loss: 0.494 |  Val. PPL:   1.639\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.479 | Train PPL:   1.614\n",
      "\t Val. Loss: 0.452 |  Val. PPL:   1.571\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.446 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.384 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.506\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 0.415 |  Val. PPL:   1.515\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.533\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.509\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.503\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.450\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.352\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.431\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.428\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.348\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.266\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.218\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.596 | Train PPL:   1.814\n",
      "\t Val. Loss: 0.494 |  Val. PPL:   1.639\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.478 | Train PPL:   1.613\n",
      "\t Val. Loss: 0.472 |  Val. PPL:   1.604\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.469 | Train PPL:   1.598\n",
      "\t Val. Loss: 0.458 |  Val. PPL:   1.581\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.478 | Train PPL:   1.612\n",
      "\t Val. Loss: 0.450 |  Val. PPL:   1.569\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.451 |  Val. PPL:   1.570\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.577\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.460 |  Val. PPL:   1.583\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.577\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.469 | Train PPL:   1.598\n",
      "\t Val. Loss: 0.501 |  Val. PPL:   1.651\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.578\n",
      "\t Val. Loss: 0.478 |  Val. PPL:   1.613\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.569\n",
      "\t Val. Loss: 0.473 |  Val. PPL:   1.605\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.492 |  Val. PPL:   1.635\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.500 | Train PPL:   1.650\n",
      "\t Val. Loss: 0.472 |  Val. PPL:   1.604\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.487\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.577\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.579\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.587\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.481 | Train PPL:   1.618\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.587\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.435 | Train PPL:   1.545\n",
      "\t Val. Loss: 0.498 |  Val. PPL:   1.646\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.459 | Train PPL:   1.583\n",
      "\t Val. Loss: 0.475 |  Val. PPL:   1.608\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.428 | Train PPL:   1.535\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.583\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.463 |  Val. PPL:   1.589\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.542\n",
      "\t Val. Loss: 0.521 |  Val. PPL:   1.683\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.445 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.467 |  Val. PPL:   1.595\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.467 | Train PPL:   1.596\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.583\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.425 | Train PPL:   1.530\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.580\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.483 |  Val. PPL:   1.621\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.500\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.580\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.467 | Train PPL:   1.596\n",
      "\t Val. Loss: 0.498 |  Val. PPL:   1.646\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.555\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.596\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.563\n",
      "\t Val. Loss: 0.479 |  Val. PPL:   1.614\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.437 | Train PPL:   1.548\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.578\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.444 | Train PPL:   1.559\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.524\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.482 | Train PPL:   1.619\n",
      "\t Val. Loss: 0.509 |  Val. PPL:   1.663\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.477 | Train PPL:   1.611\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.580\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.463 | Train PPL:   1.589\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.542\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.516\n",
      "\t Val. Loss: 0.476 |  Val. PPL:   1.610\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.541\n",
      "\t Val. Loss: 0.473 |  Val. PPL:   1.605\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.503\n",
      "\t Val. Loss: 0.497 |  Val. PPL:   1.644\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.450 | Train PPL:   1.568\n",
      "\t Val. Loss: 0.450 |  Val. PPL:   1.568\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.418 | Train PPL:   1.519\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.504\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.561\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.479 | Train PPL:   1.614\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.474 | Train PPL:   1.607\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.588\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.561\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.556\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.504\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.541\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.437 | Train PPL:   1.548\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.550\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.541\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.454 | Train PPL:   1.574\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.563\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.518\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.500\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.558\n",
      "\t Val. Loss: 0.451 |  Val. PPL:   1.570\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.476 | Train PPL:   1.609\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.556\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.455 |  Val. PPL:   1.577\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.558\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.538\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.573\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.499 |  Val. PPL:   1.647\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.474 | Train PPL:   1.607\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.578\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.580\n",
      "\t Val. Loss: 0.450 |  Val. PPL:   1.569\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.580\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.558\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.519\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.515\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.396 | Train PPL:   1.486\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.539\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.508\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.518\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.502\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.523\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.518\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.543\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.525\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.560\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.525 | Train PPL:   1.690\n",
      "\t Val. Loss: 0.486 |  Val. PPL:   1.626\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.604\n",
      "\t Val. Loss: 0.455 |  Val. PPL:   1.577\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.448 | Train PPL:   1.566\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.587\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.485 | Train PPL:   1.624\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.435 | Train PPL:   1.544\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.541\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.563\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.540\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.508\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.463 | Train PPL:   1.589\n",
      "\t Val. Loss: 0.505 |  Val. PPL:   1.657\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.461 | Train PPL:   1.586\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.560\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.503\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.552\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.550\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.555\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.449 | Train PPL:   1.566\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.428 | Train PPL:   1.535\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.530\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.498\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.532\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.564\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.547\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.448 | Train PPL:   1.565\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.467 | Train PPL:   1.595\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.524\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.434 | Train PPL:   1.543\n",
      "\t Val. Loss: 0.441 |  Val. PPL:   1.554\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.523\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.521\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.409 | Train PPL:   1.506\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.511\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.456\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.551\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.500\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.494\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.498\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.521\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.424 |  Val. PPL:   1.528\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.455 | Train PPL:   1.576\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.521\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.551\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.532\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.413 | Train PPL:   1.512\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.511\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.496\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.524\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.503\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.477 | Train PPL:   1.611\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.506\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.426 | Train PPL:   1.531\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.505\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.445 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.520\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.454 | Train PPL:   1.575\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.543\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.542\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.412 | Train PPL:   1.510\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.543\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.532\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.429 | Train PPL:   1.536\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.473\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.459 | Train PPL:   1.583\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.468\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.526\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.425\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.511\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.525\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.506\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.414 | Train PPL:   1.513\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.475 |  Val. PPL:   1.608\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.527\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.409 | Train PPL:   1.506\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.479\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.515\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.489\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.419 | Train PPL:   1.521\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.502\n",
      "\t Val. Loss: 0.482 |  Val. PPL:   1.619\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.580\n",
      "\t Val. Loss: 0.494 |  Val. PPL:   1.639\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.487 | Train PPL:   1.628\n",
      "\t Val. Loss: 0.491 |  Val. PPL:   1.635\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.468 | Train PPL:   1.597\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.419 | Train PPL:   1.520\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.555\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.421 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.393 | Train PPL:   1.482\n",
      "\t Val. Loss: 0.469 |  Val. PPL:   1.599\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.438 | Train PPL:   1.549\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.573\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.524\n",
      "\t Val. Loss: 0.443 |  Val. PPL:   1.557\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.526\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.597\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.566\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.458 |  Val. PPL:   1.582\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.489 |  Val. PPL:   1.631\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
      "\t Val. Loss: 0.454 |  Val. PPL:   1.575\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.419 | Train PPL:   1.520\n",
      "\t Val. Loss: 0.472 |  Val. PPL:   1.603\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.564\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.454 |  Val. PPL:   1.574\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.455 | Train PPL:   1.576\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.582\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.540\n",
      "\t Val. Loss: 0.481 |  Val. PPL:   1.618\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.597\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.451\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.475\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.432 | Train PPL:   1.540\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.443 |  Val. PPL:   1.557\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.516\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.555\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.421 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.505\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.489\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.543\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.516\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.464 |  Val. PPL:   1.590\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.406 | Train PPL:   1.501\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.540\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.532\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.458 |  Val. PPL:   1.582\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.456\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
      "\t Val. Loss: 0.431 |  Val. PPL:   1.539\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.519\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.493 |  Val. PPL:   1.637\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.451 |  Val. PPL:   1.570\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.564\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.451 |  Val. PPL:   1.571\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.541\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.421 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.537\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.579\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.490\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.438 | Train PPL:   1.550\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.549\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.489\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.484\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.523\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.551\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.587\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.453 | Train PPL:   1.573\n",
      "\t Val. Loss: 0.528 |  Val. PPL:   1.696\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.458 |  Val. PPL:   1.580\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.414 | Train PPL:   1.512\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.446 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.560\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.416\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.389\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.424 |  Val. PPL:   1.529\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.410\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.482\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.428\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.406\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.407\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.421\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.425 | Train PPL:   1.529\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.389\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.440\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.386\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.456\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.431\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.407\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.417\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.400\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.481\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.427\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.418\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.411\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.404\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.430\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.413\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.488\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.393\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.441\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.413\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.404\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.417\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.431\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.410\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.437\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.406\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.424\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.456\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.498\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.431\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.463\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.413\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.420\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.465\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.384 | Train PPL:   1.468\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.417\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.441\n"
     ]
    }
   ],
   "source": [
    "optParams = []\n",
    "\n",
    "models_D2 = []\n",
    "hist_losses_D2 = []\n",
    "hist_hitsss_D2 = []\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "print(model.apply(init_weights))\n",
    "\n",
    "for task_id in range(N_TASKS + TEST_ALL_TASKS):\n",
    "    SUFFIX = f\"D2.{task_id}\"\n",
    "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "    LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "    \n",
    "    hist_loss_temp, hist_hits_temp = fit_l2reg(model, task_id, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "    hist_losses_D2.append(hist_loss_temp)\n",
    "    hist_hitsss_D2.append(hist_hits_temp)\n",
    "    models_D2.append(copy.deepcopy(model))\n",
    "    onTaskUpdate_l2reg(model, task_id, train_dls[task_id], F.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "ITGjKqf6c-Bt",
    "outputId": "d49eb2d3-0c6b-4b66-fc1a-744da9729248"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss_D2 = torch.cat(hist_losses_D2, dim=2)\n",
    "hist_hits_D2 = torch.cat(hist_hitsss_D2, dim=2)\n",
    "\n",
    "plotResults(hist_loss_D2, hist_hits_D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YEnIhlRbdPUs",
    "outputId": "36ab6809-9d23-4399-c9b8-c03a0bb6a3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.56% | Gr acc 0.12 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwQE8R-QvnWj"
   },
   "source": [
    "## Transfer E: DynaMoE\n",
    "\n",
    "1. Create DynaMoe network functions:\n",
    "2. Gating\n",
    "3. Run experiment\n",
    "\n",
    "Todos:\n",
    "- Training by computing loss of every expert is cheaty?\n",
    "- Training by feeding all new inputs to new expert is cheaty?\n",
    "- Experts only train on what gating gives them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiVkxi06Nu8y"
   },
   "source": [
    "### Gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "JFB0iAdSvnWk"
   },
   "outputs": [],
   "source": [
    "class Gating(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, n_gating_hidden, n_experts,\n",
    "                 n_max_experts, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.n_experts = n_experts\n",
    "\n",
    "        self.n_max_experts = n_max_experts\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(embed_dim, n_gating_hidden, bidirectional=True)\n",
    "\n",
    "        self.fc_out = nn.Linear(n_gating_hidden * 2, n_max_experts)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, seqs, seqs_len):\n",
    "        \n",
    "        # seqs = [seq len, batch_size]\n",
    "        # seqs_len = [batch_size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(seqs))\n",
    "        \n",
    "        # embedded = [seq len, batch_size, embed_dim]\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, seqs_len.to(\"cpu\"))\n",
    "\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "\n",
    "        # outputs = [seq len, batch_size, n_experts * num directions]\n",
    "        # hidden = [n layers * num directions, batch size, n_experts]\n",
    "\n",
    "        hidden = hidden.squeeze(0)\n",
    "\n",
    "        # hidden = [batch_size, n_max_experts]\n",
    "\n",
    "        outputs = outputs[-1]\n",
    "\n",
    "        outputs = self.fc_out(outputs)\n",
    "\n",
    "        # outputs = [batch_size, n_max_experts]\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pL7WzzmNqBp"
   },
   "source": [
    "### DynaMoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Ysf64peLvnWj"
   },
   "outputs": [],
   "source": [
    "class DynaMoE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gating=None,\n",
    "        gating_optimizer=None,\n",
    "        experts=None,\n",
    "        expert_optimizers=None,\n",
    "        status=\"train_gating_initialized_expert\",\n",
    "        expert_decay=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        gating: nn.Module\n",
    "            Gating module\n",
    "        gating_optimizer: optim\n",
    "            optimizer for passed Gating module\n",
    "        expert: list of nn.Module\n",
    "            list of task experts\n",
    "        expert_optimizers: list of optim\n",
    "            list of optimizer for the expert at the same index\n",
    "        status : string\n",
    "            \"train_gating_uninitialized_expert\" | \"train_gating_train_expert\" | \n",
    "            \"train_gating_initialized_expert\"\n",
    "        \"\"\"\n",
    "        super(DynaMoE, self).__init__()\n",
    "\n",
    "        assert (gating is None) == (gating_optimizer is None), \"gating needs gating_optimizer, and vice versa\"\n",
    "        \n",
    "        if gating is None:\n",
    "            gating, gating_optimizer = init_gating()\n",
    "        self.gating = gating\n",
    "        self.gating_optimizer = gating_optimizer\n",
    "        \n",
    "        if experts is None:\n",
    "            expert, expert_optimizer = init_expert()\n",
    "            experts = [expert,]\n",
    "            expert_optimizers = [expert_optimizer,]\n",
    "        \n",
    "        assert len(experts) == len(expert_optimizers), \"unequal amount of experts and expert_optimizers\"\n",
    "\n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        self.expert_optimizers = expert_optimizers\n",
    "            \n",
    "        self.n_train_on_experts = [0 for _ in experts]   # List how often expert has been trained\n",
    "        self.n_active_experts = len(experts)\n",
    "        self.n_max_experts = gating.n_max_experts\n",
    "        self.status = status\n",
    "        \n",
    "        self.expert_decay = expert_decay\n",
    "        if self.expert_decay:\n",
    "            # Initialize Scheduler for expert decay\n",
    "            initScheduler = lambda opt: optim.lr_scheduler.StepLR(opt,\n",
    "                                                                  step_size=STEP_SIZE_DECAY,\n",
    "                                                                  gamma=GAMMA_DECAY)\n",
    "            self.expert_schedulers = [initScheduler(opt) for opt in expert_optimizers]\n",
    "\n",
    "    def forward(self, seqs, seqs_len, trgs, teacher_forcing_ratio=0.5):\n",
    "        #seqs = [seqs len, batch size]\n",
    "        #seqs_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "\n",
    "        vocab_size = self.gating.input_dim\n",
    "        seq_len, batch_size = seqs.shape\n",
    "        \n",
    "        # Decide which expert to use\n",
    "        gatings = self.gating(seqs, seqs_len)\n",
    "\n",
    "        # gatings = [batch_size, n_max_experts]\n",
    "        \n",
    "        masked_gatings = gatings[:,:self.n_active_experts]\n",
    "        \n",
    "        # @TODO: Probabilistic vs argmax?\n",
    "        network_ids = torch.argmax(masked_gatings, dim=1)\n",
    "\n",
    "        expert_outputs = []\n",
    "        for e_id in range(self.n_active_experts):\n",
    "            expert_outputs.append(self.experts[e_id](seqs, seqs_len, seqs,\n",
    "                                                     teacher_forcing_ratio))\n",
    "\n",
    "        outputs = torch.empty((seq_len, batch_size, vocab_size))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            network_id = network_ids[b]\n",
    "            outputs[:,b] = expert_outputs[network_id][:,b]\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "\n",
    "    def add_expert(self):\n",
    "        # Get new expert\n",
    "        expert, expert_optimizer = init_expert()\n",
    "        self.experts.append(expert)\n",
    "        self.expert_optimizers.append(expert_optimizer)\n",
    "        self.n_active_experts += 1\n",
    "        self.n_train_on_experts.append(0)\n",
    "        if self.expert_decay:\n",
    "            self.expert_schedulers.append(\n",
    "                optim.lr_scheduler.StepLR(expert_optimizer, \n",
    "                                          step_size=STEP_SIZE_DECAY,\n",
    "                                          gamma=GAMMA_DECAY)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITHSpNoENze8"
   },
   "source": [
    "### compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "9Opa6EZlw8g-"
   },
   "outputs": [],
   "source": [
    "def compute_loss(outputs, targets, criterion, cutFirstInSequence=True):\n",
    "    if isinstance(criterion, CosineLoss):\n",
    "        return criterion(outputs, targets)\n",
    "    elif criterion == allOrNoneLoss:\n",
    "        return criterion(outputs, targets) / targets.shape[1]\n",
    "    else:\n",
    "        outputs_dim = outputs.shape[-1]\n",
    "        \n",
    "        if cutFirstInSequence:\n",
    "            outputs = outputs[1:].view(-1, outputs_dim)\n",
    "            #outputs = [batch size, output dim]\n",
    "            targets = targets[1:].view(-1)\n",
    "            #targets = [batch size]\n",
    "            # print(\"hi\")\n",
    "        else:\n",
    "            outputs = outputs.view(-1, outputs_dim)\n",
    "            targets = targets.view(-1)\n",
    "        \n",
    "        # print(\"######\")\n",
    "        # print(outputs)\n",
    "        # print(targets)\n",
    "        # print(\"######\")\n",
    "        \n",
    "        return criterion(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K75xVqKJN2Km"
   },
   "source": [
    "### train_dynamoe_gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "icuVPvGqYI75"
   },
   "outputs": [],
   "source": [
    "def train_dynamoe_gating(\n",
    "    model,\n",
    "    iterator,\n",
    "    gating_criterion,\n",
    "    expert_criterion_unreduced,\n",
    "    clip,\n",
    "    verbose=False,\n",
    "    returnGatingChoices=False\n",
    "):\n",
    "    \n",
    "    model.gating.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    gating_choices_tracker = []\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        \n",
    "        #seqs = [seq len, batch size]\n",
    "        #output = [seq len, batch size, vocab sizen]\n",
    "        batch_size = seqs.shape[1]\n",
    "\n",
    "        model.gating_optimizer.zero_grad()\n",
    "        \n",
    "        gating_outputs = model.gating(seqs, seqs_len)\n",
    "\n",
    "        # gating_outputs = [batch_size, n_max_experts]\n",
    "\n",
    "        ## Compute best choice for gating network\n",
    "        # Compute loss for each expert network\n",
    "        loss_experts = torch.empty((batch_size, model.n_active_experts))\n",
    "        expert_trgs = seqs\n",
    "        for e_id in range(model.n_active_experts):\n",
    "\n",
    "            model.experts[e_id].eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Get model prediction\n",
    "                expert_outputs = model.experts[e_id](seqs, seqs_len,\n",
    "                                                     expert_trgs)\n",
    "\n",
    "                loss = compute_loss(expert_outputs, expert_trgs,\n",
    "                                    expert_criterion_unreduced,\n",
    "                                    cutFirstInSequence=True)\n",
    "            \n",
    "            # Log loss to train gating\n",
    "            loss_experts[:,e_id] = loss\n",
    "\n",
    "        # Indices of correct experts to have chosen\n",
    "        gating_trgs = loss_experts.argmin(dim=1)\n",
    "        # gating_trgs = [batch_size]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Target\")\n",
    "            print(gating_trgs)\n",
    "            print(\"Output\")\n",
    "            print(gating_outputs.argmax(dim=1))\n",
    "\n",
    "        gating_trgs = gating_trgs.unsqueeze(0)\n",
    "        # gating_trgs = [[batch_size]]\n",
    "        gating_outputs = gating_outputs.unsqueeze(0)\n",
    "        # gating_ouputs = [[batch_size, n_max_experts]]\n",
    "\n",
    "        gating_loss = compute_loss(gating_outputs, gating_trgs,\n",
    "                                   gating_criterion,\n",
    "                                   cutFirstInSequence=False)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\">> Gating Loss\")\n",
    "            print(gating_loss)\n",
    "\n",
    "        gating_loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        model.gating_optimizer.step()\n",
    "        \n",
    "        # Get loss of model gating chose\n",
    "        gating_masked = gating_outputs.squeeze(0)[:,:model.n_active_experts]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"-- Masked Gating\")\n",
    "            print(gating_masked)\n",
    "        gating_choices = gating_masked.argmax(dim=1)\n",
    "        # gating_choices = [batch_size]\n",
    "\n",
    "        loss_chosen_experts = loss_experts[:,gating_choices]\n",
    "        \n",
    "        loss_chosen_experts = loss_chosen_experts.mean()\n",
    "        \n",
    "        epoch_loss += loss_chosen_experts.item()\n",
    "        \n",
    "        # save gating choices\n",
    "        if returnGatingChoices:\n",
    "            gating_choices_tracker.append(gating_choices)\n",
    "    \n",
    "    ret_loss = epoch_loss / len(iterator)\n",
    "    \n",
    "    if returnGatingChoices:\n",
    "        return ret_loss, np.concatenate(gating_choices_tracker)\n",
    "    return ret_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbbZ_pYyN48v"
   },
   "source": [
    "### train_dynamoe_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "MaaIrZLCE1JG"
   },
   "outputs": [],
   "source": [
    "def train_dynamoe_both(model, iterator, gating_criterion,\n",
    "                       expert_criterion_unreduced, clip):\n",
    "\n",
    "    model.gating.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        #seqs = [seq len, batch size]\n",
    "        #output = [seq len, batch size, vocab sizen]\n",
    "\n",
    "        batch_size = seqs.shape[1]\n",
    "\n",
    "        model.gating_optimizer.zero_grad()\n",
    "        \n",
    "        gating_outputs = model.gating(seqs, seqs_len)\n",
    "        # gating_outputs = [batch_size, n_max_experts]\n",
    "\n",
    "        ## Compute best choice for gating network\n",
    "        # Compute loss for each expert network\n",
    "        loss_experts = torch.empty((batch_size, model.n_active_experts))\n",
    "        # loss_experts = [batch_size, n_active_experts]\n",
    "        expert_trgs = seqs\n",
    "        for e_id in range(model.n_active_experts):\n",
    "\n",
    "            train_model = e_id == model.n_active_experts - 1\n",
    "\n",
    "            if train_model:\n",
    "                model.experts[e_id].train()\n",
    "                model.expert_optimizers[e_id].zero_grad()\n",
    "                \n",
    "                # Get model prediction\n",
    "                expert_outputs = model.experts[e_id](seqs, seqs_len,\n",
    "                                                     expert_trgs)\n",
    "\n",
    "                loss = compute_loss(expert_outputs, expert_trgs,\n",
    "                                    expert_criterion_unreduced,\n",
    "                                    cutFirstInSequence=True)\n",
    "\n",
    "                # Log loss to train gating\n",
    "                loss_experts[:,e_id] = loss\n",
    "                \n",
    "                # Train newly initialized model on new train examples\n",
    "                reduced_loss = loss.mean()\n",
    "                reduced_loss.backward()\n",
    "                model.expert_optimizers[e_id].step()\n",
    "                \n",
    "            else:\n",
    "                model.experts[e_id].eval()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "\n",
    "                    # Get model prediction\n",
    "                    expert_outputs = model.experts[e_id](seqs, seqs_len,\n",
    "                                                         expert_trgs)\n",
    "\n",
    "                    loss = compute_loss(expert_outputs, expert_trgs,\n",
    "                                        expert_criterion_unreduced,\n",
    "                                        cutFirstInSequence=True)\n",
    "\n",
    "                    # Log loss to train gating\n",
    "                    loss_experts[:,e_id] = loss\n",
    "\n",
    "        # Compute expert which should have been chosen\n",
    "        gating_trgs = loss_experts.argmin(dim=1)\n",
    "        # gating_trgs = [batch_size]\n",
    "\n",
    "        gating_trgs = gating_trgs.unsqueeze(0)\n",
    "        # gating_trgs = [[batch_size]]\n",
    "        gating_outputs = gating_outputs.unsqueeze(0)\n",
    "        # gating_ouputs = [[batch_size, n_max_experts]]\n",
    "\n",
    "        gating_loss = compute_loss(gating_outputs, gating_trgs, gating_criterion,\n",
    "                            cutFirstInSequence=False)\n",
    "\n",
    "        gating_loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        model.gating_optimizer.step()\n",
    "\n",
    "        # Get loss of model gating chose\n",
    "        gating_masked = gating_outputs.squeeze(0)[:,:model.n_active_experts]\n",
    "        gating_choices = gating_masked.argmax(dim=1)\n",
    "        # gating_choices = [batch_size]\n",
    "\n",
    "        loss_chosen_experts = loss_experts[:,gating_choices]\n",
    "        \n",
    "        loss_chosen_experts = loss_chosen_experts.mean()\n",
    "        \n",
    "        epoch_loss += loss_chosen_experts.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_dynamoe_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_dynamoe_expert(model, expert_id, iterator, expert_criterion, clip):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        #seqs = [seq len, batch size]\n",
    "        #output = [seq len, batch size, vocab sizen]\n",
    "\n",
    "        batch_size = seqs.shape[1]\n",
    "\n",
    "        model.experts[expert_id].train()\n",
    "        model.expert_optimizers[expert_id].zero_grad()\n",
    "\n",
    "        # Get model prediction\n",
    "        expert_outputs = model.experts[expert_id](seqs, seqs_len,\n",
    "                                                  seqs)\n",
    "\n",
    "        loss = compute_loss(expert_outputs, seqs,\n",
    "                            expert_criterion,\n",
    "                            cutFirstInSequence=True)\n",
    "\n",
    "        # Train newly initialized model on new train examples\n",
    "        loss.backward()\n",
    "        model.expert_optimizers[expert_id].step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQrjb4QEODx2",
    "tags": []
   },
   "source": [
    "### fit_dynamoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "tuFUOzDwjZgI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_dynamoe(\n",
    "    n_tasks_total,\n",
    "    model,\n",
    "    task_id, \n",
    "    epochs,\n",
    "    step_size_evaluation,\n",
    "    gating_criterion,\n",
    "    expert_criterion,\n",
    "    expert_criterion_unreduced,\n",
    "    clip=1,\n",
    "    repetition=None\n",
    "):\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.empty((n_tasks_total, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.empty((n_tasks_total, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    loss_tracker = torch.zeros((epochs,))\n",
    "\n",
    "    allowed_until_check = N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # First Epoch log performance BEFORE training\n",
    "        if epoch == 0:\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, epoch, expert_criterion)\n",
    "        \n",
    "        # Train model depending on its status\n",
    "        if model.status == \"train_gating_initialized_expert\":\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_dynamoe_gating(model, train_dls[task_id],\n",
    "                                              gating_criterion,\n",
    "                                              expert_criterion_unreduced, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], expert_criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Log hits\n",
    "            loss_tracker[epoch] = evaluate_extra(model, train_dls[task_id],\n",
    "                                                 allOrNoneLoss)\n",
    "\n",
    "            # Check for improvement in loss\n",
    "            if epoch > allowed_until_check:\n",
    "                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)\n",
    "                    and \n",
    "                    (valid_loss > PERFORMANCE_TRESHHOLD_START)\n",
    "                ):\n",
    "                    # Case of no improvement:\n",
    "                    \n",
    "                    # Switch to train the expert and gating\n",
    "                    model.status = \"train_gating_train_expert\"\n",
    "                    allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                    \n",
    "                    model.last_loss = loss_tracker[epoch]\n",
    "                    \n",
    "                    print(\"-----------------------------------\")\n",
    "                    print(\"------Switch to training both------\")\n",
    "                    print(\"-----------------------------------\")\n",
    "\n",
    "            \n",
    "        if model.status == \"train_gating_train_expert\":\n",
    "            switch_status = False\n",
    "            # Lookahead check whether data from another task is coming in\n",
    "            loss_ahead = evaluate_extra(model, valid_dls[task_id],\n",
    "                                        allOrNoneLoss)\n",
    "            if loss_ahead - PERFORMANCE_DIFFERENCE_NEW_TASK > model.last_loss:\n",
    "                switch_status = True\n",
    "            else:\n",
    "                start_time = time.time()\n",
    "\n",
    "                # First train gating\n",
    "                train_loss, gatingChoices = train_dynamoe_gating(model,\n",
    "                                                                 train_dls[task_id],\n",
    "                                                                 gating_criterion,\n",
    "                                                                 expert_criterion_unreduced,\n",
    "                                                                 clip,\n",
    "                                                                 returnGatingChoices=True)\n",
    "                \n",
    "                # Check whether gating chose something different\n",
    "                if (not all(gatingChoices == model.n_active_experts - 1) and\n",
    "                    epoch > allowed_until_check):\n",
    "                    # Abort, we have a sequence from a different task!\n",
    "                    switch_status = True\n",
    "                else:\n",
    "                    expert_id = model.n_active_experts - 1\n",
    "                    train_loss = train_dynamoe_expert(model, expert_id, train_dls[task_id], expert_criterion, clip)\n",
    "                    valid_loss = evaluate(model, valid_dls[task_id],\n",
    "                                          expert_criterion)\n",
    "\n",
    "                    end_time = time.time()\n",
    "\n",
    "                    model.last_loss = evaluate_extra(model, valid_dls[task_id],\n",
    "                                                     allOrNoneLoss)\n",
    "            if switch_status:\n",
    "                # Switch mode and \"consolidate\" expert\n",
    "                model.status = \"train_gating_uninitialized_expert\"\n",
    "                allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                \n",
    "                print(\"-----------------------------------\")\n",
    "                print(\"-- Fix Expert, Train Gating only --\")\n",
    "                print(\"-----------------------------------\")\n",
    "                # (this runs into the next if and executes that case)\n",
    "\n",
    "        if model.status == \"train_gating_uninitialized_expert\":\n",
    "            # assert len(model.experts) > 0, \"Need at least one expert\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_dynamoe_gating(model, train_dls[task_id],\n",
    "                                              gating_criterion,\n",
    "                                              expert_criterion_unreduced, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], expert_criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Log loss\n",
    "            loss_tracker[epoch] = valid_loss\n",
    "\n",
    "            # Check for improvement in loss\n",
    "            if epoch > allowed_until_check:\n",
    "                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)\n",
    "                    and \n",
    "                    (valid_loss > PERFORMANCE_TRESHHOLD_START)\n",
    "                   ):\n",
    "                    # Case of no improvement:\n",
    "                    # Initiate new expert and train gating and new expert on it\n",
    "                    model.add_expert()\n",
    "\n",
    "                    model.status = \"train_gating_initialized_expert\"\n",
    "                    allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                    \n",
    "                    print(\"-----------------------------------\")\n",
    "                    print(\"-----Added Expert-train Gating-----\")\n",
    "                    print(\"-----------------------------------\")\n",
    "\n",
    "            \n",
    "#         if valid_loss < best_valid_loss:\n",
    "#             best_valid_loss = valid_loss\n",
    "#             torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        # Log performance AFTER training\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0 and epoch != 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, idx, expert_criterion)\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if repetition is not None:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | R{repetition} T{task_id}')\n",
    "        else:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | T{task_id}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCiyq9ODN9fG"
   },
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Y4hPup6qpRWp"
   },
   "outputs": [],
   "source": [
    "def init_expert():\n",
    "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "    new_expert = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "    new_expert.apply(init_weights)\n",
    "    expert_optimizer = optim.Adam(new_expert.parameters(), lr=LEARNING_RATE)\n",
    "    return new_expert, expert_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "-gPFPMm_JiO4"
   },
   "outputs": [],
   "source": [
    "def init_gating():\n",
    "    gating = Gating(INPUT_DIM, N_GATING_EMBED_DIM, N_GATING_HIDDEN_DIM,\n",
    "                    N_EXPERTS_START, N_MAX_EXPERTS, GATE_DROPOUT)\n",
    "    gating.to(device)\n",
    "    gating_optimizer = optim.Adam(gating.parameters(), lr=LEARNING_RATE_GATING)\n",
    "    return gating, gating_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouWPz-2MdMs1"
   },
   "source": [
    "### show expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "jFW0tm8tdTAl"
   },
   "outputs": [],
   "source": [
    "def show_expert(model, iterator):\n",
    "    model.gating.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            seqs, seqs_len = batch\n",
    "\n",
    "            batch_size = seqs.shape[1]\n",
    "\n",
    "            gating_outputs = model.gating(seqs, seqs_len)\n",
    "\n",
    "            gating_masked = gating_outputs[:,:model.n_active_experts]\n",
    "\n",
    "            gating_choices = gating_masked.argmax(dim=1)\n",
    "\n",
    "            for b in range(batch_size):\n",
    "                print(f\"{gating_choices[b]} - {seqs[:,b]}\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dynamoe():\n",
    "    # Initialize DynaMoE\n",
    "    model = DynaMoE()\n",
    "    print(model.apply(init_weights))\n",
    "\n",
    "    # gating_criterion = CosineLoss(N_MAX_EXPERTS, ignore_index=None)\n",
    "    # Cosine loss is inpractical for Gating because result vectors are low dimensional\n",
    "    # Cosine loss works better for small datasets, thus used for experts\n",
    "    gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    expert_criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)\n",
    "    expert_criterion_unreduced = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN,\n",
    "                                            reduction=\"none\")\n",
    "    \n",
    "    return (model, gating_criterion, expert_criterion, expert_criterion_unreduced)\n",
    "\n",
    "def repeat_dynamoe(n_tasks_total, n_task_epochs, task_id, step_size_evaluation, repetition, pass_on_variables):\n",
    "    model, gating_criterion, expert_criterion, expert_criterion_unreduced = pass_on_variables\n",
    "    hist_loss, hist_hits = fit_dynamoe(\n",
    "        n_tasks_total,\n",
    "        model,\n",
    "        task_id,\n",
    "        n_task_epochs,\n",
    "        step_size_evaluation,\n",
    "        gating_criterion,\n",
    "        expert_criterion,\n",
    "        expert_criterion_unreduced,\n",
    "        repetition=repetition\n",
    "    )\n",
    "    return hist_loss, hist_hits, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlpvLkEVfZhR"
   },
   "source": [
    "### Experiment DynaMoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "CvFmPpKQozmz"
   },
   "outputs": [],
   "source": [
    "N_EXPERTS_START = 1\n",
    "N_MAX_EXPERTS = 3\n",
    "GATE_DROPOUT = 0.5\n",
    "N_GATING_HIDDEN_DIM = 10\n",
    "N_GATING_EMBED_DIM = 10\n",
    "SCHEDULE = not_interleaved\n",
    "\n",
    "# If the amount of missed hits is bigger then PERFORMANCE_TRESHHOLD_START\n",
    "# and it stays within ALLOWED_ERROR_VARIANCE for\n",
    "# N_EPOCHS_UNTIL_NEW_EXPERT epochs, then a new\n",
    "# expert is initialized\n",
    "N_EPOCHS_UNTIL_NEW_EXPERT = 30\n",
    "ALLOWED_ERROR_VARIANCE = 0.1\n",
    "PERFORMANCE_TRESHHOLD_START = 0.3\n",
    "\n",
    "# Difference between replicated sequence accuracy of training sequence\n",
    "# before and training sequence coming in to decide to consolidate for\n",
    "# a new expert.\n",
    "PERFORMANCE_DIFFERENCE_NEW_TASK = 0.3 # 0.5 for bad example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "dUUt4knHYfDj"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRoGUVZcfgMg",
    "outputId": "2c57ceea-2add-4c03-f0b3-5a9252f522d0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@ Repetition   0 @@@@@@@@@\n",
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(19, 9, bidirectional=True)\n",
      "        (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "          (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(37, 9)\n",
      "        (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: DynaMoE-1.s0.t0.e400\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.109 | Train PPL:   3.032\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.079 | Train PPL:   2.941\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.060 | Train PPL:   2.885\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.957\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.068 | Train PPL:   2.910\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.934\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.098 | Train PPL:   2.999\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.057 | Train PPL:   2.877\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.070 | Train PPL:   2.914\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.047 | Train PPL:   2.849\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.102 | Train PPL:   3.011\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.119 | Train PPL:   3.061\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.072 | Train PPL:   2.921\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.047 | Train PPL:   2.850\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.081 | Train PPL:   2.948\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.120 | Train PPL:   3.066\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.105 | Train PPL:   3.020\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.019 | Train PPL:   2.770\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.075 | Train PPL:   2.931\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.039 | Train PPL:   2.826\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.059 | Train PPL:   2.883\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.078 | Train PPL:   2.938\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.071 | Train PPL:   2.919\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.957\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.079 | Train PPL:   2.942\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.054 | Train PPL:   2.869\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.073 | Train PPL:   2.925\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.936\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.099 | Train PPL:   3.002\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.955\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.107 | Train PPL:   3.025\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.634 | Train PPL:   1.885\n",
      "\t Val. Loss: 0.531 |  Val. PPL:   1.701\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.465 | Train PPL:   1.591\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.412 | Train PPL:   1.509\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.560\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.476\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.542\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.499\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.488\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.515\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.499\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.463\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.434\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.447\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.454\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.460\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.371\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.396\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.368\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.364\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.443\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.239\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.185\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 101 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 102 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 103 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 104 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 105 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 106 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 107 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 108 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 109 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 110 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 111 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 112 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 113 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 114 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 115 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 116 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 117 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 118 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 119 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 120 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 121 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 122 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 123 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 124 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 125 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 126 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 127 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 128 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 129 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 130 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 131 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 132 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 133 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 134 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 135 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 136 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 137 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 138 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "Epoch: 139 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 140 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 141 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 142 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 143 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 144 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 145 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 146 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 147 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 148 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 149 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 150 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 151 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 152 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 153 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 154 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 155 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 156 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 157 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 158 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 159 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 160 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 161 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 162 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 163 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 164 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 165 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 166 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 167 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 168 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 169 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 170 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 171 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 172 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 173 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 174 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 175 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 176 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 177 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 178 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 179 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 180 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 181 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 182 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 183 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 184 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 185 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 186 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 187 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 188 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 189 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 190 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 191 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 192 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 193 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 194 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 195 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 196 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 197 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 198 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 199 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 200 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 201 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 202 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 203 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 204 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 205 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 206 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 207 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 208 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 209 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 210 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 211 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 212 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 213 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 214 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 215 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 216 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 217 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 218 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 219 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 220 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 221 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 222 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 223 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 224 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 225 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 226 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 227 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 228 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 229 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 230 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 231 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 232 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 233 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 234 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 235 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 236 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 237 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 238 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 239 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 240 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 241 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 242 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 243 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 244 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 245 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 246 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 247 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 248 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 249 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 250 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 251 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 252 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 253 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 254 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 255 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 256 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 257 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 258 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 259 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 260 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 261 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 262 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 263 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 264 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 265 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 266 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 267 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 268 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 269 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 270 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 271 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 272 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 273 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 274 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 275 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 276 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 277 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 278 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 279 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 280 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 281 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 282 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 283 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 284 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 285 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 286 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 287 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 288 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 289 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 290 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 291 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 292 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 293 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 294 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 295 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.316\n",
      "Epoch: 296 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 297 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 298 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 299 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 300 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 301 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 302 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 303 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 304 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 305 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 306 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 307 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 308 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 309 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 310 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 311 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 312 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 313 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 314 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 315 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 316 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 317 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 318 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 319 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 320 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 321 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 322 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 323 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 324 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 325 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 326 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 327 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 328 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 329 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 330 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 331 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 332 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 333 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 334 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 335 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 336 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 337 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 338 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 339 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 340 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 341 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 342 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 343 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 344 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 345 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 346 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 347 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 348 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 349 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 350 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 351 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 352 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 353 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 354 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 355 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 356 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 359 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 360 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 362 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 363 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 367 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 369 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 374 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 375 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 376 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 379 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 385 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 390 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 392 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "SCHEDULE: DynaMoE-1.s1.t1.e400\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.592 | Train PPL:   1.808\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.592 | Train PPL:   1.807\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.610 | Train PPL:   1.840\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.735\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.565 | Train PPL:   1.759\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.620 | Train PPL:   1.858\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.564 | Train PPL:   1.757\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.563 | Train PPL:   1.756\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.577 | Train PPL:   1.780\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.532 | Train PPL:   1.702\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.596 | Train PPL:   1.816\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.573 | Train PPL:   1.773\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.589 | Train PPL:   1.803\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.605 | Train PPL:   1.832\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.720\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.591 | Train PPL:   1.805\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.582 | Train PPL:   1.789\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.720\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.615 | Train PPL:   1.850\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.600 | Train PPL:   1.822\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.598 | Train PPL:   1.818\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.635 | Train PPL:   1.886\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.561 | Train PPL:   1.752\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.532 | Train PPL:   1.702\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.513 | Train PPL:   1.669\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.593 | Train PPL:   1.809\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.598 | Train PPL:   1.819\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.567 | Train PPL:   1.762\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.611 | Train PPL:   1.842\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.603 | Train PPL:   1.828\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.553 | Train PPL:   1.738\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "-----------------------------------\n",
      "-----Added Expert-train Gating-----\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.581 | Train PPL:   1.788\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.579 | Train PPL:   1.784\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.580 | Train PPL:   1.787\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.591 | Train PPL:   1.805\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.566 | Train PPL:   1.762\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.612 | Train PPL:   1.845\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.578 | Train PPL:   1.783\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.528 | Train PPL:   1.696\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.599 | Train PPL:   1.820\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.719\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.633 | Train PPL:   1.882\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.588 | Train PPL:   1.800\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.620 | Train PPL:   1.859\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.530 | Train PPL:   1.698\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.581 | Train PPL:   1.787\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.606 | Train PPL:   1.833\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.552 | Train PPL:   1.736\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.558 | Train PPL:   1.747\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.599 | Train PPL:   1.821\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.608 | Train PPL:   1.837\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.566 | Train PPL:   1.761\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.628 | Train PPL:   1.874\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.580 | Train PPL:   1.786\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.575 | Train PPL:   1.777\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.585 | Train PPL:   1.795\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.587 | Train PPL:   1.799\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.564 | Train PPL:   1.758\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.589 | Train PPL:   1.802\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.546 | Train PPL:   1.727\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.559 | Train PPL:   1.749\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.617 | Train PPL:   1.853\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.680 | Train PPL:   1.974\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.481 | Train PPL:   1.617\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.445 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.560\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.413 | Train PPL:   1.511\n",
      "\t Val. Loss: 0.429 |  Val. PPL:   1.536\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.488\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.431 |  Val. PPL:   1.539\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.518\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.506\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.403 |  Val. PPL:   1.496\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.527\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.318 |  Val. PPL:   1.375\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.409 | Train PPL:   1.505\n",
      "\t Val. Loss: 0.317 |  Val. PPL:   1.373\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.337\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.340\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.394\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.506\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.472\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.316\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 101 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 102 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 103 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 104 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 105 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 106 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 107 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 108 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 109 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.179\n",
      "Epoch: 110 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.206\n",
      "Epoch: 111 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 112 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 113 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 114 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.179\n",
      "Epoch: 115 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 116 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 117 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 118 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 119 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 120 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 121 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 122 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 123 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 124 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 125 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 126 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 127 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 128 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 129 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 130 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 131 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 132 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 133 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 134 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 135 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 136 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 137 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 138 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 139 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 140 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 141 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 142 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 143 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 144 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 145 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 146 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 147 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 148 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 149 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 150 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 151 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 152 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 153 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 154 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 155 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 156 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 157 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 158 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 159 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 160 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 161 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 162 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 163 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 164 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 165 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 166 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 167 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 168 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 169 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 170 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 171 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 172 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 173 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 174 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 175 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 176 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 177 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 178 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 179 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 180 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 181 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 182 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 183 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 184 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 185 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 186 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 187 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 188 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 189 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 190 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 191 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 192 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 193 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 194 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 195 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 196 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 197 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 198 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 199 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 200 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 201 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 202 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 203 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 204 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 205 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 206 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 207 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 208 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 209 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 210 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 211 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 212 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 213 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 214 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 215 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 216 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 217 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 218 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 219 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 220 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 221 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 222 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 223 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 224 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 225 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 226 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 227 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 228 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 229 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 230 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 231 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 232 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 233 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 234 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 235 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 236 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 237 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 238 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 239 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 240 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 241 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 242 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 243 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 244 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 245 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 246 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 247 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 248 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 249 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 250 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 251 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 252 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 253 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 254 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 255 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 256 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 257 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 258 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 259 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 260 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 261 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 262 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 263 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 264 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 265 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 266 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 267 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 268 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 269 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 270 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 271 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 272 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 273 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 274 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 275 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 276 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 277 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 278 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 279 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 280 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 281 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 282 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 283 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 284 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 285 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 286 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 287 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 288 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 289 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 290 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 291 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 292 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 293 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 294 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 295 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 296 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 297 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 298 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 299 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 300 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 301 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 302 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 303 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 305 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 308 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 315 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 316 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 318 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 321 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 326 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 328 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 330 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 331 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 332 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 333 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 335 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 336 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 337 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 340 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 341 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 342 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 343 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 344 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 347 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 348 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 349 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 350 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 352 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 353 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 354 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 355 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 356 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 358 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 360 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 362 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 363 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 364 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 365 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 366 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 368 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 369 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 371 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 372 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 373 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 374 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 375 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 376 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 379 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 380 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 381 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 383 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 384 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 385 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 388 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 389 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 390 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 391 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 392 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 393 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 394 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 398 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 400 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "\n",
      "SCHEDULE: DynaMoE-1.s2.t2.e400\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 101 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 102 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 103 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 104 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 105 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 106 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 107 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 108 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 109 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 110 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 111 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 112 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 113 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 114 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 115 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 116 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 117 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 118 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 119 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 120 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 121 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 122 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 123 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 124 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 125 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 126 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 127 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 128 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 129 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 130 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 131 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 132 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 133 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 134 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 135 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 136 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 137 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 138 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 139 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 140 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 141 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 142 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 143 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 144 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 145 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 146 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 147 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 148 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 149 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 150 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 151 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 152 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 153 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 154 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 155 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 156 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 157 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 158 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 159 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 160 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 161 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 162 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 164 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 165 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 166 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 167 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 168 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 169 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 170 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 171 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 201 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 202 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 203 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 204 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 205 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 206 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 207 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 208 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 209 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 210 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 211 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 212 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 213 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 214 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 215 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 216 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 217 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 218 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 219 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 220 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 221 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 222 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 223 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 224 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 225 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 226 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 227 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 228 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 229 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 230 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 231 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 232 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 233 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 234 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 235 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 236 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 237 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 238 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 239 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 240 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 241 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 242 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 243 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 244 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 245 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 246 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 247 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 248 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 249 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 250 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 251 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 252 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 253 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 254 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 255 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 256 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 257 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 258 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 259 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 260 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 261 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 262 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 263 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 264 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 265 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 266 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 267 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 268 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 269 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 270 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 271 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 272 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 273 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 274 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 275 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 276 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 277 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 278 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 279 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 280 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 281 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 282 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 283 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 284 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 285 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 286 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 287 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 288 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 289 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 290 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 291 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 292 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 293 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 294 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 295 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 296 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 297 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 298 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 299 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 300 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 301 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 302 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 303 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 305 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 308 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 315 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 316 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 318 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 321 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 326 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 328 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 330 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 331 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 332 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 333 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 335 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 336 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 337 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 340 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 341 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 342 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 343 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 344 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 347 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 348 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 349 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 350 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 352 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 353 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 354 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 355 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 356 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 360 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 362 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 363 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 369 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 374 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 375 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 376 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 379 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 385 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 390 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 392 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = 1\n",
    "hist_all_losses_E, hist_all_hitsss_E, models_E = experiment(\n",
    "    \"DynaMoE-1\",\n",
    "    n_repetitions,\n",
    "    SCHEDULE,\n",
    "    init_dynamoe,\n",
    "    repeat_dynamoe,\n",
    "    STEP_SIZE_EVALUATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_E, SCHEDULE, STEP_SIZE_EVALUATION, datasets=(0,1), figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interleaved training DynaMoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "CvFmPpKQozmz"
   },
   "outputs": [],
   "source": [
    "N_EXPERTS_START = 1\n",
    "N_MAX_EXPERTS = 3\n",
    "GATE_DROPOUT = 0.5\n",
    "N_GATING_HIDDEN_DIM = 10\n",
    "N_GATING_EMBED_DIM = 10\n",
    "SCHEDULE = medium_interleaved\n",
    "\n",
    "# If the amount of missed hits is bigger then PERFORMANCE_TRESHHOLD_START\n",
    "# and it stays within ALLOWED_ERROR_VARIANCE for\n",
    "# N_EPOCHS_UNTIL_NEW_EXPERT epochs, then a new\n",
    "# expert is initialized\n",
    "N_EPOCHS_UNTIL_NEW_EXPERT = 30\n",
    "ALLOWED_ERROR_VARIANCE = 0.1\n",
    "PERFORMANCE_TRESHHOLD_START = 0.3\n",
    "\n",
    "# Difference between replicated sequence accuracy of training sequence\n",
    "# before and training sequence coming in to decide to consolidate for\n",
    "# a new expert.\n",
    "PERFORMANCE_DIFFERENCE_NEW_TASK = 0.3 # 0.5 for bad example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "dUUt4knHYfDj"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRoGUVZcfgMg",
    "outputId": "2c57ceea-2add-4c03-f0b3-5a9252f522d0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@ Repetition   0 @@@@@@@@@\n",
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(19, 9, bidirectional=True)\n",
      "        (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "          (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(37, 9)\n",
      "        (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s0.t0.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.109 | Train PPL:   3.032\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.079 | Train PPL:   2.941\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.060 | Train PPL:   2.885\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.957\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.068 | Train PPL:   2.910\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.934\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.098 | Train PPL:   2.999\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.057 | Train PPL:   2.877\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.070 | Train PPL:   2.914\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.047 | Train PPL:   2.849\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.102 | Train PPL:   3.011\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.119 | Train PPL:   3.061\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.072 | Train PPL:   2.921\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.047 | Train PPL:   2.850\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.081 | Train PPL:   2.948\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.120 | Train PPL:   3.066\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.105 | Train PPL:   3.020\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.019 | Train PPL:   2.770\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.075 | Train PPL:   2.931\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.039 | Train PPL:   2.826\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.059 | Train PPL:   2.883\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.078 | Train PPL:   2.938\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.071 | Train PPL:   2.919\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.957\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.079 | Train PPL:   2.942\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.054 | Train PPL:   2.869\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.073 | Train PPL:   2.925\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.936\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.099 | Train PPL:   3.002\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.955\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.107 | Train PPL:   3.025\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.634 | Train PPL:   1.885\n",
      "\t Val. Loss: 0.531 |  Val. PPL:   1.701\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.465 | Train PPL:   1.591\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.412 | Train PPL:   1.509\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.560\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.476\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.542\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.499\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.488\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.515\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.499\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.463\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.434\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.447\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.454\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.460\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.371\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.396\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.368\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.364\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.443\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.239\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.185\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s1.t1.e100\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.562 | Train PPL:   1.755\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.590 | Train PPL:   1.804\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.571 | Train PPL:   1.770\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.520 | Train PPL:   1.682\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.735\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.529 | Train PPL:   1.697\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.572 | Train PPL:   1.771\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.526 | Train PPL:   1.692\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.489 | Train PPL:   1.630\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.484 | Train PPL:   1.623\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.736\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.669\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.525 | Train PPL:   1.690\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.563 | Train PPL:   1.756\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.718\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.560 | Train PPL:   1.751\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.564 | Train PPL:   1.758\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.669\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.576 | Train PPL:   1.779\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.567 | Train PPL:   1.764\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.558 | Train PPL:   1.747\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.577 | Train PPL:   1.781\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.549 | Train PPL:   1.731\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.533 | Train PPL:   1.705\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.712\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.725\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.531 | Train PPL:   1.701\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.586 | Train PPL:   1.796\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.544 | Train PPL:   1.723\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.537 | Train PPL:   1.711\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.544 | Train PPL:   1.723\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "-----------------------------------\n",
      "-----Added Expert-train Gating-----\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.565 | Train PPL:   1.760\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.544 | Train PPL:   1.723\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.553 | Train PPL:   1.738\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.514 | Train PPL:   1.673\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.572 | Train PPL:   1.773\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.489 | Train PPL:   1.631\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.524 | Train PPL:   1.688\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.556 | Train PPL:   1.744\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.532 | Train PPL:   1.703\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.544 | Train PPL:   1.723\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.555 | Train PPL:   1.742\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.500 | Train PPL:   1.649\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.724\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.506 | Train PPL:   1.659\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.735\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.516 | Train PPL:   1.675\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.558 | Train PPL:   1.748\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.539 | Train PPL:   1.714\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.524 | Train PPL:   1.690\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.530 | Train PPL:   1.699\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.563 | Train PPL:   1.756\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.546 | Train PPL:   1.726\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.581 | Train PPL:   1.788\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.509 | Train PPL:   1.663\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.499 | Train PPL:   1.647\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.555 | Train PPL:   1.742\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.717\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.501 | Train PPL:   1.650\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.543 | Train PPL:   1.722\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.523 | Train PPL:   1.686\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.564 | Train PPL:   1.758\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.643 | Train PPL:   1.902\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.480 | Train PPL:   1.616\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.437 | Train PPL:   1.549\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.533\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.519\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.387 | Train PPL:   1.472\n",
      "\t Val. Loss: 0.414 |  Val. PPL:   1.513\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.505\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.476\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.470\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
      "\t Val. Loss: 0.414 |  Val. PPL:   1.513\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.506\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
      "\t Val. Loss: 0.417 |  Val. PPL:   1.517\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.484\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.367\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.379\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.297 |  Val. PPL:   1.346\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.393\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.274\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.417\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s2.t2.e100\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.502\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.391 | Train PPL:   1.479\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s3.t0.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s4.t1.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s5.t2.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s6.t0.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s7.t1.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s8.t2.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s9.t0.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s10.t1.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s11.t2.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = 1\n",
    "hist_all_losses_E2, hist_all_hitsss_E2, models_E2 = experiment(\n",
    "    \"DynaMoE-2\",\n",
    "    n_repetitions,\n",
    "    SCHEDULE,\n",
    "    init_dynamoe,\n",
    "    repeat_dynamoe,\n",
    "    STEP_SIZE_EVALUATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_E2, SCHEDULE, STEP_SIZE_EVALUATION, datasets=(0,1), figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5kSqVvW6e7M"
   },
   "source": [
    "### bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "gating_trgts = []\n",
    "gating_trgts.append( [torch.tensor([1,0,0]) for _ in range(len(train_dls[0]))])\n",
    "gating_trgts.append( [torch.tensor([0,1,0]) for _ in range(len(train_dls[1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_dls[1]:\n",
    "    if x[1] == 7:\n",
    "        print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "gating, gating_optimizer = init_gating()\n",
    "gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0990233421325684\n",
      "loss\n",
      "1.0725314617156982\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.06223726272583\n",
      "loss\n",
      "1.1580368280410767\n",
      "loss\n",
      "0.994653046131134\n",
      "loss\n",
      "1.0136317014694214\n",
      "loss\n",
      "1.1363170146942139\n",
      "loss\n",
      "1.0377978086471558\n",
      "loss\n",
      "1.273862361907959\n",
      "loss\n",
      "1.1935420036315918\n",
      "loss\n",
      "1.2392826080322266\n",
      "loss\n",
      "0.843536376953125\n",
      "loss\n",
      "1.0642420053482056\n",
      "loss\n",
      "0.8814082145690918\n",
      "loss\n",
      "1.1417649984359741\n",
      "loss\n",
      "1.2824432849884033\n",
      "loss\n",
      "0.8938198089599609\n",
      "loss\n",
      "0.9051280617713928\n",
      "loss\n",
      "0.9723299145698547\n",
      "loss\n",
      "1.2795649766921997\n",
      "loss\n",
      "1.1691962480545044\n",
      "loss\n",
      "1.2110319137573242\n",
      "loss\n",
      "0.7188755869865417\n",
      "loss\n",
      "1.3590319156646729\n",
      "loss\n",
      "0.9704457521438599\n",
      "loss\n",
      "0.9680742025375366\n",
      "loss\n",
      "1.385023593902588\n",
      "loss\n",
      "0.7331454157829285\n",
      "loss\n",
      "0.7141051888465881\n",
      "loss\n",
      "1.245851993560791\n",
      "loss\n",
      "0.7735995054244995\n",
      "loss\n",
      "1.3736876249313354\n",
      "loss\n",
      "0.8231424689292908\n",
      "loss\n",
      "1.3750696182250977\n",
      "loss\n",
      "1.51996910572052\n",
      "loss\n",
      "1.2661633491516113\n",
      "loss\n",
      "1.3473231792449951\n",
      "loss\n",
      "1.233702301979065\n",
      "loss\n",
      "0.7255655527114868\n",
      "loss\n",
      "0.6757468581199646\n",
      "loss\n",
      "0.6531469225883484\n",
      "loss\n",
      "0.745249330997467\n",
      "loss\n",
      "0.7534412145614624\n",
      "loss\n",
      "1.4418754577636719\n",
      "loss\n",
      "0.7937403321266174\n",
      "loss\n",
      "0.6716989278793335\n",
      "loss\n",
      "1.2280333042144775\n",
      "loss\n",
      "0.6496949195861816\n",
      "loss\n",
      "1.5460278987884521\n",
      "loss\n",
      "1.5514756441116333\n",
      "loss\n",
      "1.1636765003204346\n",
      "loss\n",
      "1.3178153038024902\n",
      "loss\n",
      "0.5455304384231567\n",
      "loss\n",
      "0.5333239436149597\n",
      "loss\n",
      "1.4688210487365723\n",
      "loss\n",
      "1.375108003616333\n",
      "loss\n",
      "0.5335070490837097\n",
      "loss\n",
      "1.430232048034668\n",
      "loss\n",
      "1.1741782426834106\n",
      "loss\n",
      "1.2446054220199585\n",
      "loss\n",
      "0.6195405721664429\n",
      "loss\n",
      "0.49828198552131653\n",
      "loss\n",
      "0.7075855135917664\n",
      "loss\n",
      "0.6252626776695251\n",
      "loss\n",
      "1.3688387870788574\n",
      "loss\n",
      "1.3391385078430176\n",
      "loss\n",
      "1.1697885990142822\n",
      "loss\n",
      "0.37812450528144836\n",
      "loss\n",
      "1.1592947244644165\n",
      "loss\n",
      "0.6913880109786987\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.6907056570053101\n",
      "loss\n",
      "0.4162207841873169\n",
      "loss\n",
      "0.5808714032173157\n",
      "loss\n",
      "1.1500698328018188\n",
      "loss\n",
      "0.352736234664917\n",
      "loss\n",
      "0.39313051104545593\n",
      "loss\n",
      "0.41192883253097534\n",
      "loss\n",
      "0.5116475820541382\n",
      "loss\n",
      "1.1172572374343872\n",
      "loss\n",
      "1.4131906032562256\n",
      "loss\n",
      "0.33028650283813477\n",
      "loss\n",
      "1.489363193511963\n",
      "loss\n",
      "1.2112787961959839\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.4293568730354309\n",
      "loss\n",
      "0.4328417181968689\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.47999051213264465\n",
      "loss\n",
      "0.19354216754436493\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.1361033916473389\n",
      "loss\n",
      "0.1798943281173706\n",
      "loss\n",
      "0.12139580398797989\n",
      "loss\n",
      "0.21990478038787842\n",
      "loss\n",
      "0.2244793325662613\n",
      "loss\n",
      "0.1716667264699936\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.5843265056610107\n",
      "loss\n",
      "0.11034240573644638\n",
      "loss\n",
      "0.3461153209209442\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.14620114862918854\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.13816894590854645\n",
      "loss\n",
      "0.15988250076770782\n",
      "loss\n",
      "0.2077673375606537\n",
      "loss\n",
      "0.13088884949684143\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.3269919157028198\n",
      "loss\n",
      "0.12710054218769073\n",
      "loss\n",
      "1.4896562099456787\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.10144913196563721\n",
      "loss\n",
      "0.07071235775947571\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.40675270557403564\n",
      "loss\n",
      "0.06046057492494583\n",
      "loss\n",
      "0.04141584411263466\n",
      "loss\n",
      "1.0897676944732666\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.07548543810844421\n",
      "loss\n",
      "1.0544909238815308\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.059570763260126114\n",
      "loss\n",
      "0.06236014887690544\n",
      "loss\n",
      "0.04088432341814041\n",
      "loss\n",
      "0.6670161485671997\n",
      "loss\n",
      "0.09925808757543564\n",
      "loss\n",
      "0.06430821120738983\n",
      "loss\n",
      "0.5717656016349792\n",
      "loss\n",
      "0.5106241106987\n",
      "loss\n",
      "0.6109991073608398\n",
      "loss\n",
      "0.4585453271865845\n",
      "loss\n",
      "0.40513986349105835\n",
      "loss\n",
      "0.028454262763261795\n",
      "loss\n",
      "0.10292236506938934\n",
      "loss\n",
      "0.14290152490139008\n",
      "loss\n",
      "0.08874966204166412\n",
      "loss\n",
      "0.1695457398891449\n",
      "loss\n",
      "0.06828229129314423\n",
      "loss\n",
      "0.03192349523305893\n",
      "loss\n",
      "0.10639900714159012\n",
      "loss\n",
      "0.13714158535003662\n",
      "loss\n",
      "0.04405619949102402\n",
      "loss\n",
      "0.020778479054570198\n",
      "loss\n",
      "0.04888411983847618\n",
      "loss\n",
      "0.02315611019730568\n",
      "loss\n",
      "0.03884715959429741\n",
      "loss\n",
      "0.0185548085719347\n",
      "loss\n",
      "0.026514191180467606\n",
      "loss\n",
      "0.015334682539105415\n",
      "loss\n",
      "0.16082657873630524\n",
      "loss\n",
      "0.16628775000572205\n",
      "loss\n",
      "0.03266807645559311\n",
      "loss\n",
      "0.024600008502602577\n",
      "loss\n",
      "0.030816741287708282\n",
      "loss\n",
      "0.021857907995581627\n",
      "loss\n",
      "0.009852115996181965\n",
      "loss\n",
      "0.0058709559962153435\n",
      "loss\n",
      "0.017790623009204865\n",
      "loss\n",
      "0.015698540955781937\n",
      "loss\n",
      "0.08955177664756775\n",
      "loss\n",
      "0.020790155977010727\n",
      "loss\n",
      "0.037340905517339706\n",
      "loss\n",
      "0.030650736764073372\n",
      "loss\n",
      "0.024169908836483955\n",
      "loss\n",
      "0.011669941246509552\n",
      "loss\n",
      "0.8218876719474792\n",
      "loss\n",
      "0.0061847250908613205\n",
      "loss\n",
      "0.04579256847500801\n",
      "loss\n",
      "0.014900343492627144\n",
      "loss\n",
      "0.021984562277793884\n",
      "loss\n",
      "0.008054869249463081\n",
      "loss\n",
      "0.017524754628539085\n",
      "loss\n",
      "0.008446445688605309\n",
      "loss\n",
      "0.005841090343892574\n",
      "loss\n",
      "0.02205546200275421\n",
      "loss\n",
      "0.01886228285729885\n",
      "loss\n",
      "0.012258898466825485\n",
      "loss\n",
      "0.00998903438448906\n",
      "loss\n",
      "0.007056789472699165\n",
      "loss\n",
      "0.010330774821341038\n",
      "loss\n",
      "0.00817713513970375\n",
      "loss\n",
      "0.006179037969559431\n",
      "loss\n",
      "0.0073410761542618275\n",
      "loss\n",
      "0.010899768210947514\n",
      "loss\n",
      "0.026390664279460907\n",
      "loss\n",
      "0.016640907153487206\n",
      "loss\n",
      "0.00597974332049489\n",
      "loss\n",
      "0.009672206826508045\n",
      "loss\n",
      "0.005973581690341234\n",
      "loss\n",
      "3.6694374084472656\n",
      "loss\n",
      "0.00877902377396822\n",
      "loss\n",
      "4.256886959075928\n",
      "loss\n",
      "0.017297949641942978\n",
      "loss\n",
      "0.011129443533718586\n",
      "loss\n",
      "0.015219627879559994\n",
      "loss\n",
      "0.0030019478872418404\n",
      "loss\n",
      "0.021291498094797134\n",
      "loss\n",
      "0.018529647961258888\n",
      "loss\n",
      "0.009522965177893639\n",
      "loss\n",
      "0.012272440828382969\n",
      "loss\n",
      "0.008545498363673687\n",
      "loss\n",
      "0.008923650719225407\n",
      "loss\n",
      "0.0023492376785725355\n",
      "loss\n",
      "0.08297166228294373\n",
      "loss\n",
      "0.006391085684299469\n",
      "loss\n",
      "0.2621001899242401\n",
      "loss\n",
      "0.0033736478071659803\n",
      "loss\n",
      "0.06227020546793938\n",
      "loss\n",
      "0.10268152505159378\n",
      "loss\n",
      "0.0015656605828553438\n",
      "loss\n",
      "0.001490197260864079\n",
      "loss\n",
      "0.0031985098030418158\n",
      "loss\n",
      "0.19377416372299194\n",
      "loss\n",
      "0.43188416957855225\n",
      "loss\n",
      "0.010465031489729881\n",
      "loss\n",
      "0.313730925321579\n",
      "loss\n",
      "0.052673447877168655\n",
      "loss\n",
      "0.010588418692350388\n",
      "loss\n",
      "0.16984111070632935\n",
      "loss\n",
      "0.003559921169653535\n",
      "loss\n",
      "0.05207546055316925\n",
      "loss\n",
      "0.02877179905772209\n",
      "loss\n",
      "0.09439631551504135\n",
      "loss\n",
      "0.003439465072005987\n",
      "loss\n",
      "0.01418695505708456\n",
      "loss\n",
      "0.005863133817911148\n",
      "loss\n",
      "0.010725353844463825\n",
      "loss\n",
      "0.01660596765577793\n",
      "loss\n",
      "0.008103824220597744\n",
      "loss\n",
      "0.020882155746221542\n",
      "loss\n",
      "0.015934040769934654\n",
      "loss\n",
      "0.009476440958678722\n",
      "loss\n",
      "0.011041258461773396\n",
      "loss\n",
      "0.0038036394398659468\n",
      "loss\n",
      "0.015622024424374104\n",
      "loss\n",
      "0.004314637742936611\n",
      "loss\n",
      "0.002865258837118745\n",
      "loss\n",
      "0.003520245896652341\n",
      "loss\n",
      "0.00621197372674942\n",
      "loss\n",
      "0.3402809202671051\n",
      "loss\n",
      "1.1181387901306152\n",
      "loss\n",
      "0.016513574868440628\n",
      "loss\n",
      "0.005002601072192192\n",
      "loss\n",
      "0.005188333801925182\n",
      "loss\n",
      "0.0484754703938961\n",
      "loss\n",
      "0.011777392588555813\n",
      "loss\n",
      "0.00955106783658266\n",
      "loss\n",
      "0.0032876271288841963\n",
      "loss\n",
      "2.739438056945801\n",
      "loss\n",
      "0.08617750555276871\n",
      "loss\n",
      "0.0030841901898384094\n",
      "loss\n",
      "0.005740703083574772\n",
      "loss\n",
      "0.022282473742961884\n",
      "loss\n",
      "0.049073684960603714\n",
      "loss\n",
      "0.010887740179896355\n",
      "loss\n",
      "0.0028799984138458967\n",
      "loss\n",
      "0.0028621682431548834\n",
      "loss\n",
      "0.020307835191488266\n",
      "loss\n",
      "0.007002928759902716\n",
      "loss\n",
      "0.004172904882580042\n",
      "loss\n",
      "0.011328539811074734\n",
      "loss\n",
      "0.002724746707826853\n",
      "loss\n",
      "0.011057999916374683\n",
      "loss\n",
      "0.007479520980268717\n",
      "loss\n",
      "0.007599018048495054\n",
      "loss\n",
      "0.019894583150744438\n",
      "loss\n",
      "0.011453117243945599\n",
      "loss\n",
      "0.006259953137487173\n",
      "loss\n",
      "0.005067836493253708\n",
      "loss\n",
      "0.07912437617778778\n",
      "loss\n",
      "0.018125612288713455\n",
      "loss\n",
      "0.00830671563744545\n",
      "loss\n",
      "0.003499813610687852\n",
      "loss\n",
      "0.009177522733807564\n",
      "loss\n",
      "0.007959552109241486\n",
      "loss\n",
      "0.0048975031822919846\n",
      "loss\n",
      "0.00563117815181613\n",
      "loss\n",
      "0.011946087703108788\n",
      "loss\n",
      "0.12055058032274246\n",
      "loss\n",
      "0.0014368696138262749\n",
      "loss\n",
      "0.0018726922571659088\n",
      "loss\n",
      "0.0020918408408761024\n",
      "loss\n",
      "0.01692248322069645\n",
      "loss\n",
      "0.05113114416599274\n",
      "loss\n",
      "0.0038523285184055567\n",
      "loss\n",
      "0.0021715410985052586\n",
      "loss\n",
      "0.0018234307644888759\n",
      "loss\n",
      "0.03622610867023468\n",
      "loss\n",
      "0.004267395939677954\n",
      "loss\n",
      "0.004066176246851683\n",
      "loss\n",
      "1.5602083206176758\n",
      "loss\n",
      "0.004498837050050497\n",
      "loss\n",
      "0.006695810705423355\n",
      "loss\n",
      "0.023411402478814125\n",
      "loss\n",
      "0.011220568791031837\n",
      "loss\n",
      "0.005698625463992357\n",
      "loss\n",
      "0.2322503626346588\n",
      "loss\n",
      "0.00617643166333437\n",
      "loss\n",
      "0.001796538126654923\n",
      "loss\n",
      "0.005663539282977581\n",
      "loss\n",
      "0.015758037567138672\n",
      "loss\n",
      "0.0018020119750872254\n",
      "loss\n",
      "0.005692461505532265\n",
      "loss\n",
      "0.016637155786156654\n",
      "loss\n",
      "0.004839136730879545\n",
      "loss\n",
      "0.010831727646291256\n",
      "loss\n",
      "0.0023832509759813547\n",
      "loss\n",
      "0.004562444519251585\n",
      "loss\n",
      "0.009627577848732471\n",
      "loss\n",
      "0.024295106530189514\n",
      "loss\n",
      "0.011150899343192577\n",
      "loss\n",
      "0.009857428260147572\n",
      "loss\n",
      "0.00823152344673872\n",
      "loss\n",
      "0.006388835143297911\n",
      "loss\n",
      "0.006659812293946743\n",
      "loss\n",
      "0.005407468415796757\n",
      "loss\n",
      "0.006669285707175732\n",
      "loss\n",
      "0.007078569382429123\n",
      "loss\n",
      "0.00319589558057487\n",
      "loss\n",
      "0.004212554078549147\n",
      "loss\n",
      "0.00865777675062418\n",
      "loss\n",
      "0.0035727499052882195\n",
      "loss\n",
      "0.0016208856832236052\n",
      "loss\n",
      "0.006474115885794163\n",
      "loss\n",
      "0.011410925537347794\n",
      "loss\n",
      "0.01145629957318306\n",
      "loss\n",
      "0.007725598756223917\n",
      "loss\n",
      "0.0021824846044182777\n",
      "loss\n",
      "0.0015101945027709007\n",
      "loss\n",
      "0.01465426292270422\n",
      "loss\n",
      "0.0017301365733146667\n",
      "loss\n",
      "0.006637786515057087\n",
      "loss\n",
      "0.002001427114009857\n",
      "loss\n",
      "0.0010892179561778903\n",
      "loss\n",
      "0.005189519841223955\n",
      "loss\n",
      "0.005124291870743036\n",
      "loss\n",
      "0.016665644943714142\n",
      "loss\n",
      "0.005322452634572983\n",
      "loss\n",
      "0.0194013062864542\n",
      "loss\n",
      "0.0016494491137564182\n",
      "loss\n",
      "0.01149224303662777\n",
      "loss\n",
      "0.03479050472378731\n",
      "loss\n",
      "0.006425435654819012\n",
      "loss\n",
      "0.005977491848170757\n",
      "loss\n",
      "0.00912188645452261\n",
      "loss\n",
      "2.2605345249176025\n",
      "loss\n",
      "0.006742346566170454\n",
      "loss\n",
      "0.003994344733655453\n",
      "loss\n",
      "0.02096504159271717\n",
      "loss\n",
      "0.0022115076426416636\n",
      "loss\n",
      "0.011889898218214512\n",
      "loss\n",
      "0.0265306755900383\n",
      "loss\n",
      "0.0021627387031912804\n",
      "loss\n",
      "0.00421172333881259\n",
      "loss\n",
      "0.015857893973588943\n",
      "loss\n",
      "0.1096530631184578\n",
      "loss\n",
      "0.018577391281723976\n",
      "loss\n",
      "0.0031585826072841883\n",
      "loss\n",
      "0.007590973284095526\n",
      "loss\n",
      "0.01869463361799717\n",
      "loss\n",
      "0.0005868143052794039\n",
      "loss\n",
      "0.0009062950266525149\n",
      "loss\n",
      "0.0009920443408191204\n",
      "loss\n",
      "0.022964363917708397\n",
      "loss\n",
      "0.0006668727728538215\n",
      "loss\n",
      "1.710546851158142\n",
      "loss\n",
      "0.020992590114474297\n",
      "loss\n",
      "0.0007769426447339356\n",
      "loss\n",
      "0.0007874249131418765\n",
      "loss\n",
      "0.002684087259694934\n",
      "loss\n",
      "0.0014432977186515927\n",
      "loss\n",
      "0.014400719664990902\n",
      "loss\n",
      "0.01647898182272911\n",
      "loss\n",
      "0.0025668551679700613\n",
      "loss\n",
      "0.02713252790272236\n",
      "loss\n",
      "0.0029743739869445562\n",
      "loss\n",
      "0.006955931894481182\n",
      "loss\n",
      "0.02879623882472515\n",
      "loss\n",
      "0.009999656118452549\n",
      "loss\n",
      "0.002252900041639805\n",
      "loss\n",
      "0.0019014865392819047\n",
      "loss\n",
      "0.004394398536533117\n",
      "loss\n",
      "0.0068917665630578995\n",
      "loss\n",
      "0.0008858094224706292\n",
      "loss\n",
      "0.009932027198374271\n",
      "loss\n",
      "0.014354773797094822\n",
      "loss\n",
      "0.003590567270293832\n",
      "loss\n",
      "0.007239537313580513\n",
      "loss\n",
      "0.010349651798605919\n",
      "loss\n",
      "0.0017913023475557566\n",
      "loss\n",
      "0.005552583374083042\n",
      "loss\n",
      "0.005063566844910383\n",
      "loss\n",
      "0.00753796985372901\n",
      "loss\n",
      "0.005215846933424473\n",
      "loss\n",
      "0.019643884152173996\n",
      "loss\n",
      "0.010137026198208332\n",
      "loss\n",
      "0.009279691614210606\n",
      "loss\n",
      "0.24630846083164215\n",
      "loss\n",
      "0.20919343829154968\n",
      "loss\n",
      "0.011986136436462402\n",
      "loss\n",
      "0.03142803534865379\n",
      "loss\n",
      "0.0016289787599816918\n",
      "loss\n",
      "0.007744170259684324\n",
      "loss\n",
      "0.0020404488313943148\n",
      "loss\n",
      "0.0019769188947975636\n",
      "loss\n",
      "0.0036828566808253527\n",
      "loss\n",
      "0.001430917764082551\n",
      "loss\n",
      "0.0032458023633807898\n",
      "loss\n",
      "0.9635264873504639\n",
      "loss\n",
      "0.001839375589042902\n",
      "loss\n",
      "0.0015397133538499475\n",
      "loss\n",
      "0.01138865016400814\n",
      "loss\n",
      "0.006255806889384985\n",
      "loss\n",
      "0.0012344843707978725\n",
      "loss\n",
      "0.012749706394970417\n",
      "loss\n",
      "0.016048895195126534\n",
      "loss\n",
      "0.016542652621865273\n",
      "loss\n",
      "0.009411019273102283\n",
      "loss\n",
      "0.016764706000685692\n",
      "loss\n",
      "0.0016001766780391335\n",
      "loss\n",
      "0.024112075567245483\n",
      "loss\n",
      "0.0008298290777020156\n",
      "loss\n",
      "0.0038938906509429216\n",
      "loss\n",
      "0.001001809723675251\n",
      "loss\n",
      "0.0008660380262881517\n",
      "loss\n",
      "0.005714982748031616\n",
      "loss\n",
      "0.005052180495113134\n",
      "loss\n",
      "0.006232587620615959\n",
      "loss\n",
      "0.01502776425331831\n",
      "loss\n",
      "0.005707752425223589\n",
      "loss\n",
      "0.002473630243912339\n",
      "loss\n",
      "0.009997768327593803\n",
      "loss\n",
      "0.012457652017474174\n",
      "loss\n",
      "0.011125553399324417\n",
      "loss\n",
      "0.0012016226537525654\n",
      "loss\n",
      "0.0020697140134871006\n",
      "loss\n",
      "0.0006506709614768624\n",
      "loss\n",
      "0.014135828241705894\n",
      "loss\n",
      "0.004716699477285147\n",
      "loss\n",
      "0.0012061471352353692\n",
      "loss\n",
      "1.7048331499099731\n",
      "loss\n",
      "0.0016477829776704311\n",
      "loss\n",
      "0.007360128220170736\n",
      "loss\n",
      "0.0018893502419814467\n",
      "loss\n",
      "0.0011075560469180346\n",
      "loss\n",
      "0.0037392713129520416\n",
      "loss\n",
      "0.001839375589042902\n",
      "loss\n",
      "0.003224414074793458\n",
      "loss\n",
      "0.0066930875182151794\n",
      "loss\n",
      "0.002837918698787689\n",
      "loss\n",
      "0.012988959439098835\n",
      "loss\n",
      "0.005118361674249172\n",
      "loss\n",
      "0.002372785471379757\n",
      "loss\n",
      "0.002551397541537881\n",
      "loss\n",
      "0.002693360671401024\n",
      "loss\n",
      "0.005424067843705416\n",
      "loss\n",
      "0.029661312699317932\n",
      "loss\n",
      "0.004786224570125341\n",
      "loss\n",
      "0.002189621329307556\n",
      "loss\n",
      "0.003100590081885457\n",
      "loss\n",
      "0.0031093843281269073\n",
      "loss\n",
      "0.009124367497861385\n",
      "loss\n",
      "0.003724069334566593\n",
      "loss\n",
      "0.02314271405339241\n",
      "loss\n",
      "0.0025100174825638533\n",
      "loss\n",
      "0.0035209585912525654\n",
      "loss\n",
      "0.002444852376356721\n",
      "loss\n",
      "0.005885177291929722\n",
      "loss\n",
      "0.013309091329574585\n",
      "loss\n",
      "0.10153012722730637\n",
      "loss\n",
      "0.004611808806657791\n",
      "loss\n",
      "0.0028217521030455828\n",
      "loss\n",
      "0.015542215667665005\n",
      "loss\n",
      "0.002033786615356803\n",
      "loss\n",
      "0.0014035383937880397\n",
      "loss\n",
      "0.0029042467940598726\n",
      "loss\n",
      "0.0021230080164968967\n",
      "loss\n",
      "0.003525234991684556\n",
      "loss\n",
      "0.0052865236066281796\n",
      "loss\n",
      "0.05563344433903694\n",
      "loss\n",
      "1.2306580543518066\n",
      "loss\n",
      "0.002805347554385662\n",
      "loss\n",
      "0.008296666666865349\n",
      "loss\n",
      "0.0025618611834943295\n",
      "loss\n",
      "0.0029922020621597767\n",
      "loss\n",
      "0.001327824778854847\n",
      "loss\n",
      "0.004977691452950239\n",
      "loss\n",
      "0.008818963542580605\n",
      "loss\n",
      "0.001708239782601595\n",
      "loss\n",
      "0.7690757513046265\n",
      "loss\n",
      "0.015876196324825287\n",
      "loss\n",
      "0.003787725931033492\n",
      "loss\n",
      "0.032406941056251526\n",
      "loss\n",
      "0.0013309201458469033\n",
      "loss\n",
      "0.0024917051196098328\n",
      "loss\n",
      "0.001479484373703599\n",
      "loss\n",
      "0.003427822608500719\n",
      "loss\n",
      "0.003249604720622301\n",
      "loss\n",
      "0.004893707111477852\n",
      "loss\n",
      "0.011703873984515667\n",
      "loss\n",
      "0.005132000893354416\n",
      "loss\n",
      "0.018576689064502716\n",
      "loss\n",
      "0.009199729189276695\n",
      "loss\n",
      "0.005308579187840223\n",
      "loss\n",
      "0.007215393707156181\n",
      "loss\n",
      "0.0033021229319274426\n",
      "loss\n",
      "0.025185471400618553\n",
      "loss\n",
      "0.001508289948105812\n",
      "loss\n",
      "0.007013701368123293\n",
      "loss\n",
      "0.05054481700062752\n",
      "loss\n",
      "0.0034468306694179773\n",
      "loss\n",
      "0.006219674367457628\n",
      "loss\n",
      "0.0019921474158763885\n",
      "loss\n",
      "0.0024717275518924\n",
      "loss\n",
      "0.0010458719916641712\n",
      "loss\n",
      "0.0013863962376490235\n",
      "loss\n",
      "0.00099871342536062\n",
      "loss\n",
      "0.003458235412836075\n",
      "loss\n",
      "0.0014266322832554579\n",
      "loss\n",
      "0.0684390440583229\n",
      "loss\n",
      "0.0028136686887592077\n",
      "loss\n",
      "0.0014552014181390405\n",
      "loss\n",
      "0.0019932182040065527\n",
      "loss\n",
      "0.0014911495381966233\n",
      "loss\n",
      "0.07449249923229218\n",
      "loss\n",
      "0.009254061616957188\n",
      "loss\n",
      "0.0026251161471009254\n",
      "loss\n",
      "0.0013711584033444524\n",
      "loss\n",
      "0.005050757434219122\n",
      "loss\n",
      "0.00182819040492177\n",
      "loss\n",
      "0.002353043295443058\n",
      "loss\n",
      "0.0033620046451687813\n",
      "loss\n",
      "0.0028780964203178883\n",
      "loss\n",
      "0.001177094760350883\n",
      "loss\n",
      "0.0012528197839856148\n",
      "loss\n",
      "0.005170782096683979\n",
      "loss\n",
      "0.006078684702515602\n",
      "loss\n",
      "0.0027772923931479454\n",
      "loss\n",
      "0.0025754161179065704\n",
      "loss\n",
      "0.0015905360924080014\n",
      "loss\n",
      "0.06839874386787415\n",
      "loss\n",
      "0.0038691910449415445\n",
      "loss\n",
      "0.00432888139039278\n",
      "loss\n",
      "0.0014771036803722382\n",
      "loss\n",
      "0.0012421043356880546\n",
      "loss\n",
      "0.0008753282018005848\n",
      "loss\n",
      "0.0018992258701473475\n",
      "loss\n",
      "0.0033263610675930977\n",
      "loss\n",
      "0.0013111574808135629\n",
      "loss\n",
      "0.0015920833684504032\n",
      "loss\n",
      "0.0006268443539738655\n",
      "loss\n",
      "0.001416633022017777\n",
      "loss\n",
      "0.0014332984574139118\n",
      "loss\n",
      "0.004780292976647615\n",
      "loss\n",
      "0.1666361540555954\n",
      "loss\n",
      "0.0019267105963081121\n",
      "loss\n",
      "0.001359730027616024\n",
      "loss\n",
      "0.007523654028773308\n",
      "loss\n",
      "0.0007602662080898881\n",
      "loss\n",
      "0.0053190141916275024\n",
      "loss\n",
      "0.0015573289711028337\n",
      "loss\n",
      "0.0013621109537780285\n",
      "loss\n",
      "0.0012852036161348224\n",
      "loss\n",
      "0.0010589712765067816\n",
      "loss\n",
      "0.0019512200960889459\n",
      "loss\n",
      "0.002201516181230545\n",
      "loss\n",
      "0.0011470888275653124\n",
      "loss\n",
      "0.0015368566382676363\n",
      "loss\n",
      "0.005285100545734167\n",
      "loss\n",
      "0.00283316383138299\n",
      "loss\n",
      "0.002679450437426567\n",
      "loss\n",
      "0.0008553183870390058\n",
      "loss\n",
      "0.005783846136182547\n",
      "loss\n",
      "0.0013325868640094995\n",
      "loss\n",
      "0.0020808966364711523\n",
      "loss\n",
      "0.0010180057724937797\n",
      "loss\n",
      "0.0015910121146589518\n",
      "loss\n",
      "0.002381110331043601\n",
      "loss\n",
      "0.0018386616138741374\n",
      "loss\n",
      "0.0021241975482553244\n",
      "loss\n",
      "0.0017482249531894922\n",
      "loss\n",
      "0.0012935374397784472\n",
      "loss\n",
      "0.0007312007946893573\n",
      "loss\n",
      "0.0012354368809610605\n",
      "loss\n",
      "0.0022005646023899317\n",
      "loss\n",
      "0.003978315275162458\n",
      "loss\n",
      "0.0013494918821379542\n",
      "loss\n",
      "0.002420830773189664\n",
      "loss\n",
      "0.003981996327638626\n",
      "loss\n",
      "0.0010885033989325166\n",
      "loss\n",
      "0.0021873614750802517\n",
      "loss\n",
      "0.001004667836241424\n",
      "loss\n",
      "0.0024986020289361477\n",
      "loss\n",
      "0.004319860599935055\n",
      "loss\n",
      "0.0005863377591595054\n",
      "loss\n",
      "0.0010850501712411642\n",
      "loss\n",
      "0.0016085079405456781\n",
      "loss\n",
      "0.0014430596493184566\n",
      "loss\n",
      "0.0023011888843029737\n",
      "loss\n",
      "0.0010885033989325166\n",
      "loss\n",
      "0.006295610684901476\n",
      "loss\n",
      "0.001369729870930314\n",
      "loss\n",
      "0.0011634016409516335\n",
      "loss\n",
      "0.0015768486773595214\n",
      "loss\n",
      "0.0007338214782066643\n",
      "loss\n",
      "0.001019553979858756\n",
      "loss\n",
      "0.0008622265886515379\n",
      "loss\n",
      "0.0012063853209838271\n",
      "loss\n",
      "0.0012644876260310411\n",
      "loss\n",
      "0.0010768335778266191\n",
      "loss\n",
      "0.0011632826644927263\n",
      "loss\n",
      "0.0032733690459281206\n",
      "loss\n",
      "0.001688008545897901\n",
      "loss\n",
      "0.0009617946925573051\n",
      "loss\n",
      "0.0008934320067055523\n",
      "loss\n",
      "0.0010639727115631104\n",
      "loss\n",
      "0.004857524763792753\n",
      "loss\n",
      "0.0022261380217969418\n",
      "loss\n",
      "0.003475104458630085\n",
      "loss\n",
      "0.0010546842822805047\n",
      "loss\n",
      "0.0016202905680984259\n",
      "loss\n",
      "0.0008507922757416964\n",
      "loss\n",
      "0.001911718980409205\n",
      "loss\n",
      "0.0008288762182928622\n",
      "loss\n",
      "0.0038984029088169336\n",
      "loss\n",
      "0.0010794533882290125\n",
      "loss\n",
      "0.0025000290479511023\n",
      "loss\n",
      "0.06439148634672165\n",
      "loss\n",
      "0.0005739472107961774\n",
      "loss\n",
      "0.0867927223443985\n",
      "loss\n",
      "0.0008388814167119563\n",
      "loss\n",
      "0.0008179179858416319\n",
      "loss\n",
      "0.0015631611458957195\n",
      "loss\n",
      "0.0008789013954810798\n",
      "loss\n",
      "0.001069450518116355\n",
      "loss\n",
      "0.0004976941272616386\n",
      "loss\n",
      "0.0016221948899328709\n",
      "loss\n",
      "0.00044431351125240326\n",
      "loss\n",
      "0.0019490785198286176\n",
      "loss\n",
      "0.0010580186499282718\n",
      "loss\n",
      "0.003044258337467909\n",
      "loss\n",
      "0.0017691688844934106\n",
      "loss\n",
      "0.0010022860951721668\n",
      "loss\n",
      "0.0004898302140645683\n",
      "loss\n",
      "0.000834117061458528\n",
      "loss\n",
      "0.0032225127797573805\n",
      "loss\n",
      "0.006845948286354542\n",
      "loss\n",
      "0.005816676188260317\n",
      "loss\n",
      "0.0025216706562787294\n",
      "loss\n",
      "0.0019045800436288118\n",
      "loss\n",
      "0.013794328086078167\n",
      "loss\n",
      "0.0015889888163655996\n",
      "loss\n",
      "0.0017301365733146667\n",
      "loss\n",
      "0.0013323486782610416\n",
      "loss\n",
      "0.0006561510381288826\n",
      "loss\n",
      "0.010727358050644398\n",
      "loss\n",
      "0.0008112476789392531\n",
      "loss\n",
      "0.0013940150383859873\n",
      "loss\n",
      "0.0067525296472013\n",
      "loss\n",
      "0.001179595128633082\n",
      "loss\n",
      "1.133132815361023\n",
      "loss\n",
      "0.0005555993411689997\n",
      "loss\n",
      "0.000635183765552938\n",
      "loss\n",
      "0.003932482097297907\n",
      "loss\n",
      "0.0025434307754039764\n",
      "loss\n",
      "0.002568519674241543\n",
      "loss\n",
      "0.0027159492019563913\n",
      "loss\n",
      "0.5777732133865356\n",
      "loss\n",
      "0.0015739921946078539\n",
      "loss\n",
      "0.009132400155067444\n",
      "loss\n",
      "0.0021394239738583565\n",
      "loss\n",
      "0.0009253510506823659\n",
      "loss\n",
      "0.004545712377876043\n",
      "loss\n",
      "0.0010368215152993798\n",
      "loss\n",
      "0.0029525042045861483\n",
      "loss\n",
      "0.0005034133209846914\n",
      "loss\n",
      "0.00035565727739594877\n",
      "loss\n",
      "0.0013454442378133535\n",
      "loss\n",
      "0.0008417400531470776\n",
      "loss\n",
      "0.00573691027238965\n",
      "loss\n",
      "0.0008646087371744215\n",
      "loss\n",
      "0.004140377044677734\n",
      "loss\n",
      "0.002090056659653783\n",
      "loss\n",
      "0.0017508429009467363\n",
      "loss\n",
      "0.0005458295345306396\n",
      "loss\n",
      "0.007576303090900183\n",
      "loss\n",
      "0.002320099389180541\n",
      "loss\n",
      "0.006270022597163916\n",
      "loss\n",
      "0.0028695380315184593\n",
      "loss\n",
      "0.0013546108966693282\n",
      "loss\n",
      "0.0011938833631575108\n",
      "loss\n",
      "0.0004612335760612041\n",
      "loss\n",
      "0.0020207001361995935\n",
      "loss\n",
      "0.0028019000310450792\n",
      "loss\n",
      "0.0006723527330905199\n",
      "loss\n",
      "0.0005999195855110884\n",
      "loss\n",
      "0.0004219118563923985\n",
      "loss\n",
      "0.0046008918434381485\n",
      "loss\n",
      "0.002042233245447278\n",
      "loss\n",
      "0.006754186935722828\n",
      "loss\n",
      "0.0007593132322654128\n",
      "loss\n",
      "0.00041547726141288877\n",
      "loss\n",
      "0.0008174415561370552\n",
      "loss\n",
      "0.0022442173212766647\n",
      "loss\n",
      "0.000581572181545198\n",
      "loss\n",
      "0.003397290362045169\n",
      "loss\n",
      "0.0006323245470412076\n",
      "loss\n",
      "0.0019155264599248767\n",
      "loss\n",
      "0.0005765683017671108\n",
      "loss\n",
      "0.0003532739356160164\n",
      "loss\n",
      "0.002211745595559478\n",
      "loss\n",
      "0.03702251985669136\n",
      "loss\n",
      "0.0009205871028825641\n",
      "loss\n",
      "0.008095311000943184\n",
      "loss\n",
      "0.0005775213940069079\n",
      "loss\n",
      "0.0011470888275653124\n",
      "loss\n",
      "0.0011156531982123852\n",
      "loss\n",
      "0.0019378946162760258\n",
      "loss\n",
      "0.003487696871161461\n",
      "loss\n",
      "0.0017197832930833101\n",
      "loss\n",
      "0.0022155519109219313\n",
      "loss\n",
      "0.0010141950333490968\n",
      "loss\n",
      "0.07641874998807907\n",
      "loss\n",
      "0.0013409203384071589\n",
      "loss\n",
      "0.001179595128633082\n",
      "loss\n",
      "0.0014016337227076292\n",
      "loss\n",
      "0.017542092129588127\n",
      "loss\n",
      "0.0032928551081568003\n",
      "loss\n",
      "0.0005334384622983634\n",
      "loss\n",
      "0.0008803306263871491\n",
      "loss\n",
      "0.0032828745897859335\n",
      "loss\n",
      "0.0023209319915622473\n",
      "loss\n",
      "0.0004161922261118889\n",
      "loss\n",
      "0.0004433602443896234\n",
      "loss\n",
      "0.0006065912893973291\n",
      "loss\n",
      "0.0072313714772462845\n",
      "loss\n",
      "0.0024629279505461454\n",
      "loss\n",
      "0.0021594080608338118\n",
      "loss\n",
      "0.0022205475252121687\n",
      "loss\n",
      "0.0010596857173368335\n",
      "loss\n",
      "0.001319729257375002\n",
      "loss\n",
      "0.009429323486983776\n",
      "loss\n",
      "0.002771110739558935\n",
      "loss\n",
      "0.001105888863094151\n",
      "loss\n",
      "0.006941134110093117\n",
      "loss\n",
      "0.0014033003244549036\n",
      "loss\n",
      "0.00024911639047786593\n",
      "loss\n",
      "0.00403613829985261\n",
      "loss\n",
      "0.0013616346986964345\n",
      "loss\n",
      "0.002226970624178648\n",
      "loss\n",
      "0.0029974314384162426\n",
      "loss\n",
      "0.0010115751065313816\n",
      "loss\n",
      "0.0009703694959171116\n",
      "loss\n",
      "0.0023328252136707306\n",
      "loss\n",
      "0.0012416280806064606\n",
      "loss\n",
      "0.008411219343543053\n",
      "loss\n",
      "0.0014676999999210238\n",
      "loss\n",
      "0.00047267231275327504\n",
      "loss\n",
      "0.000969535845797509\n",
      "loss\n",
      "0.00026556302327662706\n",
      "loss\n",
      "0.00041261743172071874\n",
      "loss\n",
      "0.0005370128201320767\n",
      "loss\n",
      "0.001522930571809411\n",
      "loss\n",
      "0.0008607972995378077\n",
      "loss\n",
      "0.001702408422715962\n",
      "loss\n",
      "0.00075049843871966\n",
      "loss\n",
      "0.0002951186615973711\n",
      "loss\n",
      "0.0009332115878351033\n",
      "loss\n",
      "0.0014611531514674425\n",
      "loss\n",
      "0.0015103134792298079\n",
      "loss\n",
      "0.0012911563972011209\n",
      "loss\n",
      "0.000649956171400845\n",
      "loss\n",
      "0.002857889048755169\n",
      "loss\n",
      "0.001141611486673355\n",
      "loss\n",
      "0.0008865240379236639\n",
      "loss\n",
      "0.004283539019525051\n",
      "loss\n",
      "0.0007805161876603961\n",
      "loss\n",
      "0.0007681279676035047\n",
      "loss\n",
      "0.0013560395454987884\n",
      "loss\n",
      "0.0003937899600714445\n",
      "loss\n",
      "0.002218644367530942\n",
      "loss\n",
      "0.0014017528155818582\n",
      "loss\n",
      "0.0006868863711133599\n",
      "loss\n",
      "0.026876559481024742\n",
      "loss\n",
      "0.002644615015015006\n",
      "loss\n",
      "0.0009517907164990902\n",
      "loss\n",
      "0.000523430178873241\n",
      "loss\n",
      "0.003852209774777293\n",
      "loss\n",
      "0.0003496989083942026\n",
      "loss\n",
      "0.00044764988706447184\n",
      "loss\n",
      "0.001259963377378881\n",
      "loss\n",
      "0.002960229991003871\n",
      "loss\n",
      "0.001384491566568613\n",
      "loss\n",
      "0.0019179059891030192\n",
      "loss\n",
      "0.0013440155889838934\n",
      "loss\n",
      "0.0005482124397531152\n",
      "loss\n",
      "0.005588384345173836\n",
      "loss\n",
      "0.0002812943421304226\n",
      "loss\n",
      "0.0008534126682206988\n",
      "loss\n",
      "0.0009110590908676386\n",
      "loss\n",
      "0.0007183355046436191\n",
      "loss\n",
      "0.0003687655262183398\n",
      "loss\n",
      "0.0008818790083751082\n",
      "loss\n",
      "0.0012532960390672088\n",
      "loss\n",
      "0.002041638595983386\n",
      "loss\n",
      "0.002261463785544038\n",
      "loss\n",
      "0.0010599239030852914\n",
      "loss\n",
      "0.0013654442736878991\n",
      "loss\n",
      "0.004441278520971537\n",
      "loss\n",
      "0.0003947432560380548\n",
      "loss\n",
      "0.010698935016989708\n",
      "loss\n",
      "0.000696654780767858\n",
      "loss\n",
      "0.0009991897968575358\n",
      "loss\n",
      "0.00774192251265049\n",
      "loss\n",
      "0.0011189873330295086\n",
      "loss\n",
      "0.0007273888913914561\n",
      "loss\n",
      "0.0004629017203114927\n",
      "loss\n",
      "0.00035232058144174516\n",
      "loss\n",
      "0.003114256775006652\n",
      "loss\n",
      "0.005950829479843378\n",
      "loss\n",
      "0.0010342017048969865\n",
      "loss\n",
      "0.00035398892941884696\n",
      "loss\n",
      "0.0009184433147311211\n",
      "loss\n",
      "0.00034850722295232117\n",
      "loss\n",
      "0.0016182672698050737\n",
      "loss\n",
      "0.0009114163694903255\n",
      "loss\n",
      "0.010211721062660217\n",
      "loss\n",
      "0.0008002892718650401\n",
      "loss\n",
      "0.0028330450877547264\n",
      "loss\n",
      "0.000977038755081594\n",
      "loss\n",
      "0.001192573574371636\n",
      "loss\n",
      "0.0009078433504328132\n",
      "loss\n",
      "0.0004047528200317174\n",
      "loss\n",
      "0.0012709167785942554\n",
      "loss\n",
      "0.0021843877620995045\n",
      "loss\n",
      "0.0014002051902934909\n",
      "loss\n",
      "0.001053255284205079\n",
      "loss\n",
      "0.0003840185818262398\n",
      "loss\n",
      "0.0006049233488738537\n",
      "loss\n",
      "0.0006123098428361118\n",
      "loss\n",
      "0.0007838514284230769\n",
      "loss\n",
      "0.005385533440858126\n",
      "loss\n",
      "0.0015470929211005569\n",
      "loss\n",
      "0.0017260904423892498\n",
      "loss\n",
      "0.002130145439878106\n",
      "loss\n",
      "0.000745018885936588\n",
      "loss\n",
      "0.0004693360242526978\n",
      "loss\n",
      "0.0018774517811834812\n",
      "loss\n",
      "0.0003051292151212692\n",
      "loss\n",
      "0.0018917298875749111\n",
      "loss\n",
      "0.0010977915953844786\n",
      "loss\n",
      "0.001210909802466631\n",
      "loss\n",
      "0.0024791003670543432\n",
      "loss\n",
      "0.002086844528093934\n",
      "loss\n",
      "0.0011520899133756757\n",
      "loss\n",
      "0.00046957432641647756\n",
      "loss\n",
      "0.0006668727728538215\n",
      "loss\n",
      "0.0006725909770466387\n",
      "loss\n",
      "0.0010973153403028846\n",
      "loss\n",
      "0.0013992529129609466\n",
      "loss\n",
      "0.0011435167398303747\n",
      "loss\n",
      "0.0009629856795072556\n",
      "loss\n",
      "0.009789552539587021\n",
      "loss\n",
      "0.00024577934527769685\n",
      "loss\n",
      "0.0004914983292110264\n",
      "loss\n",
      "0.00042906138696707785\n",
      "loss\n",
      "0.005800202023237944\n",
      "loss\n",
      "0.0008683010237291455\n",
      "loss\n",
      "0.00041976699139922857\n",
      "loss\n",
      "0.0016599221853539348\n",
      "loss\n",
      "0.0010418231831863523\n",
      "loss\n",
      "0.0007952864980325103\n",
      "loss\n",
      "0.0007488307310268283\n",
      "loss\n",
      "0.00200130813755095\n",
      "loss\n",
      "0.0012532960390672088\n",
      "loss\n",
      "0.0005787128466181457\n",
      "loss\n",
      "0.0008704449282959104\n",
      "loss\n",
      "0.0008831891464069486\n",
      "loss\n",
      "0.00041976699139922857\n",
      "loss\n",
      "0.0006168370018713176\n",
      "loss\n",
      "0.0010122895473614335\n",
      "loss\n",
      "0.000797192333266139\n",
      "loss\n",
      "0.001840565470047295\n",
      "loss\n",
      "0.0009284476400353014\n",
      "loss\n",
      "0.002931585069745779\n",
      "loss\n",
      "0.00023958197562023997\n",
      "loss\n",
      "0.0002543602604418993\n",
      "loss\n",
      "0.0008665143977850676\n",
      "loss\n",
      "0.0005872909096069634\n",
      "loss\n",
      "0.0009951406391337514\n",
      "loss\n",
      "0.0030088413041085005\n",
      "loss\n",
      "4.661959648132324\n",
      "loss\n",
      "0.012682733125984669\n",
      "loss\n",
      "0.0011266082292422652\n",
      "loss\n",
      "0.0011711412807926536\n",
      "loss\n",
      "0.0006866481271572411\n",
      "loss\n",
      "0.005445883143693209\n",
      "loss\n",
      "0.0044141001999378204\n",
      "loss\n",
      "0.0148676922544837\n",
      "loss\n",
      "0.0033791130408644676\n",
      "loss\n",
      "0.01798722706735134\n",
      "loss\n",
      "0.0019363479223102331\n",
      "loss\n",
      "0.0010003806091845036\n",
      "loss\n",
      "0.001105412608012557\n",
      "loss\n",
      "0.000695463502779603\n",
      "loss\n",
      "0.0007178590167313814\n",
      "loss\n",
      "0.058921705931425095\n",
      "loss\n",
      "0.0010342017048969865\n",
      "loss\n",
      "0.002141089178621769\n",
      "loss\n",
      "0.0011211306555196643\n",
      "loss\n",
      "0.06521012634038925\n",
      "loss\n",
      "0.0007843278581276536\n",
      "loss\n",
      "0.0017260904423892498\n",
      "loss\n",
      "0.0006530536338686943\n",
      "loss\n",
      "0.002709410386160016\n",
      "loss\n",
      "0.0004638549580704421\n",
      "loss\n",
      "0.007267585955560207\n",
      "loss\n",
      "0.005277866963297129\n",
      "loss\n",
      "0.0009411911014467478\n",
      "loss\n",
      "0.0005138983833603561\n",
      "loss\n",
      "0.0004433602443896234\n",
      "loss\n",
      "0.002880473854020238\n",
      "loss\n",
      "0.026496661826968193\n",
      "loss\n",
      "0.001375206047669053\n",
      "loss\n",
      "0.002098502591252327\n",
      "loss\n",
      "0.37235745787620544\n",
      "loss\n",
      "0.0003971264814026654\n",
      "loss\n",
      "0.0007009433466009796\n",
      "loss\n",
      "0.006032118573784828\n",
      "loss\n",
      "0.0004629017203114927\n",
      "loss\n",
      "0.001023483811877668\n",
      "loss\n",
      "0.00044550508027896285\n",
      "loss\n",
      "0.0014649622607976198\n",
      "loss\n",
      "0.0023430532310158014\n",
      "loss\n",
      "0.0051649706438183784\n",
      "loss\n",
      "0.0011328000109642744\n",
      "loss\n",
      "0.012277033179998398\n",
      "loss\n",
      "0.0009029601933434606\n",
      "loss\n",
      "0.0016807490028440952\n",
      "loss\n",
      "0.0008746135863475502\n",
      "loss\n",
      "0.0005560758872888982\n",
      "loss\n",
      "0.0037347583565860987\n",
      "loss\n",
      "0.0024947968777269125\n",
      "loss\n",
      "0.005204224959015846\n",
      "loss\n",
      "0.0009472650708630681\n",
      "loss\n",
      "0.000876757490914315\n",
      "loss\n",
      "0.0005915798828937113\n",
      "loss\n",
      "0.0004741021548397839\n",
      "loss\n",
      "0.0028621682431548834\n",
      "loss\n",
      "0.0012935374397784472\n",
      "loss\n",
      "0.0007281036232598126\n",
      "loss\n",
      "0.001086598145775497\n",
      "loss\n",
      "0.0012180536286905408\n",
      "loss\n",
      "0.0010181248653680086\n",
      "loss\n",
      "0.0006835508393123746\n",
      "loss\n",
      "0.0008760428754612803\n",
      "loss\n",
      "0.001959786517545581\n",
      "loss\n",
      "0.0006316096987575293\n",
      "loss\n",
      "0.005911367479711771\n",
      "loss\n",
      "0.001192573574371636\n",
      "loss\n",
      "0.0007014198345132172\n",
      "loss\n",
      "0.001320443581789732\n",
      "loss\n",
      "0.0007500219508074224\n",
      "loss\n",
      "0.0008264940115623176\n",
      "loss\n",
      "0.0005551227368414402\n",
      "loss\n",
      "0.0007950482540763915\n",
      "loss\n",
      "0.002404181519523263\n",
      "loss\n",
      "0.0028921226039528847\n",
      "loss\n",
      "0.06963595002889633\n",
      "loss\n",
      "0.0018892312655225396\n",
      "loss\n",
      "0.002776341512799263\n",
      "loss\n",
      "1.097870945930481\n",
      "loss\n",
      "0.0005777596961706877\n",
      "loss\n",
      "0.0008172033121809363\n",
      "loss\n",
      "0.009694874286651611\n",
      "loss\n",
      "0.006388006266206503\n",
      "loss\n",
      "0.0017229963559657335\n",
      "loss\n",
      "0.001210671616718173\n",
      "loss\n",
      "0.0014313939027488232\n",
      "loss\n",
      "0.008590529672801495\n",
      "loss\n",
      "0.00047743841423653066\n",
      "loss\n",
      "0.0011382774682715535\n",
      "loss\n",
      "0.0026242840103805065\n",
      "loss\n",
      "0.0003644755925051868\n",
      "loss\n",
      "0.0015656605828553438\n",
      "loss\n",
      "0.00157803890760988\n",
      "loss\n",
      "0.0009129646932706237\n",
      "loss\n",
      "0.0011154150124639273\n",
      "loss\n",
      "0.001625765347853303\n",
      "loss\n",
      "2.8665642738342285\n",
      "loss\n",
      "0.012164924293756485\n",
      "loss\n",
      "0.0004950728034600616\n",
      "loss\n",
      "0.0003947432560380548\n",
      "loss\n",
      "0.005643506534397602\n",
      "loss\n",
      "0.00040451448876410723\n",
      "loss\n",
      "0.00030298411729745567\n",
      "loss\n",
      "0.04739883169531822\n",
      "loss\n",
      "0.036705754697322845\n",
      "loss\n",
      "0.00945920031517744\n",
      "loss\n",
      "0.00024720950750634074\n",
      "loss\n",
      "0.061474040150642395\n",
      "loss\n",
      "0.0006235085893422365\n",
      "loss\n",
      "0.007738137152045965\n",
      "loss\n",
      "0.00026246439665555954\n",
      "loss\n",
      "0.04900194704532623\n",
      "loss\n",
      "0.00022837892174720764\n",
      "loss\n",
      "0.021302001550793648\n",
      "loss\n",
      "0.07682917267084122\n",
      "loss\n",
      "0.0003003622987307608\n",
      "loss\n",
      "0.03357384353876114\n",
      "loss\n",
      "0.0002033503697020933\n",
      "loss\n",
      "0.00017331528943032026\n",
      "loss\n",
      "0.0002858230145648122\n",
      "loss\n",
      "0.012821382842957973\n",
      "loss\n",
      "0.016321489587426186\n",
      "loss\n",
      "0.08305186033248901\n",
      "loss\n",
      "0.0014668668154627085\n",
      "loss\n",
      "0.00033158526639454067\n",
      "loss\n",
      "0.018014157190918922\n",
      "loss\n",
      "0.00038211196078918874\n",
      "loss\n",
      "0.05664663016796112\n",
      "loss\n",
      "0.0001652104256208986\n",
      "loss\n",
      "0.0010937429033219814\n",
      "loss\n",
      "0.0409710630774498\n",
      "loss\n",
      "0.040607012808322906\n",
      "loss\n",
      "0.0007090438157320023\n",
      "loss\n",
      "0.005363598000258207\n",
      "loss\n",
      "0.00033968876232393086\n",
      "loss\n",
      "0.0037461596075445414\n",
      "loss\n",
      "0.000534868217073381\n",
      "loss\n",
      "0.00042715485324151814\n",
      "loss\n",
      "0.005437346640974283\n",
      "loss\n",
      "0.0005265279905870557\n",
      "loss\n",
      "0.018740614876151085\n",
      "loss\n",
      "0.005633667577058077\n",
      "loss\n",
      "0.002406084444373846\n",
      "loss\n",
      "0.0005832401220686734\n",
      "loss\n",
      "0.0005103239673189819\n",
      "loss\n",
      "0.0033570146188139915\n",
      "loss\n",
      "0.007710220292210579\n",
      "loss\n",
      "0.0002157455455744639\n",
      "loss\n",
      "0.004921702668070793\n",
      "loss\n",
      "0.00028391621890477836\n",
      "loss\n",
      "0.009648121893405914\n",
      "loss\n",
      "0.0002779574424494058\n",
      "loss\n",
      "0.000506511190906167\n",
      "loss\n",
      "0.00031704644788987935\n",
      "loss\n",
      "0.002370406873524189\n",
      "loss\n",
      "0.0003496989083942026\n",
      "loss\n",
      "0.0037905762437731028\n",
      "loss\n",
      "0.003954449202865362\n",
      "loss\n",
      "0.00046695294440723956\n",
      "loss\n",
      "0.00443332688882947\n",
      "loss\n",
      "0.0001833270798670128\n",
      "loss\n",
      "0.0022333934903144836\n",
      "loss\n",
      "0.003699603257700801\n",
      "loss\n",
      "0.0019722788129001856\n",
      "loss\n",
      "0.010431881994009018\n",
      "loss\n",
      "0.0002743821241892874\n",
      "loss\n",
      "6.502380847930908\n",
      "loss\n",
      "0.0034477810841053724\n",
      "loss\n",
      "0.0003575639275368303\n",
      "loss\n",
      "0.0002674698771443218\n",
      "loss\n",
      "0.0030373651534318924\n",
      "loss\n",
      "0.0006556744920089841\n",
      "loss\n",
      "0.002501931507140398\n",
      "loss\n",
      "0.0031947072129696608\n",
      "loss\n",
      "0.0007147617870941758\n",
      "loss\n",
      "0.0033837463706731796\n",
      "loss\n",
      "0.001416394836269319\n",
      "loss\n",
      "0.0024562685284763575\n",
      "loss\n",
      "0.00471147894859314\n",
      "loss\n",
      "0.0022033003624528646\n",
      "loss\n",
      "0.002617269055917859\n",
      "loss\n",
      "0.00243831193074584\n",
      "loss\n",
      "0.0016720612766221166\n",
      "loss\n",
      "0.12599332630634308\n",
      "loss\n",
      "0.0017636949196457863\n",
      "loss\n",
      "0.0017951102927327156\n",
      "loss\n",
      "0.00887355301529169\n",
      "loss\n",
      "0.007288059685379267\n",
      "loss\n",
      "0.0009672730811871588\n",
      "loss\n",
      "0.0016561138909310102\n",
      "loss\n",
      "0.005817742552608252\n",
      "loss\n",
      "0.0010451575508341193\n",
      "loss\n",
      "0.0009803733555600047\n",
      "loss\n",
      "0.022383779287338257\n",
      "loss\n",
      "0.0032427129335701466\n",
      "loss\n",
      "0.0009266611887142062\n",
      "loss\n",
      "0.019285082817077637\n",
      "loss\n",
      "0.0009420248097740114\n",
      "loss\n",
      "0.0016465928638353944\n",
      "loss\n",
      "0.0023178397677838802\n",
      "loss\n",
      "0.002938360208645463\n",
      "loss\n",
      "0.08495984971523285\n",
      "loss\n",
      "0.0014054430648684502\n",
      "loss\n",
      "0.0009221353684552014\n",
      "loss\n",
      "0.00436816830188036\n",
      "loss\n",
      "0.0030544791370630264\n",
      "loss\n",
      "0.005048622377216816\n",
      "loss\n",
      "0.0060799880884587765\n",
      "loss\n",
      "0.016540072858333588\n",
      "loss\n",
      "0.2260076254606247\n",
      "loss\n",
      "0.007220128085464239\n",
      "loss\n",
      "0.004577396437525749\n",
      "loss\n",
      "0.0031294680666178465\n",
      "loss\n",
      "0.0011528043542057276\n",
      "loss\n",
      "0.008339227177202702\n",
      "loss\n",
      "0.0013830630341544747\n",
      "loss\n",
      "0.0027998790610581636\n",
      "loss\n",
      "0.001560185570269823\n",
      "loss\n",
      "0.0018848287872970104\n",
      "loss\n",
      "0.030961912125349045\n",
      "loss\n",
      "0.0022762122098356485\n",
      "loss\n",
      "0.003579401643946767\n",
      "loss\n",
      "0.0034686895087361336\n",
      "loss\n",
      "0.002000237349420786\n",
      "loss\n",
      "0.001800346071831882\n",
      "loss\n",
      "0.0035957936197519302\n",
      "loss\n",
      "0.014768090099096298\n",
      "loss\n",
      "0.0046996138989925385\n",
      "loss\n",
      "0.0067700534127652645\n",
      "loss\n",
      "0.0009396428358741105\n",
      "loss\n",
      "0.0017360866768285632\n",
      "loss\n",
      "0.001462581567466259\n",
      "loss\n",
      "0.0004752936656586826\n",
      "loss\n",
      "0.0007898071780800819\n",
      "loss\n",
      "0.0008774721063673496\n",
      "loss\n",
      "0.0011689979583024979\n",
      "loss\n",
      "0.004408997017890215\n",
      "loss\n",
      "0.01529852394014597\n",
      "loss\n",
      "0.0017256144201382995\n",
      "loss\n",
      "0.0005324853118509054\n",
      "loss\n",
      "0.00284944917075336\n",
      "loss\n",
      "0.3101171851158142\n",
      "loss\n",
      "0.0016018429305404425\n",
      "loss\n",
      "0.0008465044084005058\n",
      "loss\n",
      "0.00684417225420475\n",
      "loss\n",
      "0.0010065733222290874\n",
      "loss\n",
      "0.001503290724940598\n",
      "loss\n",
      "0.01349575724452734\n",
      "loss\n",
      "0.0005980133428238332\n",
      "loss\n",
      "0.005173865240067244\n",
      "loss\n",
      "0.0010562323732301593\n",
      "loss\n",
      "0.04587842524051666\n",
      "loss\n",
      "0.001095886342227459\n",
      "loss\n",
      "0.0011753087164834142\n",
      "loss\n",
      "0.000433112756581977\n",
      "loss\n",
      "0.0035584955476224422\n",
      "loss\n",
      "0.03456712141633034\n",
      "loss\n",
      "0.01112897228449583\n",
      "loss\n",
      "0.19578249752521515\n",
      "loss\n",
      "0.0008631794480606914\n",
      "loss\n",
      "0.0010082405060529709\n",
      "loss\n",
      "0.0028364923782646656\n",
      "loss\n",
      "0.0018275955226272345\n",
      "loss\n",
      "0.0023895539343357086\n",
      "loss\n",
      "0.000523430178873241\n",
      "loss\n",
      "0.00041214076918549836\n",
      "loss\n",
      "0.0006101653561927378\n",
      "loss\n",
      "0.0008522216230630875\n",
      "loss\n",
      "0.0012179345358163118\n",
      "loss\n",
      "0.0008753282018005848\n",
      "loss\n",
      "0.0016235039802268147\n",
      "loss\n",
      "0.0009777533123269677\n",
      "loss\n",
      "0.001444011926651001\n",
      "loss\n",
      "0.0035663354210555553\n",
      "loss\n",
      "0.01225253939628601\n",
      "loss\n",
      "0.000615407363511622\n",
      "loss\n",
      "0.002785851713269949\n",
      "loss\n",
      "0.0010449193650856614\n",
      "loss\n",
      "0.0018184330547228456\n",
      "loss\n",
      "0.0007669368060305715\n",
      "loss\n",
      "0.0027184458449482918\n",
      "loss\n",
      "0.001904699020087719\n",
      "loss\n",
      "0.0006840273272246122\n",
      "loss\n",
      "0.0010415849974378943\n",
      "loss\n",
      "0.000568228424526751\n",
      "loss\n",
      "0.0031387372873723507\n",
      "loss\n",
      "0.00719101307913661\n",
      "loss\n",
      "0.0006353028584271669\n",
      "loss\n",
      "0.000486970558995381\n",
      "loss\n",
      "0.0007688426994718611\n",
      "loss\n",
      "0.0007824220228940248\n",
      "loss\n",
      "0.0013804440386593342\n",
      "loss\n",
      "0.0009111781837418675\n",
      "loss\n",
      "0.0014972201315686107\n",
      "loss\n",
      "0.0006042085005901754\n",
      "loss\n",
      "0.0012735360069200397\n",
      "loss\n",
      "0.0013263961300253868\n",
      "loss\n",
      "0.0005981324939057231\n",
      "loss\n",
      "0.0006130246329121292\n",
      "loss\n",
      "0.0023889592848718166\n",
      "loss\n",
      "0.0011750705307349563\n",
      "loss\n",
      "0.0005891970940865576\n",
      "loss\n",
      "0.0004895919119007885\n",
      "loss\n",
      "0.0005679901223629713\n",
      "loss\n",
      "0.5016879439353943\n",
      "loss\n",
      "0.0017500099493190646\n",
      "loss\n",
      "0.000506511190906167\n",
      "loss\n",
      "0.0005539313424378633\n",
      "loss\n",
      "0.003807914676144719\n",
      "loss\n",
      "0.028101855888962746\n",
      "loss\n",
      "0.0008391196606680751\n",
      "loss\n",
      "0.0009672730811871588\n",
      "loss\n",
      "0.00046433156239800155\n",
      "loss\n",
      "0.1355494260787964\n",
      "loss\n",
      "0.000809818331617862\n",
      "loss\n",
      "0.005297314375638962\n",
      "loss\n",
      "0.0014513921923935413\n",
      "loss\n",
      "0.0007186928996816278\n",
      "loss\n",
      "0.0013705631718039513\n",
      "loss\n",
      "0.0024447336327284575\n",
      "loss\n",
      "0.0013624681159853935\n",
      "loss\n",
      "0.000847933697514236\n",
      "loss\n",
      "0.0033197076991200447\n",
      "loss\n",
      "0.0002531684876885265\n",
      "loss\n",
      "0.0006457865820266306\n",
      "loss\n",
      "0.00021288513380568475\n",
      "loss\n",
      "0.0028309053741395473\n",
      "loss\n",
      "0.0006840273272246122\n",
      "loss\n",
      "5.774600982666016\n",
      "loss\n",
      "0.0007955246837809682\n",
      "loss\n",
      "0.0029955299105495214\n",
      "loss\n",
      "1.699110507965088\n",
      "loss\n",
      "0.07665915042161942\n",
      "loss\n",
      "0.0006833125371485949\n",
      "loss\n",
      "0.0008222059695981443\n",
      "loss\n",
      "0.001655756845138967\n",
      "loss\n",
      "0.000842692912556231\n",
      "loss\n",
      "0.001057184999808669\n",
      "loss\n",
      "0.0017527469899505377\n",
      "loss\n",
      "0.0015608996618539095\n",
      "loss\n",
      "0.0008507922757416964\n",
      "loss\n",
      "0.001906721736304462\n",
      "loss\n",
      "0.0029006809927523136\n",
      "loss\n",
      "0.0015824426664039493\n",
      "loss\n",
      "0.006690955720841885\n",
      "loss\n",
      "0.026949666440486908\n",
      "loss\n",
      "0.005086457822471857\n",
      "loss\n",
      "0.0062572285532951355\n",
      "loss\n",
      "0.0013733012601733208\n",
      "loss\n",
      "0.015596204437315464\n",
      "loss\n",
      "0.004771632142364979\n",
      "loss\n",
      "0.010210777632892132\n",
      "loss\n",
      "0.0018556771101430058\n",
      "loss\n",
      "0.0012656782055273652\n",
      "loss\n",
      "0.001777260797098279\n",
      "loss\n",
      "0.0009158230968751013\n",
      "loss\n",
      "0.00970514491200447\n",
      "loss\n",
      "0.0015851801726967096\n",
      "loss\n",
      "0.007525546941906214\n",
      "loss\n",
      "0.0019395602867007256\n",
      "loss\n",
      "0.14365465939044952\n",
      "loss\n",
      "0.006396060809493065\n",
      "loss\n",
      "0.0021193204447627068\n",
      "loss\n",
      "0.007879721000790596\n",
      "loss\n",
      "0.03223589062690735\n",
      "loss\n",
      "0.0014585343888029456\n",
      "loss\n",
      "0.12759247422218323\n",
      "loss\n",
      "0.0017432268941774964\n",
      "loss\n",
      "0.0009039129945449531\n",
      "loss\n",
      "0.02098698727786541\n",
      "loss\n",
      "0.0011002921964973211\n",
      "loss\n",
      "0.015343839302659035\n",
      "loss\n",
      "0.0017241863533854485\n",
      "loss\n",
      "0.012848097831010818\n",
      "loss\n",
      "0.016196461394429207\n",
      "loss\n",
      "0.007988644763827324\n",
      "loss\n",
      "0.0008741371566429734\n",
      "loss\n",
      "0.0033289750572293997\n",
      "loss\n",
      "0.002584690460935235\n",
      "loss\n",
      "0.3376161456108093\n",
      "loss\n",
      "0.17622306942939758\n",
      "loss\n",
      "0.005285219289362431\n",
      "loss\n",
      "0.0030551922973245382\n",
      "loss\n",
      "0.0048184944316744804\n",
      "loss\n",
      "0.004540016409009695\n",
      "loss\n",
      "0.004049673210829496\n",
      "loss\n",
      "0.004293984733521938\n",
      "loss\n",
      "0.0020384264644235373\n",
      "loss\n",
      "0.6701012849807739\n",
      "loss\n",
      "0.006541150622069836\n",
      "loss\n",
      "0.0017422748496755958\n",
      "loss\n",
      "0.03246106579899788\n",
      "loss\n",
      "0.0052717006765306\n",
      "loss\n",
      "0.005346760619431734\n",
      "loss\n",
      "0.0010355116100981832\n",
      "loss\n",
      "0.0005392765742726624\n",
      "loss\n",
      "0.001001809723675251\n",
      "loss\n",
      "0.0026057357899844646\n",
      "loss\n",
      "0.003418674925342202\n",
      "loss\n",
      "0.0008293526479974389\n",
      "loss\n",
      "0.009370513260364532\n",
      "loss\n",
      "0.0006006343755871058\n",
      "loss\n",
      "0.003137667663395405\n",
      "loss\n",
      "0.0011179156135767698\n",
      "loss\n",
      "0.01869744248688221\n",
      "loss\n",
      "0.0004447901446837932\n",
      "loss\n",
      "0.001640761154703796\n",
      "loss\n",
      "0.0004435985756572336\n",
      "loss\n",
      "0.004437480587512255\n",
      "loss\n",
      "0.0004259632551111281\n",
      "loss\n",
      "0.02258472703397274\n",
      "loss\n",
      "0.0006771179032512009\n",
      "loss\n",
      "0.0015863704029470682\n",
      "loss\n",
      "0.0030540036968886852\n",
      "loss\n",
      "0.0007546676206402481\n",
      "loss\n",
      "0.002878334140405059\n",
      "loss\n",
      "0.0005322470096871257\n",
      "loss\n",
      "0.026510940864682198\n",
      "loss\n",
      "0.009304258041083813\n",
      "loss\n",
      "0.021776961162686348\n",
      "loss\n",
      "0.001404728856869042\n",
      "loss\n",
      "0.00036638224264606833\n",
      "loss\n",
      "0.0008524598088115454\n",
      "loss\n",
      "0.0027340196538716555\n",
      "loss\n",
      "0.002498839981853962\n",
      "loss\n",
      "0.0013705631718039513\n",
      "loss\n",
      "0.00899465661495924\n",
      "loss\n",
      "0.0012816318776458502\n",
      "loss\n",
      "0.001966211013495922\n",
      "loss\n",
      "0.0018600797047838569\n",
      "loss\n",
      "0.0019061268540099263\n",
      "loss\n",
      "0.0014174662064760923\n",
      "loss\n",
      "0.003535688389092684\n",
      "loss\n",
      "0.0014075858052819967\n",
      "loss\n",
      "0.0005794276366941631\n",
      "loss\n",
      "0.0028871302492916584\n",
      "loss\n",
      "0.0061088986694812775\n",
      "loss\n",
      "0.00042572495294734836\n",
      "loss\n",
      "0.000428108120104298\n",
      "loss\n",
      "0.0028246049769222736\n",
      "loss\n",
      "0.007277645170688629\n",
      "loss\n",
      "0.0010798105504363775\n",
      "loss\n",
      "0.0030859727412462234\n",
      "loss\n",
      "0.0006139777251519263\n",
      "loss\n",
      "0.0005782362422905862\n",
      "loss\n",
      "0.010593726299703121\n",
      "loss\n",
      "0.0012159105390310287\n",
      "loss\n",
      "0.0005053196800872684\n",
      "loss\n",
      "0.001134705264121294\n",
      "loss\n",
      "0.00047267231275327504\n",
      "loss\n",
      "0.000846266164444387\n",
      "loss\n",
      "0.0016955060418695211\n",
      "loss\n",
      "0.001279369811527431\n",
      "loss\n",
      "0.00779361417517066\n",
      "loss\n",
      "0.0016464737709611654\n",
      "loss\n",
      "0.003351786872372031\n",
      "loss\n",
      "0.005628214683383703\n",
      "loss\n",
      "0.0035672858357429504\n",
      "loss\n",
      "0.0005370128201320767\n",
      "loss\n",
      "0.0006885541952215135\n",
      "loss\n",
      "0.0004690977220889181\n",
      "loss\n",
      "0.0005258131423033774\n",
      "loss\n",
      "0.0017419178038835526\n",
      "loss\n",
      "0.001593511551618576\n",
      "loss\n",
      "0.0007316772826015949\n",
      "loss\n",
      "0.000707971747033298\n",
      "loss\n",
      "0.0006911749369464815\n",
      "loss\n",
      "0.002665896899998188\n",
      "loss\n",
      "0.0006254147156141698\n",
      "loss\n",
      "0.0015022194711491466\n",
      "loss\n",
      "0.009392124600708485\n",
      "loss\n",
      "0.012946008704602718\n",
      "loss\n",
      "0.0010157431242987514\n",
      "loss\n",
      "0.0014280608156695962\n",
      "loss\n",
      "0.003851853543892503\n",
      "loss\n",
      "0.000559173640795052\n",
      "loss\n",
      "0.0003511289251036942\n",
      "loss\n",
      "0.004013104364275932\n",
      "loss\n",
      "0.0014513921923935413\n",
      "loss\n",
      "0.030644262209534645\n",
      "loss\n",
      "0.0011582816950976849\n",
      "loss\n",
      "0.0014397265622392297\n",
      "loss\n",
      "0.0021531034726649523\n",
      "loss\n",
      "0.001191621064208448\n",
      "loss\n",
      "0.0016000575851649046\n",
      "loss\n",
      "0.0017477489309385419\n",
      "loss\n",
      "0.008792140521109104\n",
      "loss\n",
      "0.0028020190075039864\n",
      "loss\n",
      "0.0006766413571313024\n",
      "loss\n",
      "0.0018878034316003323\n",
      "loss\n",
      "0.0017471539322286844\n",
      "loss\n",
      "0.001256510615348816\n",
      "loss\n",
      "0.001255677198059857\n",
      "loss\n",
      "0.0038440159987658262\n",
      "loss\n",
      "0.0017959432443603873\n",
      "loss\n",
      "0.0016683719586580992\n",
      "loss\n",
      "0.0008314966107718647\n",
      "loss\n",
      "0.0022165034897625446\n",
      "loss\n",
      "0.0023671959061175585\n",
      "loss\n",
      "0.0002982171718031168\n",
      "loss\n",
      "0.014729914255440235\n",
      "loss\n",
      "0.0027272433508187532\n",
      "loss\n",
      "0.0008842610404826701\n",
      "loss\n",
      "0.0004936429904773831\n",
      "loss\n",
      "0.0007914748275652528\n",
      "loss\n",
      "0.0013392536202445626\n",
      "loss\n",
      "0.0038484097458422184\n",
      "loss\n",
      "0.0009913297835737467\n",
      "loss\n",
      "0.03474237769842148\n",
      "loss\n",
      "0.0005641775787808001\n",
      "loss\n",
      "0.0003849719068966806\n",
      "loss\n",
      "0.0013365155318751931\n",
      "loss\n",
      "0.0008220868767239153\n",
      "loss\n",
      "0.0014574630185961723\n",
      "loss\n",
      "0.0016183863626793027\n",
      "loss\n",
      "0.00040356122190132737\n",
      "loss\n",
      "0.0005911033367738128\n",
      "loss\n",
      "0.0027939353603869677\n",
      "loss\n",
      "0.00039641151670366526\n",
      "loss\n",
      "0.0027140469755977392\n",
      "loss\n",
      "0.0018346159486100078\n",
      "loss\n",
      "0.002303329762071371\n",
      "loss\n",
      "0.0006030171643942595\n",
      "loss\n",
      "0.009124013595283031\n",
      "loss\n",
      "0.0012161486083641648\n",
      "loss\n",
      "0.0027712297160178423\n",
      "loss\n",
      "0.0002739054325502366\n",
      "loss\n",
      "0.0006935574929229915\n",
      "loss\n",
      "0.0003129946126136929\n",
      "loss\n",
      "0.000476246903417632\n",
      "loss\n",
      "0.00044288364006206393\n",
      "loss\n",
      "0.0018682897789403796\n",
      "loss\n",
      "0.0003418338019400835\n",
      "loss\n",
      "0.0012497241841629148\n",
      "loss\n",
      "0.0015631611458957195\n",
      "loss\n",
      "0.0023768290411680937\n",
      "loss\n",
      "0.02574142999947071\n",
      "loss\n",
      "0.0017401328077539802\n",
      "loss\n",
      "0.004134322516620159\n",
      "loss\n",
      "0.0008989107445813715\n",
      "loss\n",
      "0.0005607224884442985\n",
      "loss\n",
      "0.001046705641783774\n",
      "loss\n",
      "0.0007827793597243726\n",
      "loss\n",
      "0.0006280356901697814\n",
      "loss\n",
      "0.02786920592188835\n",
      "loss\n",
      "0.0008538890979252756\n",
      "loss\n",
      "0.0010851691477000713\n",
      "loss\n",
      "0.00240013818256557\n",
      "loss\n",
      "0.0021174170542508364\n",
      "loss\n",
      "0.002991488901898265\n",
      "loss\n",
      "0.0014588914345949888\n",
      "loss\n",
      "0.004241281189024448\n",
      "loss\n",
      "0.003736658487468958\n",
      "loss\n",
      "0.0022800182923674583\n",
      "loss\n",
      "0.0007152383332140744\n",
      "loss\n",
      "0.000924993772059679\n",
      "loss\n",
      "0.003994819708168507\n",
      "loss\n",
      "0.000894146622158587\n",
      "loss\n",
      "0.0027646913658827543\n",
      "loss\n",
      "0.0007594323833473027\n",
      "loss\n",
      "0.0021627387031912804\n",
      "loss\n",
      "0.0012949660886079073\n",
      "loss\n",
      "0.002144657773897052\n",
      "loss\n",
      "0.0016444505890831351\n",
      "loss\n",
      "0.0005864569102413952\n",
      "loss\n",
      "0.00039188333903439343\n",
      "loss\n",
      "0.0029439465142786503\n",
      "loss\n",
      "0.0022638426162302494\n",
      "loss\n",
      "0.0009483369067311287\n",
      "loss\n",
      "0.0116086695343256\n",
      "loss\n",
      "0.0008537700050510466\n",
      "loss\n",
      "0.001016814960166812\n",
      "loss\n",
      "0.0008986725588329136\n",
      "loss\n",
      "0.000701658078469336\n",
      "loss\n",
      "0.0017089537577703595\n",
      "loss\n",
      "0.0011794761521741748\n",
      "loss\n",
      "0.0004247716860845685\n",
      "loss\n",
      "0.0005683475756086409\n",
      "loss\n",
      "0.0006723527330905199\n",
      "loss\n",
      "0.0010266992030665278\n",
      "loss\n",
      "0.0019143365789204836\n",
      "loss\n",
      "0.0005214046686887741\n",
      "loss\n",
      "0.001148994080722332\n",
      "loss\n",
      "0.004422408062964678\n",
      "loss\n",
      "0.0005122303264215589\n",
      "loss\n",
      "0.0011468507582321763\n",
      "loss\n",
      "0.00022957073815632612\n",
      "loss\n",
      "0.0027576773427426815\n",
      "loss\n",
      "0.000952267087996006\n",
      "loss\n",
      "0.016635630279779434\n",
      "loss\n",
      "0.0007250064518302679\n",
      "loss\n",
      "0.0006623458466492593\n",
      "loss\n",
      "0.0007408496458083391\n",
      "loss\n",
      "0.002483262214809656\n",
      "loss\n",
      "0.0021470370702445507\n",
      "loss\n",
      "0.0009137984015978873\n",
      "loss\n",
      "0.00023457636416424066\n",
      "loss\n",
      "0.0023837266489863396\n",
      "loss\n",
      "0.0011623300379142165\n",
      "loss\n",
      "0.0013184197014197707\n",
      "loss\n",
      "0.001476865611039102\n",
      "loss\n",
      "0.003979858942329884\n",
      "loss\n",
      "0.0005212855176068842\n",
      "loss\n",
      "0.0005154472892172635\n",
      "loss\n",
      "0.00042965717148035765\n",
      "loss\n",
      "0.000764792668633163\n",
      "loss\n",
      "0.0009112972766160965\n",
      "loss\n",
      "0.0008796160109341145\n",
      "loss\n",
      "0.0008474572678096592\n",
      "loss\n",
      "0.0005744237569160759\n",
      "loss\n",
      "0.0019026764202862978\n",
      "loss\n",
      "0.002003092784434557\n",
      "loss\n",
      "0.42928773164749146\n",
      "loss\n",
      "0.0008839037618599832\n",
      "loss\n",
      "0.001987983239814639\n",
      "loss\n",
      "0.014038268476724625\n",
      "loss\n",
      "0.0011518517276272178\n",
      "loss\n",
      "0.00032658010604791343\n",
      "loss\n",
      "0.0008891443139873445\n",
      "loss\n",
      "0.000750736624468118\n",
      "loss\n",
      "0.013025554828345776\n",
      "loss\n",
      "0.004036019556224346\n",
      "loss\n",
      "0.005081120412796736\n",
      "loss\n",
      "0.0002636561985127628\n",
      "loss\n",
      "0.003246871754527092\n",
      "loss\n",
      "0.00031418632715940475\n",
      "loss\n",
      "0.005515950731933117\n",
      "loss\n",
      "0.0016540905926376581\n",
      "loss\n",
      "0.00022194306075107306\n",
      "loss\n",
      "0.0026214304380118847\n",
      "loss\n",
      "0.016800694167613983\n",
      "loss\n",
      "0.004386090207844973\n",
      "loss\n",
      "0.0003225283289793879\n",
      "loss\n",
      "0.000758955895435065\n",
      "loss\n",
      "0.00035613393993116915\n",
      "loss\n",
      "0.00037448544753715396\n",
      "loss\n",
      "0.0017446548445150256\n",
      "loss\n",
      "0.00015090756642166525\n",
      "loss\n",
      "0.0002574589161667973\n",
      "loss\n",
      "0.0007256020326167345\n",
      "loss\n",
      "0.001990600721910596\n",
      "loss\n",
      "0.00012468514614738524\n",
      "loss\n",
      "0.0026844439562410116\n",
      "loss\n",
      "0.0004638549580704421\n",
      "loss\n",
      "0.000788258679676801\n",
      "loss\n",
      "0.00028796817059628665\n",
      "loss\n",
      "0.0013338964199647307\n",
      "loss\n",
      "0.00020621081057470292\n",
      "loss\n",
      "0.05171101912856102\n",
      "loss\n",
      "0.0030126445926725864\n",
      "loss\n",
      "0.005566809326410294\n",
      "loss\n",
      "0.00016234986833296716\n",
      "loss\n",
      "0.00017736769223120064\n",
      "loss\n",
      "0.0002821285743266344\n",
      "loss\n",
      "0.0006914132391102612\n",
      "loss\n",
      "0.0004648081958293915\n",
      "loss\n",
      "0.0008348317351192236\n",
      "loss\n",
      "0.0019875073339790106\n",
      "loss\n",
      "0.0002169373765354976\n",
      "loss\n",
      "0.0012260308722034097\n",
      "loss\n",
      "0.00018523407925385982\n",
      "loss\n",
      "0.00041571559268049896\n",
      "loss\n",
      "0.0006481691962108016\n",
      "loss\n",
      "0.0010397987207397819\n",
      "loss\n",
      "0.0006696127820760012\n",
      "loss\n",
      "0.011480340734124184\n",
      "loss\n",
      "0.0003594706067815423\n",
      "loss\n",
      "0.0024949158541858196\n",
      "loss\n",
      "0.011587812565267086\n",
      "loss\n",
      "0.0006804534932598472\n",
      "loss\n",
      "0.0003280101518612355\n",
      "loss\n",
      "0.0011869773734360933\n",
      "loss\n",
      "0.0009833505610004067\n",
      "loss\n",
      "0.00016330339713022113\n",
      "loss\n",
      "0.0008563903393223882\n",
      "loss\n",
      "0.0004122599493712187\n",
      "loss\n",
      "0.0005706112715415657\n",
      "loss\n",
      "0.003935806918889284\n",
      "loss\n",
      "0.00036638224264606833\n",
      "loss\n",
      "0.0002915434306487441\n",
      "loss\n",
      "0.00020215852418914437\n",
      "loss\n",
      "0.00126008247025311\n",
      "loss\n",
      "0.0009531007381156087\n",
      "loss\n",
      "0.00030632095877081156\n",
      "loss\n",
      "0.0005411829333752394\n"
     ]
    }
   ],
   "source": [
    "gating, gating_optimizer = init_gating()\n",
    "gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "for i in range(100):\n",
    "    for seqs, seqs_len in train_dls[2]:\n",
    "\n",
    "        gating.train()\n",
    "\n",
    "        gating_optimizer.zero_grad()\n",
    "\n",
    "        outputs = gating(seqs, seqs_len)\n",
    "\n",
    "        #print(\"seq\")\n",
    "        #print(seqs)\n",
    "        #print(\"outputs\")\n",
    "        #print(outputs)\n",
    "\n",
    "        if seqs[1] == 7:\n",
    "            trgts = torch.tensor([0])\n",
    "        else:\n",
    "            trgts = torch.tensor([1])\n",
    "\n",
    "        loss = compute_loss(outputs, trgts, gating_criterion,\n",
    "                            cutFirstInSequence=False)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        gating_optimizer.step()\n",
    "\n",
    "        print(\"loss\")\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ONMUcvfWeipD"
   },
   "outputs": [],
   "source": [
    "gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "expert_criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)\n",
    "expert_criterion_unreduced = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN,\n",
    "                                        reduction=\"none\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcqtDHZcJah8",
    "outputId": "a48e44cc-67ab-474d-8e5b-5267ec501dac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(30, 10, bidirectional=True)\n",
      "        (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "          (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(50, 10)\n",
      "        (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "expert, expert_optimizer = init_expert()\n",
    "gating, gating_optimizer = init_gating()\n",
    "model = DynaMoE(gating, gating_optimizer, [expert,], [expert_optimizer,])\n",
    "print(model.apply(init_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "UYduyRNYJolw"
   },
   "outputs": [],
   "source": [
    "model.add_expert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARkJQvgNgU27",
    "outputId": "0012ec60-7122-4876-8fb5-5a863be5f25f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - tensor([1, 5, 4, 7, 4, 6, 2])\n",
      "1 - tensor([1, 5, 3, 4, 7, 2])\n",
      "1 - tensor([1, 5, 4, 6, 7, 4, 6, 2])\n",
      "1 - tensor([1, 5, 4, 7, 2])\n",
      "1 - tensor([1, 5, 3, 4, 7, 4, 2])\n",
      "1 - tensor([1, 5, 3, 4, 6, 7, 4, 6, 2])\n",
      "1 - tensor([1, 5, 3, 4, 7, 4, 6, 2])\n",
      "1 - tensor([1, 5, 4, 6, 7, 2])\n"
     ]
    }
   ],
   "source": [
    "show_expert(model, train_dls[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kO7XbXIIgjWN",
    "outputId": "67eb26e2-95b1-4844-e801-752078589da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - tensor([1, 7, 4, 6, 5, 2])\n",
      "1 - tensor([1, 7, 3, 4, 5, 4, 2])\n",
      "1 - tensor([1, 7, 3, 4, 6, 5, 4, 6, 2])\n",
      "1 - tensor([1, 7, 3, 4, 5, 2])\n",
      "1 - tensor([1, 7, 3, 4, 5, 4, 6, 2])\n",
      "1 - tensor([1, 7, 4, 6, 5, 4, 6, 2])\n",
      "1 - tensor([1, 7, 4, 5, 4, 6, 2])\n",
      "1 - tensor([1, 7, 4, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "show_expert(model, train_dls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "XQStSnwP-bwf"
   },
   "outputs": [],
   "source": [
    "gating_optimizer = optim.Adam(model.gating.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xtz-JxQ--n-U",
    "outputId": "a4164e4b-10f8-4a69-acb9-d454881779b5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.3362e-14,\n",
      "          0.0000e+00,  0.0000e+00, -1.0627e-14,  1.2231e-14,  0.0000e+00],\n",
      "        [ 9.5526e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.5729e-09,\n",
      "         -8.0322e-09,  0.0000e+00, -1.3070e-08,  2.0716e-08,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -1.2082e-14, -4.8358e-12,  6.6748e-12, -6.2617e-12,\n",
      "         -5.3810e-12,  7.6332e-12, -2.3784e-12,  0.0000e+00, -7.3044e-12],\n",
      "        [-4.1353e-13, -4.9267e-13, -3.5623e-13,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  5.5728e-13,  0.0000e+00,  4.3358e-13, -5.3819e-13],\n",
      "        [ 0.0000e+00,  0.0000e+00, -8.9321e-11,  5.7395e-14,  0.0000e+00,\n",
      "         -1.0017e-10,  1.4144e-10,  0.0000e+00,  5.0259e-14, -6.3196e-14],\n",
      "        [ 0.0000e+00, -1.0079e-14,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -8.3384e-15,  1.0995e-14, -6.3370e-15,  0.0000e+00, -1.1091e-14]])\n",
      "rnn.weight_ih_l0\n",
      "tensor([[-1.2741e-11, -4.1343e-15, -1.2010e-12,  5.5348e-14, -6.1908e-12,\n",
      "         -2.3396e-11,  1.1962e-12,  1.5775e-12,  1.0668e-11, -6.6139e-14],\n",
      "        [-1.7986e-11, -6.1930e-15, -1.7422e-12,  8.5238e-14, -8.7462e-12,\n",
      "         -3.3065e-11,  1.7365e-12,  2.2192e-12,  1.5059e-11, -1.0169e-13],\n",
      "        [-1.7355e-11, -5.7870e-15, -1.6031e-12,  7.9681e-14, -8.4371e-12,\n",
      "         -3.1844e-11,  1.5982e-12,  2.1442e-12,  1.4531e-11, -9.5055e-14],\n",
      "        [-1.8238e-11, -6.5488e-15, -1.8412e-12,  8.8614e-14, -8.8711e-12,\n",
      "         -3.3589e-11,  1.8348e-12,  2.2480e-12,  1.5270e-11, -1.0583e-13],\n",
      "        [-9.1758e-12, -2.9753e-15, -9.0931e-13,  3.4315e-14, -4.4529e-12,\n",
      "         -1.6883e-11,  9.0369e-13,  1.1421e-12,  7.6826e-12, -4.1435e-14],\n",
      "        [-1.2727e-17, -2.0106e-19, -7.9940e-19,  1.0168e-20, -3.3603e-17,\n",
      "         -2.3203e-17,  1.0065e-18, -1.8089e-17,  2.2752e-17, -2.1822e-19],\n",
      "        [-2.1118e-11, -6.7207e-15, -1.9434e-12,  9.6933e-14, -1.0267e-11,\n",
      "         -3.8743e-11,  1.9376e-12,  2.6091e-12,  1.7681e-11, -1.1530e-13],\n",
      "        [-4.2235e-12, -1.5118e-15, -4.4800e-13,  1.6007e-14, -2.0497e-12,\n",
      "         -7.7942e-12,  4.4502e-13,  5.2544e-13,  3.5362e-12, -1.9453e-14],\n",
      "        [-1.2104e-11, -4.2045e-15, -1.2392e-12,  4.8817e-14, -5.8774e-12,\n",
      "         -2.2303e-11,  1.2321e-12,  1.5028e-12,  1.0134e-11, -5.8929e-14],\n",
      "        [-4.7408e-12, -2.2792e-15, -5.7470e-13,  2.1609e-14, -2.3041e-12,\n",
      "         -8.8055e-12,  5.7121e-13,  5.8587e-13,  3.9693e-12, -2.6509e-14],\n",
      "        [ 6.6286e-13,  7.3984e-16, -8.5020e-14, -1.5201e-15,  3.2065e-13,\n",
      "          1.0991e-12,  8.3078e-14, -8.3417e-14, -5.5514e-13,  2.3342e-15],\n",
      "        [ 8.7248e-13,  8.9356e-16, -1.1475e-13, -1.8182e-15,  4.2183e-13,\n",
      "          1.4442e-12,  1.1224e-13, -1.1005e-13, -7.3065e-13,  2.8197e-15],\n",
      "        [ 8.4736e-13,  8.5097e-16, -1.0343e-13, -1.5827e-15,  4.0949e-13,\n",
      "          1.4090e-12,  1.0117e-13, -1.0709e-13, -7.0961e-13,  2.5146e-15],\n",
      "        [ 1.0759e-12,  9.2496e-16, -1.3016e-13, -2.1993e-15,  5.2013e-13,\n",
      "          1.7902e-12,  1.2728e-13, -1.3580e-13, -9.0093e-13,  3.2712e-15],\n",
      "        [ 5.6700e-13,  8.6599e-16, -8.4782e-14, -2.0148e-15,  2.7489e-13,\n",
      "          9.3052e-13,  8.2685e-14, -7.0587e-14, -4.7490e-13,  2.9716e-15],\n",
      "        [ 3.2364e-17,  1.4174e-16,  3.2161e-17, -3.8775e-17,  2.1664e-16,\n",
      "          1.3156e-16, -1.3289e-16,  2.2662e-16, -1.2378e-16,  1.6023e-16],\n",
      "        [ 1.1006e-12,  8.1159e-16, -1.1776e-13, -1.7806e-15,  5.3170e-13,\n",
      "          1.8437e-12,  1.1522e-13, -1.3944e-13, -9.2167e-13,  2.7080e-15],\n",
      "        [ 3.1208e-13,  9.5127e-16, -5.4686e-14, -1.5326e-15,  1.5156e-13,\n",
      "          5.0549e-13,  5.3179e-14, -3.8384e-14, -2.6151e-13,  2.4646e-15],\n",
      "        [ 6.8898e-13,  1.1269e-15, -1.1249e-13, -2.6129e-15,  3.3411e-13,\n",
      "          1.1229e-12,  1.0977e-13, -8.5628e-14, -5.7703e-13,  3.8803e-15],\n",
      "        [ 4.0820e-13,  1.7776e-15, -8.1602e-14, -2.9815e-15,  1.9877e-13,\n",
      "          6.5251e-13,  7.9180e-14, -4.9381e-14, -3.4215e-13,  4.7378e-15],\n",
      "        [-1.1690e-10, -4.0569e-14, -9.9090e-12,  5.7459e-13, -5.6869e-11,\n",
      "         -2.1379e-10,  9.9007e-12,  1.4401e-11,  9.7877e-11, -6.8428e-13],\n",
      "        [ 1.5179e-10,  5.5814e-14,  1.3358e-11, -8.0338e-13,  7.3899e-11,\n",
      "          2.7800e-10, -1.3354e-11, -1.8637e-11, -1.2709e-10,  9.5584e-13],\n",
      "        [ 1.4691e-10,  5.1973e-14,  1.2321e-11, -7.5753e-13,  7.1503e-11,\n",
      "          2.6858e-10, -1.2322e-11, -1.8059e-11, -1.2300e-10,  9.0062e-13],\n",
      "        [ 1.5032e-10,  5.7241e-14,  1.3668e-11, -8.0311e-13,  7.3187e-11,\n",
      "          2.7565e-10, -1.3659e-11, -1.8447e-11, -1.2585e-10,  9.5701e-13],\n",
      "        [ 7.5015e-11,  2.6564e-14,  6.6023e-12, -3.2660e-13,  3.6450e-11,\n",
      "          1.3737e-10, -6.5820e-12, -9.2864e-12, -6.2807e-11,  3.9260e-13],\n",
      "        [-4.9563e-15, -4.4676e-18, -2.3274e-16,  6.1259e-18, -2.7919e-15,\n",
      "         -8.9162e-15,  2.3483e-16,  3.4348e-16,  4.3254e-15, -1.1415e-17],\n",
      "        [-1.7836e-10, -5.9934e-14, -1.4637e-11,  8.8339e-13, -8.6773e-11,\n",
      "         -3.2581e-10,  1.4634e-11,  2.1964e-11,  1.4933e-10, -1.0495e-12],\n",
      "        [-3.8141e-11, -1.4796e-14, -3.5249e-12,  1.7314e-13, -1.8539e-11,\n",
      "         -6.9977e-11,  3.5140e-12,  4.7135e-12,  3.1934e-11, -2.0884e-13],\n",
      "        [ 9.0623e-11,  3.4656e-14,  8.3105e-12, -4.2677e-13,  4.4064e-11,\n",
      "          1.6622e-10, -8.2893e-12, -1.1184e-11, -7.5875e-11,  5.1302e-13],\n",
      "        [-3.8155e-11, -2.0677e-14, -4.1315e-12,  2.1317e-13, -1.8584e-11,\n",
      "         -7.0482e-11,  4.1218e-12,  4.6718e-12,  3.1946e-11, -2.5969e-13]])\n",
      "rnn.weight_hh_l0\n",
      "tensor([[ 3.2406e-11, -3.2207e-11, -3.2240e-11, -3.2168e-11, -3.2604e-11,\n",
      "          3.2936e-11,  3.2080e-11,  3.2774e-11, -3.2534e-11,  3.2767e-11],\n",
      "        [ 4.5892e-11, -4.5609e-11, -4.5657e-11, -4.5555e-11, -4.6172e-11,\n",
      "          4.6641e-11,  4.5431e-11,  4.6412e-11, -4.6072e-11,  4.6402e-11],\n",
      "        [ 4.4040e-11, -4.3768e-11, -4.3814e-11, -4.3715e-11, -4.4309e-11,\n",
      "          4.4760e-11,  4.3596e-11,  4.4539e-11, -4.4213e-11,  4.4530e-11],\n",
      "        [ 4.6769e-11, -4.6481e-11, -4.6530e-11, -4.6426e-11, -4.7053e-11,\n",
      "          4.7530e-11,  4.6300e-11,  4.7297e-11, -4.6952e-11,  4.7287e-11],\n",
      "        [ 2.3477e-11, -2.3332e-11, -2.3357e-11, -2.3304e-11, -2.3620e-11,\n",
      "          2.3859e-11,  2.3241e-11,  2.3742e-11, -2.3569e-11,  2.3737e-11],\n",
      "        [ 3.1307e-17, -3.1110e-17, -3.1140e-17, -3.1068e-17, -3.1490e-17,\n",
      "          3.1876e-17,  3.0985e-17,  3.1657e-17, -3.1424e-17,  3.1647e-17],\n",
      "        [ 5.3566e-11, -5.3236e-11, -5.3291e-11, -5.3171e-11, -5.3893e-11,\n",
      "          5.4443e-11,  5.3027e-11,  5.4174e-11, -5.3777e-11,  5.4162e-11],\n",
      "        [ 1.0898e-11, -1.0831e-11, -1.0842e-11, -1.0818e-11, -1.0964e-11,\n",
      "          1.1075e-11,  1.0789e-11,  1.1021e-11, -1.0940e-11,  1.1018e-11],\n",
      "        [ 3.1093e-11, -3.0902e-11, -3.0934e-11, -3.0865e-11, -3.1282e-11,\n",
      "          3.1599e-11,  3.0781e-11,  3.1444e-11, -3.1215e-11,  3.1437e-11],\n",
      "        [ 1.2456e-11, -1.2380e-11, -1.2393e-11, -1.2366e-11, -1.2531e-11,\n",
      "          1.2657e-11,  1.2332e-11,  1.2595e-11, -1.2505e-11,  1.2593e-11],\n",
      "        [-1.2247e-12,  1.2160e-12,  1.2177e-12,  1.2142e-12,  1.2331e-12,\n",
      "         -1.2481e-12, -1.2107e-12, -1.2407e-12,  1.2299e-12, -1.2403e-12],\n",
      "        [-1.6030e-12,  1.5915e-12,  1.5938e-12,  1.5892e-12,  1.6139e-12,\n",
      "         -1.6337e-12, -1.5846e-12, -1.6239e-12,  1.6098e-12, -1.6235e-12],\n",
      "        [-1.5819e-12,  1.5707e-12,  1.5729e-12,  1.5684e-12,  1.5927e-12,\n",
      "         -1.6119e-12, -1.5639e-12, -1.6024e-12,  1.5886e-12, -1.6020e-12],\n",
      "        [-2.0123e-12,  1.9981e-12,  2.0009e-12,  1.9952e-12,  2.0260e-12,\n",
      "         -2.0505e-12, -1.9894e-12, -2.0384e-12,  2.0208e-12, -2.0378e-12],\n",
      "        [-1.0099e-12,  1.0026e-12,  1.0041e-12,  1.0011e-12,  1.0169e-12,\n",
      "         -1.0296e-12, -9.9817e-13, -1.0233e-12,  1.0142e-12, -1.0230e-12],\n",
      "        [-2.7163e-16,  2.6840e-16,  2.6669e-16,  2.6575e-16,  2.6862e-16,\n",
      "         -3.1099e-16, -2.6572e-16, -2.7043e-16,  2.6912e-16, -2.6736e-16],\n",
      "        [-2.1069e-12,  2.0921e-12,  2.0950e-12,  2.0891e-12,  2.1210e-12,\n",
      "         -2.1463e-12, -2.0831e-12, -2.1339e-12,  2.1157e-12, -2.1333e-12],\n",
      "        [-5.3080e-13,  5.2684e-13,  5.2764e-13,  5.2603e-13,  5.3453e-13,\n",
      "         -5.4151e-13, -5.2447e-13, -5.3800e-13,  5.3309e-13, -5.3782e-13],\n",
      "        [-1.1973e-12,  1.1885e-12,  1.1903e-12,  1.1867e-12,  1.2056e-12,\n",
      "         -1.2209e-12, -1.1832e-12, -1.2134e-12,  1.2024e-12, -1.2130e-12],\n",
      "        [-6.6248e-13,  6.5742e-13,  6.5846e-13,  6.5637e-13,  6.6724e-13,\n",
      "         -6.7629e-13, -6.5440e-13, -6.7170e-13,  6.6538e-13, -6.7146e-13],\n",
      "        [ 2.7702e-10, -2.7530e-10, -2.7560e-10, -2.7497e-10, -2.7872e-10,\n",
      "          2.8157e-10,  2.7422e-10,  2.8018e-10, -2.7811e-10,  2.8012e-10],\n",
      "        [-3.5715e-10,  3.5494e-10,  3.5531e-10,  3.5451e-10,  3.5933e-10,\n",
      "         -3.6301e-10, -3.5355e-10, -3.6121e-10,  3.5856e-10, -3.6113e-10],\n",
      "        [-3.4445e-10,  3.4232e-10,  3.4268e-10,  3.4190e-10,  3.4656e-10,\n",
      "         -3.5012e-10, -3.4097e-10, -3.4838e-10,  3.4581e-10, -3.4830e-10],\n",
      "        [-3.5395e-10,  3.5176e-10,  3.5213e-10,  3.5134e-10,  3.5611e-10,\n",
      "         -3.5975e-10, -3.5038e-10, -3.5797e-10,  3.5534e-10, -3.5789e-10],\n",
      "        [-1.7851e-10,  1.7741e-10,  1.7759e-10,  1.7719e-10,  1.7960e-10,\n",
      "         -1.8144e-10, -1.7671e-10, -1.8054e-10,  1.7921e-10, -1.8050e-10],\n",
      "        [ 1.1874e-14, -1.1799e-14, -1.1812e-14, -1.1784e-14, -1.1948e-14,\n",
      "          1.2074e-14,  1.1752e-14,  1.2012e-14, -1.1921e-14,  1.2009e-14],\n",
      "        [ 4.1527e-10, -4.1270e-10, -4.1313e-10, -4.1219e-10, -4.1782e-10,\n",
      "          4.2211e-10,  4.1107e-10,  4.2001e-10, -4.1691e-10,  4.1992e-10],\n",
      "        [ 9.2472e-11, -9.1901e-11, -9.1998e-11, -9.1790e-11, -9.3037e-11,\n",
      "          9.3986e-11,  9.1541e-11,  9.3522e-11, -9.2836e-11,  9.3501e-11],\n",
      "        [-2.1470e-10,  2.1337e-10,  2.1360e-10,  2.1312e-10,  2.1601e-10,\n",
      "         -2.1821e-10, -2.1254e-10, -2.1714e-10,  2.1554e-10, -2.1709e-10],\n",
      "        [ 9.3705e-11, -9.3130e-11, -9.3226e-11, -9.3019e-11, -9.4273e-11,\n",
      "          9.5226e-11,  9.2767e-11,  9.4760e-11, -9.4072e-11,  9.4740e-11]])\n",
      "rnn.bias_ih_l0\n",
      "tensor([ 3.2936e-11,  4.6642e-11,  4.4761e-11,  4.7531e-11,  2.3860e-11,\n",
      "         1.0899e-16,  5.4443e-11,  1.1075e-11,  3.1599e-11,  1.2657e-11,\n",
      "        -1.2491e-12, -1.6347e-12, -1.6129e-12, -2.0514e-12, -1.0307e-12,\n",
      "        -1.0369e-15, -2.1472e-12, -5.4274e-13, -1.2221e-12, -6.7793e-13,\n",
      "         2.9870e-10, -3.8939e-10, -3.7496e-10, -3.8699e-10, -1.9244e-10,\n",
      "         1.3200e-14,  4.5421e-10,  9.8371e-11, -2.3352e-10,  1.0031e-10])\n",
      "rnn.bias_hh_l0\n",
      "tensor([ 3.2936e-11,  4.6642e-11,  4.4761e-11,  4.7531e-11,  2.3860e-11,\n",
      "         1.0899e-16,  5.4443e-11,  1.1075e-11,  3.1599e-11,  1.2657e-11,\n",
      "        -1.2491e-12, -1.6347e-12, -1.6129e-12, -2.0514e-12, -1.0307e-12,\n",
      "        -1.0369e-15, -2.1472e-12, -5.4274e-13, -1.2221e-12, -6.7793e-13,\n",
      "         2.8158e-10, -3.6301e-10, -3.5012e-10, -3.5975e-10, -1.8144e-10,\n",
      "         1.2938e-14,  4.2211e-10,  9.3988e-11, -2.1822e-10,  9.5228e-11])\n",
      "rnn.weight_ih_l0_reverse\n",
      "tensor([[-8.4594e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.0747e-11,\n",
      "         -1.4896e-10,  0.0000e+00,  1.0874e-11,  7.0829e-11,  0.0000e+00],\n",
      "        [-2.9883e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4394e-11,\n",
      "         -5.2622e-11,  0.0000e+00,  3.8413e-12,  2.5020e-11,  0.0000e+00],\n",
      "        [-5.3683e-12,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.5858e-12,\n",
      "         -9.4532e-12,  0.0000e+00,  6.9007e-13,  4.4948e-12,  0.0000e+00],\n",
      "        [-1.1488e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.5334e-11,\n",
      "         -2.0229e-10,  0.0000e+00,  1.4767e-11,  9.6184e-11,  0.0000e+00],\n",
      "        [-1.3635e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.5675e-11,\n",
      "         -2.4010e-10,  0.0000e+00,  1.7527e-11,  1.1416e-10,  0.0000e+00],\n",
      "        [-1.1582e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.5790e-12,\n",
      "         -2.0396e-11,  0.0000e+00,  1.4889e-12,  9.6977e-12,  0.0000e+00],\n",
      "        [-5.7277e-12,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.7589e-12,\n",
      "         -1.0086e-11,  0.0000e+00,  7.3627e-13,  4.7957e-12,  0.0000e+00],\n",
      "        [ 1.6456e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.9265e-12,\n",
      "          2.8978e-11,  0.0000e+00, -2.1153e-12, -1.3778e-11,  0.0000e+00],\n",
      "        [-9.6630e-12,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.6545e-12,\n",
      "         -1.7016e-11,  0.0000e+00,  1.2421e-12,  8.0906e-12,  0.0000e+00],\n",
      "        [-4.0936e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.9718e-11,\n",
      "         -7.2086e-11,  0.0000e+00,  5.2621e-12,  3.4275e-11,  0.0000e+00],\n",
      "        [ 8.6252e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1546e-10,\n",
      "          1.5188e-09,  0.0000e+00, -1.1087e-10, -7.2217e-10,  0.0000e+00],\n",
      "        [ 6.7716e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.2618e-11,\n",
      "          1.1924e-10,  0.0000e+00, -8.7046e-12, -5.6697e-11,  0.0000e+00],\n",
      "        [ 3.7308e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7971e-10,\n",
      "          6.5697e-10,  0.0000e+00, -4.7958e-11, -3.1237e-10,  0.0000e+00],\n",
      "        [ 8.7162e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1984e-10,\n",
      "          1.5349e-09,  0.0000e+00, -1.1204e-10, -7.2979e-10,  0.0000e+00],\n",
      "        [ 8.1539e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.9276e-10,\n",
      "          1.4359e-09,  0.0000e+00, -1.0482e-10, -6.8271e-10,  0.0000e+00],\n",
      "        [ 5.1821e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.4961e-10,\n",
      "          9.1254e-10,  0.0000e+00, -6.6614e-11, -4.3389e-10,  0.0000e+00],\n",
      "        [-5.1632e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.4870e-11,\n",
      "         -9.0921e-11,  0.0000e+00,  6.6371e-12,  4.3231e-11,  0.0000e+00],\n",
      "        [-1.8575e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.9473e-11,\n",
      "         -3.2710e-10,  0.0000e+00,  2.3878e-11,  1.5553e-10,  0.0000e+00],\n",
      "        [-2.9914e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4409e-11,\n",
      "         -5.2677e-11,  0.0000e+00,  3.8454e-12,  2.5047e-11,  0.0000e+00],\n",
      "        [ 1.2659e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.0975e-11,\n",
      "          2.2291e-10,  0.0000e+00, -1.6272e-11, -1.0599e-10,  0.0000e+00],\n",
      "        [ 8.4579e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.0740e-10,\n",
      "          1.4894e-09,  0.0000e+00, -1.0872e-10, -7.0816e-10,  0.0000e+00],\n",
      "        [-1.3305e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.4088e-10,\n",
      "         -2.3429e-09,  0.0000e+00,  1.7103e-10,  1.1140e-09,  0.0000e+00],\n",
      "        [-1.0231e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.9283e-11,\n",
      "         -1.8017e-10,  0.0000e+00,  1.3152e-11,  8.5665e-11,  0.0000e+00],\n",
      "        [ 1.1314e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.4499e-10,\n",
      "          1.9924e-09,  0.0000e+00, -1.4544e-10, -9.4733e-10,  0.0000e+00],\n",
      "        [-1.4078e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.7809e-10,\n",
      "         -2.4790e-09,  0.0000e+00,  1.8096e-10,  1.1787e-09,  0.0000e+00],\n",
      "        [-1.6939e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.1590e-11,\n",
      "         -2.9828e-10,  0.0000e+00,  2.1774e-11,  1.4182e-10,  0.0000e+00],\n",
      "        [ 1.1274e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.4306e-10,\n",
      "          1.9853e-09,  0.0000e+00, -1.4493e-10, -9.4398e-10,  0.0000e+00],\n",
      "        [-7.6532e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.6864e-10,\n",
      "         -1.3477e-09,  0.0000e+00,  9.8378e-11,  6.4078e-10,  0.0000e+00],\n",
      "        [ 1.0906e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.2532e-10,\n",
      "          1.9205e-09,  0.0000e+00, -1.4019e-10, -9.1313e-10,  0.0000e+00],\n",
      "        [-1.3831e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.6623e-10,\n",
      "         -2.4356e-09,  0.0000e+00,  1.7780e-10,  1.1581e-09,  0.0000e+00]])\n",
      "rnn.weight_hh_l0_reverse\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "rnn.bias_ih_l0_reverse\n",
      "tensor([ 1.9356e-10,  6.8377e-11,  1.2284e-11,  2.6286e-10,  3.1198e-10,\n",
      "         2.6502e-11,  1.3106e-11, -3.7654e-11,  2.2110e-11,  9.3668e-11,\n",
      "        -1.9736e-09, -1.5495e-10, -8.5366e-10, -1.9944e-09, -1.8657e-09,\n",
      "        -1.1858e-09,  1.1814e-10,  4.2503e-10,  6.8449e-11, -2.8965e-10,\n",
      "        -1.9353e-09,  3.0444e-09,  2.3411e-10, -2.5889e-09,  3.2212e-09,\n",
      "         3.8758e-10, -2.5797e-09,  1.7512e-09, -2.4954e-09,  3.1648e-09])\n",
      "rnn.bias_hh_l0_reverse\n",
      "tensor([ 1.9356e-10,  6.8377e-11,  1.2284e-11,  2.6286e-10,  3.1198e-10,\n",
      "         2.6502e-11,  1.3106e-11, -3.7654e-11,  2.2110e-11,  9.3668e-11,\n",
      "        -1.9736e-09, -1.5495e-10, -8.5366e-10, -1.9944e-09, -1.8657e-09,\n",
      "        -1.1858e-09,  1.1814e-10,  4.2503e-10,  6.8449e-11, -2.8965e-10,\n",
      "        -1.4298e-09,  1.0445e-09,  2.2012e-10, -1.7819e-09,  2.0395e-09,\n",
      "         3.5252e-10, -7.8725e-10,  4.3729e-10, -7.7968e-10,  1.1508e-09])\n",
      "fc_out.weight\n",
      "tensor([[-9.4410e-09,  9.3946e-09,  9.4020e-09,  9.3900e-09,  9.4874e-09,\n",
      "         -9.5659e-09, -9.3705e-09, -9.5267e-09,  9.4705e-09, -9.5252e-09,\n",
      "          5.9604e-09, -2.1056e-10, -8.9559e-09,  5.1718e-09, -4.0032e-09,\n",
      "         -8.5015e-09, -1.5177e-10,  5.1033e-10, -8.8546e-11, -4.0043e-10],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.2325e-08,  1.2264e-08,  1.2274e-08,  1.2258e-08,  1.2385e-08,\n",
      "         -1.2488e-08, -1.2233e-08, -1.2437e-08,  1.2363e-08, -1.2435e-08,\n",
      "          7.7809e-09, -2.7487e-10, -1.1691e-08,  6.7515e-09, -5.2260e-09,\n",
      "         -1.1098e-08, -1.9813e-10,  6.6621e-10, -1.1559e-10, -5.2274e-10]])\n",
      "fc_out.bias\n",
      "tensor([-9.5659e-09,  0.0000e+00, -1.2488e-08])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.gating.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "        print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgyhIfAtNrto"
   },
   "outputs": [],
   "source": [
    "gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKTw6jmppP8y",
    "outputId": "6161027d-7195-48ea-952c-005a6461f81b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 238,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_criterion(torch.tensor([[0, 1.0000e+10, 0],\n",
    "        [0, 1.0000e+10, 0]]),\n",
    "        torch.tensor([1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DbNMxqrZp_S-",
    "outputId": "7d2a76f9-b577-4a74-bb77-4af9d2f93c01",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([2])\n",
      ">> Gating Loss\n",
      "tensor(1.0986, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[-2.3803e-05]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0837, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0112]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0650, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0255]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0419, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0432]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0131, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0657]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.9796, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0922]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.9351, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.1283]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.8878, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.1678]], grad_fn=<SliceBackward>)\n",
      "0.82974424213171\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.8212, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2259]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.7432, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2976]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.6585, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3814]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.5479, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.5036]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.4752, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.5942]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.3468, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.7856]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.2496, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.9747]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.1614, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[1.2153]], grad_fn=<SliceBackward>)\n",
      "0.9061286374926567\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0966, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[1.4879]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0580, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[1.7517]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0428, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[1.9076]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.1951]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.4174]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.5413]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0075, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.7837]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0071, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.8154]], grad_fn=<SliceBackward>)\n",
      "0.9074996039271355\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0053, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.9616]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0032, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.2070]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0021, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.4383]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0030, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.2398]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0023, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.3716]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0025, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.3423]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0013, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.6711]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0013, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.6474]], grad_fn=<SliceBackward>)\n",
      "0.9339357018470764\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0014, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.6148]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.9905]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0013, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.6527]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2055]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1248]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.9430]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1311]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1320]], grad_fn=<SliceBackward>)\n",
      "0.8914671167731285\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.9443]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3064]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2918]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4889]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0006, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.0696]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3859]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6106]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3627]], grad_fn=<SliceBackward>)\n",
      "0.9092058017849922\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3146]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2314]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4475]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4427]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4040]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.8662]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2487]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4927]], grad_fn=<SliceBackward>)\n",
      "0.9086853638291359\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7027]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5852]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6655]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4688]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5112]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4657]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5495]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8051]], grad_fn=<SliceBackward>)\n",
      "0.9540341049432755\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5898]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5801]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4838]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5563]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3491]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7965]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7696]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5286]], grad_fn=<SliceBackward>)\n",
      "0.9324938133358955\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3866]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6102]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0006, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.0642]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9495]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5487]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4700]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6850]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7848]], grad_fn=<SliceBackward>)\n",
      "0.9254426285624504\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7214]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7740]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.8104e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9637]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2422]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2787]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5567]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2047]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3928]], grad_fn=<SliceBackward>)\n",
      "0.8661048337817192\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6382]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.7866e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9646]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4085]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4792]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3596]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7272]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5722]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6154]], grad_fn=<SliceBackward>)\n",
      "0.8831527382135391\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.2502e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9931]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7738]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4963]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8352]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4841]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6868]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5468]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4436]], grad_fn=<SliceBackward>)\n",
      "0.880535438656807\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4818]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8102]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0006, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.0234]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7940]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4468]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8329]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.4648e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9808]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5697]], grad_fn=<SliceBackward>)\n",
      "0.8675431832671165\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6845]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6166]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8095]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4355]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8134]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6103]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9230]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6864]], grad_fn=<SliceBackward>)\n",
      "0.9496602267026901\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6443]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4083]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7828]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5357]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7782]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6642]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4203]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3674]], grad_fn=<SliceBackward>)\n",
      "0.9153327122330666\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4941]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9431]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8273]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8151]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.0953e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0011]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9210]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7318]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6719]], grad_fn=<SliceBackward>)\n",
      "0.9284641966223717\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7545]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6887]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2209]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7534]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6434]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5050]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5450]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4922]], grad_fn=<SliceBackward>)\n",
      "0.9609360843896866\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6869]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5671]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.6555e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9714]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.4290e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9830]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7235]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4139]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1793]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7433]], grad_fn=<SliceBackward>)\n",
      "0.9041190296411514\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3575e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9867]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5629]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.6078e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9731]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6897]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9092]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6544]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7483]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8843]], grad_fn=<SliceBackward>)\n",
      "0.903328500688076\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8428]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7444]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.2502e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9924]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4540]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2109]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7608]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7668]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3372]], grad_fn=<SliceBackward>)\n",
      "0.9555044546723366\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8617]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5408]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8953]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8898]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7394]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8731]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7303]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2313]], grad_fn=<SliceBackward>)\n",
      "0.9302210956811905\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2986]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3337e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9876]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6920]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8961]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6884]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5304]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7666]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4039e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0407]], grad_fn=<SliceBackward>)\n",
      "0.8923574611544609\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5188]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7251]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.1046e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1254]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8343]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7112]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7108]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6158]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.5218e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0967]], grad_fn=<SliceBackward>)\n",
      "0.9684783220291138\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.8901e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1406]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6835]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.4026e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1039]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8303]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9114]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8457]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1415]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2771]], grad_fn=<SliceBackward>)\n",
      "0.9525922313332558\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7273]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5614]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.6529e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0881]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.9403e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0092]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9356]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.4741e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0997]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8350]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3187]], grad_fn=<SliceBackward>)\n",
      "0.8704231753945351\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7197]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5723]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.0595e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0026]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6777]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.9654e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9546]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5881]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7264]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5248]], grad_fn=<SliceBackward>)\n",
      "0.8824385702610016\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.5601e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9758]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4923]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.6065e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0286]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7087]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1972]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.8807e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0128]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.9761e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0074]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0543e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2956]], grad_fn=<SliceBackward>)\n",
      "0.9546280577778816\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3962]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3098e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9884]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7303]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0331e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1300]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9107]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7753]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6634]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9485]], grad_fn=<SliceBackward>)\n",
      "0.9295287430286407\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.1761e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1197]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3370]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9178]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1839]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.9654e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9543]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3047e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2717]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9308]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.2145e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9944]], grad_fn=<SliceBackward>)\n",
      "0.8718064799904823\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6463]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7837]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8458]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1749e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1950]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.5933e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0913]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1272e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1991]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1590e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5339]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8529]], grad_fn=<SliceBackward>)\n",
      "0.8350213691592216\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9841e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2115]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9113e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3111]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.8939e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9581]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.0357e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0039]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1571]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9377e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1373]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0569e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1284]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6946]], grad_fn=<SliceBackward>)\n",
      "0.941309466958046\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9735e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1341]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6921]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8385e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4358]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7919]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4941e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3554]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1072e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9998]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8343]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8788]], grad_fn=<SliceBackward>)\n",
      "0.9300222471356392\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.6980e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2356]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4106e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3648]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2035]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7696e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2303]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7536]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.2370e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0498]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1484e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3950]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.3549e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1079]], grad_fn=<SliceBackward>)\n",
      "0.9206623733043671\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2090]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0424e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2973]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9398]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8575]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1965]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8679]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6243]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7694]], grad_fn=<SliceBackward>)\n",
      "0.9505183324217796\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2036]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7998]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7457e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2310]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8407]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3803]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1271]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6526]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8411e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2224]], grad_fn=<SliceBackward>)\n",
      "0.8610163629055023\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8198e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0756]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.5444e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1655]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.0953e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0000]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8999]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.8926e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0120]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.7721e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0792]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8875e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3135]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0279e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5534]], grad_fn=<SliceBackward>)\n",
      "0.9559815302491188\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6014e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3432]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9841e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2115]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3666]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.5469e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0311]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9510]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8998]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7809]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.7747e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9638]], grad_fn=<SliceBackward>)\n",
      "0.982656717300415\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3537e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1805]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.7947e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1464]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.7828e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1478]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8675e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0726]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6755e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1562]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3537e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1804]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0093e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1315]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.4848e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1703]], grad_fn=<SliceBackward>)\n",
      "0.9076273813843727\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7778]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7198]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1668e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9961]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7073e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4522]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0199e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2078]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.7138e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0213]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9854e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1328]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.6053e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0903]], grad_fn=<SliceBackward>)\n",
      "0.9130949825048447\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2036]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6566]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7200]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9442]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6954e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4543]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.1178e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0568]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6848e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3338]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.2357e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1155]], grad_fn=<SliceBackward>)\n",
      "0.9232373982667923\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0663e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2943]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5479]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0517e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5495]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3715]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6675]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9175]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.4886e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9791]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9351e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3081]], grad_fn=<SliceBackward>)\n",
      "0.9844113141298294\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4941e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3553]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9841e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2106]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.7245e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0817]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7457e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2312]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8675e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0724]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.2013e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0521]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.8662e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1420]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.7960e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0777]], grad_fn=<SliceBackward>)\n",
      "0.906353659927845\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3285e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2683]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4951]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9352]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.4980e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0970]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9287]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.5921e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1621]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5701]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7325e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3286]], grad_fn=<SliceBackward>)\n",
      "0.9535688981413841\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1269]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4226e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3641]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.7138e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0211]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7402]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8875e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3129]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8623e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4322]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4635e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0359]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.2119e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1173]], grad_fn=<SliceBackward>)\n",
      "0.97125643491745\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7514]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.9165e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0093]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5485]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6330]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.4252e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1748]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.5840e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9730]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6014e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3423]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3298e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1825]], grad_fn=<SliceBackward>)\n",
      "1.0099817141890526\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8133e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5900]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3060e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1847]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1749e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1950]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0543e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2959]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7908e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4402]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3417e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1812]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7100e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2336]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.2715e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1123]], grad_fn=<SliceBackward>)\n",
      "0.9761429280042648\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0663e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2945]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1815e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7194]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6716e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4567]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1948e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5258]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8944]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.9152e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0694]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3700]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6055]], grad_fn=<SliceBackward>)\n",
      "0.9192996472120285\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1429e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9978]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.6065e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0276]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8623e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4312]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0517e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5497]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5775e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3450]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3697]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1139e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2895]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2093e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2797]], grad_fn=<SliceBackward>)\n",
      "0.9878424927592278\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0530e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4064]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8269]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.6423e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0256]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7722]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3060e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1844]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8407]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4887]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.2106e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1912]], grad_fn=<SliceBackward>)\n",
      "0.9360367059707642\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8172e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2252]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3832]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3749e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3678]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3298e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1815]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4332e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4901]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9126e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2162]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8398e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3179]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1429e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9967]], grad_fn=<SliceBackward>)\n",
      "0.9009067863225937\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9006]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.0940e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0581]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4516e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0364]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1126e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3992]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.2383e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9912]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8675e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0727]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3520]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5749e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6362]], grad_fn=<SliceBackward>)\n",
      "0.9399682730436325\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8371e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5874]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6546]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3456e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9861]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8623e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4309]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.9338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4212]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1365e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3954]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9444e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5671]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8437e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0737]], grad_fn=<SliceBackward>)\n",
      "0.9046507701277733\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1352e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5363]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5669e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2467]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8861e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4278]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3298e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1814]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9299]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9921e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5588]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0054e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4131]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4277e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0379]], grad_fn=<SliceBackward>)\n",
      "0.8922909200191498\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3813e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9843]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8172e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2239]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4592]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9311]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9364e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2146]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8861e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4288]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8172e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2247]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9376]], grad_fn=<SliceBackward>)\n",
      "0.9415184482932091\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3762e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2638]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8385e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4342]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6306]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.6291e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0882]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6040e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1603]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.2715e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1120]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8385e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4346]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2186e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5226]], grad_fn=<SliceBackward>)\n",
      "0.9168690741062164\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5312e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2502]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1815e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7180]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4277e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0374]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9205]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6464e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6203]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2857]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4941e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3545]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8505]], grad_fn=<SliceBackward>)\n",
      "0.9186825603246689\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8924]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8905]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1855e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2817]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6391]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1603e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3925]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7895e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5944]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6252e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3396]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.8424e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1430]], grad_fn=<SliceBackward>)\n",
      "1.0298770293593407\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3640]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8706]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6598]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8146e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4381]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.4477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2565]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.5695e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0920]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8133e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5918]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3518]], grad_fn=<SliceBackward>)\n",
      "0.9632200226187706\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3154]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2023]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5643e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4719]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7444e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3268]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3060e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1830]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1365e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3953]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7895e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5928]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8894]], grad_fn=<SliceBackward>)\n",
      "0.8725502789020538\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9735e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1332]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4833]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4914e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6502]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9735e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1328]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6226e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6266]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4332e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4901]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5034e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6489]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8371e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5859]], grad_fn=<SliceBackward>)\n",
      "0.9114488065242767\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9960e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2089]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5629]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3777]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5034]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2292e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7095]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.4954e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2518]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1842e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3901]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3826]], grad_fn=<SliceBackward>)\n",
      "0.997397854924202\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8003]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9113e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3097]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3484e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6815]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8146e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4366]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8341]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.9577e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4170]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8716e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7963]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1381]], grad_fn=<SliceBackward>)\n",
      "0.8821030855178833\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.3788e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1053]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2053e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7115]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.9641e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0062]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2053e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7124]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1152e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1991]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.5363e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9755]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8124]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8623e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4294]], grad_fn=<SliceBackward>)\n",
      "0.9253259226679802\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9364e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2139]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2018]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2544e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5151]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3007e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6894]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8625]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1590e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5305]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2318e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3835]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2530e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6994]], grad_fn=<SliceBackward>)\n",
      "0.826819896697998\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1259e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2869]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3514]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2914e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3774]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7934e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2261]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4464e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3588]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3484e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6805]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3285e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2675]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3351e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9637]], grad_fn=<SliceBackward>)\n",
      "0.9356672465801239\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6941e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6112]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.5946e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0275]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9377e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1360]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3590e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9557]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3524e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2666]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8146e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4350]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6464e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6202]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2928e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2711]], grad_fn=<SliceBackward>)\n",
      "0.9173995330929756\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5826]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7444e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3270]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7982]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1100e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7323]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5736e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8838]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9802e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5618]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.7734e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0173]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.9577e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4181]], grad_fn=<SliceBackward>)\n",
      "0.9404812529683113\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1484e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3945]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4937]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8411e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2214]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1815e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7173]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2763]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6613]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5537e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3469]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5192e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2514]], grad_fn=<SliceBackward>)\n",
      "0.908241368830204\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5550e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2469]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5391e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6407]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0411e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4078]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3775e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1784]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6141]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9007e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2168]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6144]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4947]], grad_fn=<SliceBackward>)\n",
      "0.960708424448967\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3524e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2662]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4928e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4815]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9051]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9057]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.9271e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0684]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7193e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4498]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3351]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6400]], grad_fn=<SliceBackward>)\n",
      "0.9716712683439255\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4067e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9403]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4570e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4854]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.4741e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0977]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7643e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8236]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9031]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5405e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4747]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.3085e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0452]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6464e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6196]], grad_fn=<SliceBackward>)\n",
      "0.9475336000323296\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1352e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5354]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6060]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6597]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9364e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2132]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9193e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7824]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4424e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9261]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9193e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7819]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6303]], grad_fn=<SliceBackward>)\n",
      "0.9496114104986191\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3762e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2637]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6014e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3421]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9377e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1358]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8172e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2233]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8546]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5047e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4791]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4318e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6619]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9064]], grad_fn=<SliceBackward>)\n",
      "0.9413476660847664\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6716e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4557]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2769e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6966]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4809e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4819]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3232e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9695]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8497]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7643e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8241]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8239e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8081]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8603]], grad_fn=<SliceBackward>)\n",
      "0.8706775084137917\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2570e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2751]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.0582e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0599]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6306]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2411e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7039]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2292e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7073]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9113e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3102]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5034e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6489]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3231]], grad_fn=<SliceBackward>)\n",
      "0.9316414818167686\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3961e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6714]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3150]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0994e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5410]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6669]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4570e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4859]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1352e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5345]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2053e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7125]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0067e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2988]], grad_fn=<SliceBackward>)\n",
      "0.9206378161907196\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7277]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3272e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3717]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2903]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1855e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2824]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.7470e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1489]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4822e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3551]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9563e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5657]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7206e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3291]], grad_fn=<SliceBackward>)\n",
      "0.8711266815662384\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.0027e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7609]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0398e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5503]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1842e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3892]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3649]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5139]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7206e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3286]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3722e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6760]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2318e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3847]], grad_fn=<SliceBackward>)\n",
      "0.9439148530364037\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9087e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5720]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1352e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5354]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2544e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5161]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0398e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5509]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8880]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3775e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1769]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5106]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8602]], grad_fn=<SliceBackward>)\n",
      "0.9201693758368492\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1868e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1925]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3007e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6908]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5094]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8918]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.7721e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0773]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8538]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2318e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3829]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7656e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5983]], grad_fn=<SliceBackward>)\n",
      "0.9166541695594788\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1577e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7206]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7563e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3248]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.8662e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1399]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.5801e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1612]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2305e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5201]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9325e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5670]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3129]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6542]], grad_fn=<SliceBackward>)\n",
      "0.9151469171047211\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0888e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4002]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5656e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3452]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9099]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5418e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3490]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9325e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5680]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.0861e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7409]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0769e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4017]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1219e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7309]], grad_fn=<SliceBackward>)\n",
      "0.8847057297825813\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3875]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6755e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1543]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6040e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1602]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9150]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9603e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2116]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5619]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5259e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8996]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7073e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4501]], grad_fn=<SliceBackward>)\n",
      "0.9680259078741074\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9802e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5604]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4186e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9340]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7770]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6059]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6166]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0650e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4043]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8954e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7885]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8000e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8142]], grad_fn=<SliceBackward>)\n",
      "0.9062342792749405\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5669e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2459]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9841e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2098]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8358e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8056]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9116]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5405e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4743]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7908e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4388]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2013]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8358e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8037]], grad_fn=<SliceBackward>)\n",
      "0.8556824177503586\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3272e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3724]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6071]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7881e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8161]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7773]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7696e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2281]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8358e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8051]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4212e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4915]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6226e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6260]], grad_fn=<SliceBackward>)\n",
      "0.8968541398644447\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6239e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4615]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6059]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2875e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9842]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3736e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4981]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7166e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8386]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6171]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3722e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6759]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6369]], grad_fn=<SliceBackward>)\n",
      "0.9463448226451874\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.9815e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4136]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7524e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8272]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8018]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7656e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5963]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6928e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8466]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2308]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7012]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2570e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2749]], grad_fn=<SliceBackward>)\n",
      "0.9251849204301834\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6106e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6263]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6928e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8456]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0530e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4059]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3736e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4978]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8716e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7937]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3232e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9686]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.6559e-06, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.1310]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6954e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4523]], grad_fn=<SliceBackward>)\n",
      "0.9190128594636917\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2278e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0058]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.4835e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2537]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3060e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1821]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5047e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4788]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4212e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4912]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6536]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2755e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9889]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2517e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9957]], grad_fn=<SliceBackward>)\n",
      "0.9369889944791794\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6675]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8579]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6681]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6682]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6093e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8702]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7166e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8357]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3961e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6698]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.4175e-06, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.1353]], grad_fn=<SliceBackward>)\n",
      "0.8986769914627075\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3961e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6694]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4452]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2517e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9970]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2093e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2790]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7287]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3134]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5072]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5080]], grad_fn=<SliceBackward>)\n",
      "0.8923800513148308\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.4477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2570]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8728]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2292e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7044]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7047e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8404]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1791e-06, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.1504]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.1086e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0575]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.5827e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0272]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8530e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2201]], grad_fn=<SliceBackward>)\n",
      "0.8838892206549644\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8861e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4253]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3345]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3228]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5107]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.1802e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0254]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6332e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8626]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2544e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5174]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5139e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9010]], grad_fn=<SliceBackward>)\n",
      "0.8662857860326767\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4945]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6371e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3378]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0040e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5552]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2067e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5242]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8772]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5620]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.1563e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0331]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6600]], grad_fn=<SliceBackward>)\n",
      "0.8832837343215942\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9028]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3345]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6212e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8647]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1113e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5368]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7733]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6592]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0967e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0618]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7280]], grad_fn=<SliceBackward>)\n",
      "0.9058996215462685\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8928]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9312e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7773]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1378e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2865]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0848e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0713]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1100e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7309]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1139e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2887]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3590e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9529]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7744]], grad_fn=<SliceBackward>)\n",
      "0.9235536307096481\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0279e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5516]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0756]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2875e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9810]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4543e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9174]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6135]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.0385e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7508]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4873e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0328]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7895e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5926]], grad_fn=<SliceBackward>)\n",
      "0.8536463901400566\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9828e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3014]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8437e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0728]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3007e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6915]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9802e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5596]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5736e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8787]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4067e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9365]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5812]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1232e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5363]], grad_fn=<SliceBackward>)\n",
      "0.9422603696584702\n"
     ]
    }
   ],
   "source": [
    "for x in range(100):\n",
    "    print( train_dynamoe_gating(model, train_dls[1], gating_criterion,\n",
    "                                expert_criterion_unreduced,\n",
    "                                CLIP, verbose=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fcaN82bJ-f5",
    "outputId": "2c94f843-e801-4dfb-d79c-89ab704be8fd",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.630012683570385\n",
      "0.47744666039943695\n",
      "0.44646283984184265\n",
      "0.4113902524113655\n",
      "0.4127246364951134\n",
      "0.37290962785482407\n",
      "0.43906231224536896\n",
      "0.385328508913517\n",
      "0.37053677439689636\n",
      "0.3609902262687683\n",
      "0.3726344630122185\n",
      "0.3744181916117668\n",
      "0.36561162024736404\n",
      "0.3524796664714813\n",
      "0.39585812389850616\n",
      "0.32904189825057983\n",
      "0.3342840299010277\n",
      "0.3753737062215805\n",
      "0.40356797724962234\n",
      "0.3463684171438217\n",
      "0.35821671038866043\n",
      "0.36753934621810913\n",
      "0.31112828850746155\n",
      "0.3395487293601036\n",
      "0.32247451692819595\n",
      "0.31689079105854034\n",
      "0.3214147612452507\n",
      "0.3431061953306198\n",
      "0.32723822444677353\n",
      "0.3066282793879509\n",
      "0.2927127256989479\n",
      "0.3012479245662689\n",
      "0.2619609013199806\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-d7e8f4aa73e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_dynamoe_both\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgating_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_criterion_unreduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-00addaa6fd57>\u001b[0m in \u001b[0;36mtrain_dynamoe_both\u001b[0;34m(model, iterator, gating_criterion, expert_criterion, clip)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# Train newly initialized model on new train examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mreduced_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mreduced_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpert_optimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for x in range(400):\n",
    "    print( train_dynamoe_both(model, train_dls[1], gating_criterion, expert_criterion_unreduced, CLIP) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer G: Adaptive Mixture of Experts - adamoe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_adamoe\n",
    "Experts only get trained on sequences gating choses for them\n",
    "\n",
    "Variant 1:\n",
    "1. Compute Model Error\n",
    "2. Compute Gating choice\n",
    "3. \"Question\" Gating choice depending on model error\n",
    "4. Train experts depending on resulting choice\n",
    "5. Train gating on resulting error\n",
    "\n",
    "Variant 2:\n",
    "1. Train gating (pre-expert-train), byproduct: gating error, gating choices\n",
    "2. Depending on gating error, \"question\" gating choices\n",
    "3. Train experts depending on resulting choice\n",
    "\n",
    "Variant 3: (has an inf-loop)\n",
    "1. Compute Gating choices\n",
    "2. Depending result train expert, byproduct expert losses\n",
    "3. train Gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adamoe(model, iterator, gating_criterion, expert_criterion,\n",
    "                         expert_criterion_unreduced, clip, SIGNAL=False):\n",
    "\n",
    "    model.gating.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        #seqs = [seq len, batch size]\n",
    "        #output = [seq len, batch size, vocab sizen]\n",
    "\n",
    "        batch_size = seqs.shape[1]\n",
    "        \n",
    "        # 1. Compute model error\n",
    "        model_outputs = model.forward(seqs, seqs_len, seqs)\n",
    "        \n",
    "        model_loss = compute_loss(model_outputs, seqs,\n",
    "                                  expert_criterion,\n",
    "                                  cutFirstInSequence=True)\n",
    "        \n",
    "        \n",
    "        # 2. Gating choices\n",
    "        model.gating_optimizer.zero_grad()\n",
    "        \n",
    "        gating_outputs = model.gating(seqs, seqs_len)\n",
    "        # gating_outputs = [batch_size, n_max_experts]\n",
    "        \n",
    "        gating_masked = gating_outputs[:,:model.n_active_experts]\n",
    "        \n",
    "        gating_choices = torch.argmax(gating_masked, dim=1)\n",
    "        # gating_choices = [batch_size]\n",
    "        \n",
    "        if SIGNAL:\n",
    "            print(\"gating_out\", gating_outputs)\n",
    "            print(\"choices\", gating_choices)\n",
    "        # 3. \"Question\" choices\n",
    "        # Roll dice depending on error for every batch content\n",
    "        gating_override = torch.empty(batch_size)\n",
    "        for b in range(batch_size):\n",
    "            rand = random.random()\n",
    "            # print(\"rand\", rand)\n",
    "            if SIGNAL:\n",
    "                print(\"model_loss\", model_loss)\n",
    "            if rand < model_loss * 0.75 and model.n_active_experts > 1:\n",
    "                randchoice = random.randint(0, model.n_active_experts - 2)\n",
    "                # cannot choose the original choice\n",
    "                gating_override[b] = randchoice\n",
    "                if randchoice >= gating_choices[b]:\n",
    "                    gating_override[b] += 1\n",
    "            else:\n",
    "                gating_override[b] = gating_choices[b]\n",
    "        if SIGNAL:\n",
    "            print(\"override\", gating_override)\n",
    "        \n",
    "        # 4. Train expert on resulting choice\n",
    "        # Compute loss for all experts at the same time\n",
    "        loss_experts = torch.empty((batch_size, model.n_active_experts))\n",
    "        # loss_experts = [batch_size, n_active_experts]\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            \n",
    "            train_id = int(gating_override[b])\n",
    "            \n",
    "            for e_id in range(model.n_active_experts):\n",
    "                \n",
    "                seq_curr = seqs[:,b].unsqueeze(1)\n",
    "                seq_len_curr = seqs_len[b].unsqueeze(0)\n",
    "\n",
    "                #passage = True\n",
    "                #if SIGNAL and train_id == 0:\n",
    "                #   passage = False\n",
    "            \n",
    "                if train_id == e_id:\n",
    "                    model.experts[e_id].train()\n",
    "                    model.expert_optimizers[e_id].zero_grad()\n",
    "\n",
    "                    # Get model prediction\n",
    "                    expert_outputs = model.experts[e_id](seq_curr,\n",
    "                                                         seq_len_curr,\n",
    "                                                         seq_curr)\n",
    "\n",
    "                    loss = compute_loss(expert_outputs, seq_curr,\n",
    "                                        expert_criterion,\n",
    "                                        cutFirstInSequence=True)\n",
    "\n",
    "                    # Log loss to train gating\n",
    "                    loss_experts[b,e_id] = loss\n",
    "\n",
    "                    # Train newly initialized model on new train examples\n",
    "                    loss.backward()\n",
    "                    model.expert_optimizers[e_id].step()\n",
    "\n",
    "                else:\n",
    "                    model.experts[e_id].eval()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "\n",
    "                        # Get model prediction\n",
    "                        expert_outputs = model.experts[e_id](seq_curr,\n",
    "                                                             seq_len_curr,\n",
    "                                                             seq_curr)\n",
    "\n",
    "                        loss = compute_loss(expert_outputs, seq_curr,\n",
    "                                            expert_criterion,\n",
    "                                            cutFirstInSequence=True)\n",
    "#                         if SIGNAL:\n",
    "#                             print(f\"e_id:{e_id} loss:{loss}\")\n",
    "\n",
    "                        # Log loss to train gating\n",
    "                        loss_experts[b,e_id] = loss\n",
    "            \n",
    "            model.n_train_on_experts[train_id] += 1\n",
    "            model.expert_schedulers[train_id].step()\n",
    "\n",
    "        # 5. Train Gating\n",
    "        # Compute expert which should have been chosen\n",
    "        gating_trgs = loss_experts.argmin(dim=1)\n",
    "        # gating_trgs = [batch_size]\n",
    "        \n",
    "        if SIGNAL:\n",
    "            print(\"loss_experts\", loss_experts)\n",
    "            print(\"gating_trgs\", gating_trgs)\n",
    "\n",
    "        gating_trgs = gating_trgs.unsqueeze(0)\n",
    "        # gating_trgs = [[batch_size]]\n",
    "        gating_outputs = gating_outputs.unsqueeze(0)\n",
    "        # gating_ouputs = [[batch_size, n_max_experts]]\n",
    "\n",
    "        gating_loss = compute_loss(gating_outputs, gating_trgs, gating_criterion,\n",
    "                            cutFirstInSequence=False)\n",
    "\n",
    "        gating_loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        model.gating_optimizer.step()\n",
    "\n",
    "        # Get loss of model gating chose\n",
    "        loss_chosen_experts = loss_experts[:,gating_choices]\n",
    "        \n",
    "        loss_chosen_experts = loss_chosen_experts.mean()\n",
    "        \n",
    "        epoch_loss += loss_chosen_experts.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replay_adamoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_adamoe(model, task_id, gating_criterion, expert_criterion_unreduced, clip,SIGNAL):\n",
    "    for other_id in range(task_id):\n",
    "        train_dynamoe_gating(model, train_dls[other_id],\n",
    "                              gating_criterion,\n",
    "                              expert_criterion_unreduced,\n",
    "                              clip,SIGNAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_adamoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_adamoe(\n",
    "    n_tasks_total,\n",
    "    model,\n",
    "    task_id,\n",
    "    n_task_epochs,\n",
    "    step_size_evaluation,\n",
    "    gating_criterion,\n",
    "    expert_criterion,\n",
    "    expert_criterion_unreduced,\n",
    "    clip=1,\n",
    "    repetition=None,\n",
    "    verbose=False\n",
    "):\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((n_tasks_total, 3, n_task_epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((n_tasks_total, 3, n_task_epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "    \n",
    "    for epoch in range(n_task_epochs):\n",
    "        # First Epoch log performance BEFORE training\n",
    "        if epoch == 0:\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, 0, expert_criterion)\n",
    "        \n",
    "        # If training does not lead anywhere, init new expert\n",
    "        if (model.epoch > model.allowed_until_check\n",
    "            and model.n_active_experts < model.n_max_experts\n",
    "           ):\n",
    "            init_expert = False\n",
    "            # Lookahead check whether data from another task is coming in\n",
    "            loss_ahead = evaluate_extra(model, valid_dls[task_id],\n",
    "                                        allOrNoneLoss)\n",
    "            last_loss = model.loss_tracker[-1]\n",
    "            if ((loss_ahead - PERFORMANCE_DIFFERENCE_NEW_TASK > last_loss)):\n",
    "                init_expert = True\n",
    "            \n",
    "            # Check whether learning is stagnating\n",
    "            # (difference to N_EPOCHS_UNTIL_NEW_EXPERT is small enough)\n",
    "            if (((model.loss_tracker[model.epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                     last_loss) < ALLOWED_ERROR_VARIANCE)\n",
    "                and \n",
    "                (model.valid_loss > PERFORMANCE_TRESHHOLD_START)\n",
    "               ):\n",
    "                init_expert = True\n",
    "                        \n",
    "            if init_expert:\n",
    "                # Add new expert\n",
    "                model.status = \"train_gating_uninitialized_expert\"\n",
    "                model.allowed_until_check = model.epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                model.add_expert()\n",
    "\n",
    "                print(\"-----------------------------------\")\n",
    "                print(\"--------- Init new Expert ---------\")\n",
    "                print(\"-----------------------------------\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train model\n",
    "        train_loss = train_adamoe(model, train_dls[task_id],\n",
    "                                  gating_criterion, expert_criterion,\n",
    "                                  expert_criterion_unreduced,\n",
    "                                  clip, verbose)\n",
    "\n",
    "        model.valid_loss = evaluate(model, valid_dls[task_id],\n",
    "                                    expert_criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Log hits\n",
    "        model.loss_tracker.append(evaluate_extra(model, valid_dls[task_id],\n",
    "                                                 allOrNoneLoss))\n",
    "            \n",
    "#         if model.valid_loss < best_valid_loss:\n",
    "#             best_valid_loss = model.valid_loss\n",
    "#             torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        # Log performance AFTER training\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0 and epoch != 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, idx, expert_criterion)\n",
    "            \n",
    "        if epoch % STEP_SIZE_REPLAY == 0 and epoch != 0:\n",
    "            print(\"-Replay\")\n",
    "            replay_adamoe(model, task_id, gating_criterion, expert_criterion_unreduced, clip, verbose)\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if repetition is not None:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | R{repetition} T{task_id}')\n",
    "        else:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | T{task_id}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {model.valid_loss:.3f} |  Val. PPL: {math.exp(model.valid_loss):7.3f}')\n",
    "        \n",
    "        model.epoch += 1\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_adamoe():\n",
    "    # Initialize DynaMoE\n",
    "    model = DynaMoE(expert_decay=True)\n",
    "    print(model.apply(init_weights))\n",
    "    \n",
    "    # Need some extra variables for the model\n",
    "    model.epoch = 0\n",
    "    model.loss_tracker = []\n",
    "    model.allowed_until_check = N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "\n",
    "    # gating_criterion = CosineLoss(N_MAX_EXPERTS, ignore_index=None)\n",
    "    # Cosine loss is inpractical for Gating because result vectors are low dimensional\n",
    "    # Cosine loss works better for small datasets, thus used for experts\n",
    "    gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    expert_criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)\n",
    "    expert_criterion_unreduced = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN,\n",
    "                                            reduction=\"none\")\n",
    "    \n",
    "    return (model, gating_criterion, expert_criterion, expert_criterion_unreduced)\n",
    "\n",
    "def repeat_adamoe(n_tasks_total, n_task_epochs, task_id, step_size_evaluation, repetition, pass_on_variables):\n",
    "    model, gating_criterion, expert_criterion, expert_criterion_unreduced = pass_on_variables\n",
    "    \n",
    "    hist_loss, hist_hits = fit_adamoe(\n",
    "        n_tasks_total,\n",
    "        model,\n",
    "        task_id,\n",
    "        n_task_epochs,\n",
    "        step_size_evaluation,\n",
    "        gating_criterion,\n",
    "        expert_criterion,\n",
    "        expert_criterion_unreduced,\n",
    "        repetition=repetition,\n",
    "        verbose=False\n",
    "    )\n",
    "    return hist_loss, hist_hits, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment AdaMoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "CvFmPpKQozmz"
   },
   "outputs": [],
   "source": [
    "N_EXPERTS_START = 1\n",
    "N_MAX_EXPERTS = 3\n",
    "GATE_DROPOUT = 0.5\n",
    "N_GATING_HIDDEN_DIM = 15\n",
    "N_GATING_EMBED_DIM = 9\n",
    "LEARNING_RATE_GATING = 0.008\n",
    "SCHEDULE = not_interleaved\n",
    "\n",
    "# If the amount of missed hits is bigger then PERFORMANCE_TRESHHOLD_START\n",
    "# and it stays within ALLOWED_ERROR_VARIANCE for\n",
    "# N_EPOCHS_UNTIL_NEW_EXPERT epochs, then a new\n",
    "# expert is initialized\n",
    "N_EPOCHS_UNTIL_NEW_EXPERT = 30\n",
    "ALLOWED_ERROR_VARIANCE = 0.05\n",
    "PERFORMANCE_TRESHHOLD_START = 0.4\n",
    "\n",
    "# Difference between replicated sequence accuracy of training sequence\n",
    "# before and training sequence coming in to decide to consolidate for\n",
    "# a new expert.\n",
    "PERFORMANCE_DIFFERENCE_NEW_TASK = 0.3 # 0.5 for bad example\n",
    "\n",
    "STEP_SIZE_REPLAY = 5\n",
    "STEP_SIZE_DECAY = 20\n",
    "GAMMA_DECAY = 0.95\n",
    "LEARNING_RATE = 0.05\n",
    "# need seperate learning rates for gating and experts\n",
    "\n",
    "TEST_ALL_TASKS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "id": "dUUt4knHYfDj"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRoGUVZcfgMg",
    "outputId": "2c57ceea-2add-4c03-f0b3-5a9252f522d0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@ Repetition   0 @@@@@@@@@\n",
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 9)\n",
      "    (rnn): GRU(9, 15, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=30, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(19, 9, bidirectional=True)\n",
      "        (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "          (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(37, 9)\n",
      "        (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: AdaMoE-1.s0.t0.e400\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.620 | Train PPL:   1.858\n",
      "\t Val. Loss: 0.465 |  Val. PPL:   1.592\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.465 | Train PPL:   1.592\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.412 | Train PPL:   1.509\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.488\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "-Replay\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.502\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.462\n",
      "-Replay\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.449\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.499\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.457\n",
      "-Replay\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "-Replay\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "-Replay\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "-Replay\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "-Replay\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "-Replay\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.232\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "-Replay\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "-Replay\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "-Replay\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "-Replay\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "-Replay\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.134\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "-Replay\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "-Replay\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "-Replay\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "-Replay\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "-Replay\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "-Replay\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "-Replay\n",
      "Epoch: 101 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 102 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 103 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 104 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 105 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "-Replay\n",
      "Epoch: 106 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 107 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 108 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 109 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 110 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "-Replay\n",
      "Epoch: 111 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 112 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 113 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 114 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 115 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "-Replay\n",
      "Epoch: 116 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 117 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 118 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 119 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 120 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-Replay\n",
      "Epoch: 121 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 122 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 123 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 124 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 125 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-Replay\n",
      "Epoch: 126 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 127 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 128 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 129 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 130 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-Replay\n",
      "Epoch: 131 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 132 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 133 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 134 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 135 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-Replay\n",
      "Epoch: 136 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 137 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 138 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 139 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 140 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-Replay\n",
      "Epoch: 141 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 142 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 143 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 144 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 145 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-Replay\n",
      "Epoch: 146 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 147 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 148 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 149 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 150 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "-Replay\n",
      "Epoch: 151 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 152 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 153 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 154 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 155 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-Replay\n",
      "Epoch: 156 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 157 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 158 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 159 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 160 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-Replay\n",
      "Epoch: 161 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 162 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 163 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 164 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 165 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-Replay\n",
      "Epoch: 166 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 167 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 168 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 169 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 170 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "-Replay\n",
      "Epoch: 171 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 172 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 173 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 174 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 175 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-Replay\n",
      "Epoch: 176 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 177 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 178 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 179 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 180 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-Replay\n",
      "Epoch: 181 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 182 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 183 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 184 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 185 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "-Replay\n",
      "Epoch: 186 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 187 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 188 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 189 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 190 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 191 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 192 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 193 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 194 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 195 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 196 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 197 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 198 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 199 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 200 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 201 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 202 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 203 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 204 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 205 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "-Replay\n",
      "Epoch: 206 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 207 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 208 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 209 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 210 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-Replay\n",
      "Epoch: 211 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 212 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 213 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 214 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 215 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-Replay\n",
      "Epoch: 216 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 217 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 218 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 219 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 220 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "-Replay\n",
      "Epoch: 221 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 222 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 223 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 224 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 225 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 226 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 227 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 228 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 229 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 230 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 231 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 232 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 233 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 234 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 235 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 236 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 237 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 238 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 239 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 240 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 241 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 242 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 243 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 244 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 245 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 246 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 247 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 248 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 249 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 250 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 251 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 252 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 253 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 254 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 255 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 256 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 257 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 258 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 259 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 260 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 261 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 262 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 263 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 264 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 265 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 266 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 267 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 268 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 269 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 270 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 271 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 272 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 273 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 274 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 275 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 276 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 277 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 278 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 279 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 280 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 281 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 282 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 283 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 284 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 285 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 286 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 287 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 288 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 289 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 290 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 291 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 292 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 293 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 294 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 295 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 296 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 297 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 298 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 299 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 300 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 301 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 302 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 303 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 304 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 305 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 306 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 307 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 308 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 309 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 310 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 311 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 312 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 313 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 314 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 315 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 316 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 317 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 318 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 319 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 320 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 321 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 322 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 323 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 324 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 325 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 326 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 327 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 328 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 329 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 330 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 331 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 332 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 333 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 334 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 335 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 336 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 337 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 338 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 339 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 340 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 341 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 342 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 343 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 344 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 345 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 346 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 347 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 348 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 349 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 350 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 351 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 352 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 353 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 354 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 355 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 356 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 357 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 358 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 359 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 360 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 361 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 362 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 363 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 364 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 365 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 366 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 367 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 368 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 369 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 370 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 371 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 372 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 373 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 374 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 375 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 376 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 377 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 378 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 379 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 380 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 381 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 382 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 383 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 384 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 385 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 386 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 387 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 388 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 389 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 390 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 391 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 392 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 393 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 394 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 395 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 396 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 397 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 398 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 399 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 400 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "\n",
      "SCHEDULE: AdaMoE-1.s1.t1.e400\n",
      "-----------------------------------\n",
      "--------- Init new Expert ---------\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.621 | Train PPL:   1.860\n",
      "\t Val. Loss: 0.627 |  Val. PPL:   1.872\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.629 | Train PPL:   1.876\n",
      "\t Val. Loss: 0.627 |  Val. PPL:   1.871\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.607 | Train PPL:   1.834\n",
      "\t Val. Loss: 0.560 |  Val. PPL:   1.750\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.562 | Train PPL:   1.755\n",
      "\t Val. Loss: 0.626 |  Val. PPL:   1.869\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.475 | Train PPL:   1.608\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.583\n",
      "-Replay\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.419 | Train PPL:   1.520\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.533\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.638 | Train PPL:   1.892\n",
      "\t Val. Loss: 0.625 |  Val. PPL:   1.868\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.578\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.547\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.387 | Train PPL:   1.473\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.509\n",
      "-Replay\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.505\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.505\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.481\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.509\n",
      "-Replay\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
      "\t Val. Loss: 0.429 |  Val. PPL:   1.535\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.417 |  Val. PPL:   1.517\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.494\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "-Replay\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.474\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "-Replay\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.475\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.404 |  Val. PPL:   1.498\n",
      "-Replay\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.429 |  Val. PPL:   1.536\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.500\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "-Replay\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "-Replay\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.318 |  Val. PPL:   1.375\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "-Replay\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.352\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.421\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "-Replay\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.297 |  Val. PPL:   1.345\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.340\n",
      "-Replay\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.257\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "-Replay\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "-Replay\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "-Replay\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "-Replay\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "-Replay\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "-Replay\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.125\n",
      "-Replay\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "-Replay\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "-Replay\n",
      "Epoch: 101 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 102 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 103 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 104 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 105 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "-Replay\n",
      "Epoch: 106 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 107 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 108 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 109 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 110 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "-Replay\n",
      "Epoch: 111 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 112 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 113 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 114 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 115 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-Replay\n",
      "Epoch: 116 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 117 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 118 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 119 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 120 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 121 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 122 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 123 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 124 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 125 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-Replay\n",
      "Epoch: 126 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 127 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 128 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 129 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 130 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-Replay\n",
      "Epoch: 131 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 132 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 133 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 134 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 135 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-Replay\n",
      "Epoch: 136 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 137 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 138 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 139 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 140 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "-Replay\n",
      "Epoch: 141 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 142 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 143 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 144 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 145 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 146 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 147 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 148 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 149 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 150 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "-Replay\n",
      "Epoch: 151 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 152 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 153 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 154 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 155 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 156 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 157 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 158 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 159 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 160 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "-Replay\n",
      "Epoch: 161 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 162 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 163 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 164 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 165 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "-Replay\n",
      "Epoch: 166 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 167 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 168 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 169 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 170 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "-Replay\n",
      "Epoch: 171 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 172 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 173 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 174 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 175 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "-Replay\n",
      "Epoch: 176 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 177 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 178 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 179 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 180 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "-Replay\n",
      "Epoch: 181 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 182 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 183 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 184 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 185 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "-Replay\n",
      "Epoch: 186 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 187 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 188 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 189 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 190 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "-Replay\n",
      "Epoch: 191 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 192 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 193 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 194 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 195 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "-Replay\n",
      "Epoch: 196 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 197 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 198 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 199 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 200 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "-Replay\n",
      "Epoch: 201 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 202 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 203 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 204 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 205 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 206 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 207 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 208 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 209 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 210 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 211 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 212 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 213 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 214 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 215 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 216 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 217 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 218 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 219 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 220 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 221 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 222 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 223 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 224 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 225 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 226 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 227 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 228 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 229 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 230 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 231 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 232 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 233 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 234 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 235 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 236 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 237 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 238 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 239 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 240 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 241 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 242 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 243 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 244 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 245 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 246 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 247 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 248 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 249 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 250 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 251 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 252 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 253 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 254 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 255 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 256 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 257 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 258 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 259 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 260 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 261 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 262 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 263 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 264 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 265 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 266 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 267 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 268 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 269 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 270 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 271 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 272 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 273 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 274 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 275 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 276 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 277 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 278 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 279 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 280 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 281 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 282 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 283 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 284 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 285 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 286 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 287 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 288 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 289 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 290 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 291 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 292 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 293 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 294 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 295 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 296 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 297 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 298 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 299 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 300 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 301 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 302 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 303 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 304 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 305 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 306 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 307 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 308 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 309 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 310 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 311 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 312 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 313 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 314 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 315 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 316 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 317 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 318 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 319 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 320 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 321 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 322 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 323 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 324 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 325 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 326 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 327 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 328 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 329 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 330 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 331 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 332 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 333 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 334 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 335 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 336 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 337 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 338 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 339 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 340 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 341 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 342 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 343 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 344 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 345 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 346 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 347 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 348 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 349 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 350 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 351 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 352 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 353 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 354 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 355 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 356 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 357 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 358 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 359 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 360 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 361 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 362 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 363 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 364 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 365 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 366 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 367 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 368 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 369 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 370 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 371 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 372 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 373 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 374 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 375 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 376 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 377 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 378 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 379 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 380 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 381 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 382 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 383 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 384 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 385 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 386 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 387 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 388 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 389 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 390 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 391 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 392 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 393 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 394 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 395 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 396 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 397 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 398 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 399 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 400 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "\n",
      "SCHEDULE: AdaMoE-1.s2.t2.e400\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 101 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 102 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 103 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 104 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 105 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 106 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 107 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 108 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 109 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 110 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 111 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 112 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 113 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 114 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 115 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 116 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 117 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 118 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 119 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 120 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 121 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 122 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 123 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 124 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 125 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 126 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 127 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 128 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 129 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 130 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 131 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 132 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 133 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 134 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 135 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 136 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 137 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 138 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 139 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 140 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 141 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 142 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 143 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 144 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 145 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 146 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 147 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 148 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 149 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 150 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 151 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 152 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 153 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 154 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 155 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 156 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 157 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 158 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 159 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 160 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 161 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 162 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 163 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 164 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 165 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 166 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 167 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 168 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 169 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 170 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 171 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 172 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 173 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 174 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 175 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 176 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 177 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 178 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 179 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 180 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 181 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 182 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 183 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 184 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 185 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 186 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 187 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 188 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 189 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 190 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 191 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 192 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 193 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 194 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 195 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 196 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 197 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 198 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 199 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 200 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 201 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 202 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 203 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 204 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 205 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 206 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 207 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 208 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 209 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 210 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 211 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 212 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 213 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 214 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 215 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 216 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 217 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 218 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 219 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 220 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 221 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 222 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 223 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 224 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 225 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 226 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 227 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 228 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 229 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 230 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 231 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 232 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 233 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 234 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 235 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 236 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 237 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 238 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 239 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 240 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 241 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 242 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 243 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 244 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 245 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 246 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 247 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 248 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 249 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 250 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 251 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 252 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 253 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 254 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 255 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 256 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 257 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 258 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 259 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 260 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 261 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 262 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 263 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 264 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 265 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 266 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 267 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 268 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 269 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 270 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 271 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 272 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 273 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 274 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 275 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 276 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 277 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 278 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 279 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 280 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 281 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 282 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 283 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 284 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 285 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 286 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 287 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 288 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 289 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 290 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 291 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 292 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 293 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 294 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 295 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 296 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 297 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 298 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 299 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 300 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 301 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 302 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 303 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 304 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 305 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 306 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 307 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 308 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 309 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 310 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 311 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 312 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 313 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 314 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 315 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 316 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 317 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 318 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 319 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 320 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 321 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 322 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 323 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 324 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 325 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 326 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 327 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 328 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 329 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 330 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 331 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 332 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 333 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 334 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 335 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 336 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 337 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 338 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 339 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 340 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 341 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 342 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 343 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 344 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 345 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 346 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 347 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 348 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 349 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 350 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 351 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 352 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 353 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 354 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 355 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 356 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 357 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 358 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 359 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 360 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 361 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 362 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 363 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 364 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 365 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 366 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 367 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 368 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 369 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 370 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 371 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 372 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 373 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 374 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 375 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 376 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 377 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 378 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 379 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 380 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 381 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 382 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 383 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 384 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 385 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 386 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 387 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 388 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 389 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 390 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 391 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 392 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 393 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 394 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 395 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 396 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 397 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 398 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 399 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 400 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = 1\n",
    "hist_all_losses_G, hist_all_hitsss_G, models_G = experiment(\n",
    "    \"AdaMoE-1\",\n",
    "    n_repetitions,\n",
    "    SCHEDULE,\n",
    "    init_adamoe,\n",
    "    repeat_adamoe,\n",
    "    STEP_SIZE_EVALUATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_G, SCHEDULE, STEP_SIZE_EVALUATION, datasets=(0,1), figsize=(12,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - tensor([1, 7, 6, 5, 4, 6, 5, 2])\n",
      "0 - tensor([1, 7, 3, 6, 4, 6, 5, 2])\n",
      "0 - tensor([1, 7, 6, 4, 6, 5, 2])\n",
      "0 - tensor([1, 7, 6, 4, 2])\n",
      "0 - tensor([1, 7, 6, 5, 4, 2])\n",
      "0 - tensor([1, 7, 3, 6, 4, 2])\n",
      "0 - tensor([1, 7, 3, 6, 5, 4, 6, 5, 2])\n",
      "0 - tensor([1, 7, 3, 6, 4, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "show_expert(models_G[2], train_dls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expertiment AdaMoE interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "CvFmPpKQozmz"
   },
   "outputs": [],
   "source": [
    "N_EXPERTS_START = 1\n",
    "N_MAX_EXPERTS = 3\n",
    "GATE_DROPOUT = 0.5\n",
    "N_GATING_HIDDEN_DIM = 15\n",
    "N_GATING_EMBED_DIM = 9\n",
    "LEARNING_RATE_GATING = 0.008\n",
    "SCHEDULE = dbg_not_interleaved\n",
    "\n",
    "# If the amount of missed hits is bigger then PERFORMANCE_TRESHHOLD_START\n",
    "# and it stays within ALLOWED_ERROR_VARIANCE for\n",
    "# N_EPOCHS_UNTIL_NEW_EXPERT epochs, then a new\n",
    "# expert is initialized\n",
    "N_EPOCHS_UNTIL_NEW_EXPERT = 30\n",
    "ALLOWED_ERROR_VARIANCE = 0.05\n",
    "PERFORMANCE_TRESHHOLD_START = 0.4\n",
    "\n",
    "# Difference between replicated sequence accuracy of training sequence\n",
    "# before and training sequence coming in to decide to consolidate for\n",
    "# a new expert.\n",
    "PERFORMANCE_DIFFERENCE_NEW_TASK = 0.3 # 0.5 for bad example\n",
    "\n",
    "STEP_SIZE_REPLAY = 5\n",
    "STEP_SIZE_DECAY = 40\n",
    "GAMMA_DECAY = 0.95\n",
    "\n",
    "TEST_ALL_TASKS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "dUUt4knHYfDj"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRoGUVZcfgMg",
    "outputId": "2c57ceea-2add-4c03-f0b3-5a9252f522d0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@ Repetition   0 @@@@@@@@@\n",
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 9)\n",
      "    (rnn): GRU(9, 15, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=30, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(19, 9, bidirectional=True)\n",
      "        (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "          (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(37, 9)\n",
      "        (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: AdaMoE-2.s0.t0.e50\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.603 | Train PPL:   1.828\n",
      "\t Val. Loss: 0.502 |  Val. PPL:   1.651\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.483 | Train PPL:   1.620\n",
      "\t Val. Loss: 0.460 |  Val. PPL:   1.583\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.436 | Train PPL:   1.547\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.493\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.494\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.387 | Train PPL:   1.473\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "-Replay\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "-Replay\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.437\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.317 |  Val. PPL:   1.372\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "-Replay\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "-Replay\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.244\n",
      "-Replay\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.424\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "-Replay\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.269\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "-Replay\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "-Replay\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "-Replay\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "\n",
      "SCHEDULE: AdaMoE-2.s1.t1.e50\n",
      "-----------------------------------\n",
      "--------- Init new Expert ---------\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.582 | Train PPL:   1.790\n",
      "\t Val. Loss: 0.663 |  Val. PPL:   1.941\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.491 | Train PPL:   1.634\n",
      "\t Val. Loss: 0.478 |  Val. PPL:   1.613\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.425 | Train PPL:   1.529\n",
      "\t Val. Loss: 0.424 |  Val. PPL:   1.527\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.451\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.509\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.484\n",
      "-Replay\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.469\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.499\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.449\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.484\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "-Replay\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.484\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.493\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.417 |  Val. PPL:   1.517\n",
      "-Replay\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.484 |  Val. PPL:   1.622\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.511\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.421\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "-Replay\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.410\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.407\n",
      "-Replay\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.434\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.463\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.460\n",
      "-Replay\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "-Replay\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.493\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.488\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "-Replay\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.437\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
      "-Replay\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.515\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.414 |  Val. PPL:   1.513\n",
      "-----------------------------------\n",
      "--------- Init new Expert ---------\n",
      "-----------------------------------\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.433\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
      "\n",
      "SCHEDULE: AdaMoE-2.s2.t2.e50\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 0.478 |  Val. PPL:   1.612\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.463 |  Val. PPL:   1.589\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.579\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
      "\t Val. Loss: 0.429 |  Val. PPL:   1.535\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.524\n",
      "-Replay\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.550\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.537\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.404 |  Val. PPL:   1.498\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.476\n",
      "\t Val. Loss: 0.415 |  Val. PPL:   1.514\n",
      "-Replay\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.424 |  Val. PPL:   1.529\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.515\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.532\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.543\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.543\n",
      "-Replay\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.441 |  Val. PPL:   1.554\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.415 |  Val. PPL:   1.515\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "-Replay\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.356\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.389\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
      "-Replay\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.317\n",
      "-Replay\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.359\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.340\n",
      "-Replay\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.337\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.308 |  Val. PPL:   1.360\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
      "-Replay\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "-Replay\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.317\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = 1\n",
    "hist_all_losses_G, hist_all_hitsss_G, models_G = experiment(\n",
    "    \"AdaMoE-2\",\n",
    "    n_repetitions,\n",
    "    SCHEDULE,\n",
    "    init_adamoe,\n",
    "    repeat_adamoe,\n",
    "    STEP_SIZE_EVALUATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_G, SCHEDULE, STEP_SIZE_EVALUATION, datasets=(0,1), figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAASoarxMs3k"
   },
   "source": [
    "## Transfer F: Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gating_Ensembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gating_Ensembler(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, n_gating_hidden, n_experts,\n",
    "                 n_max_experts, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.n_experts = n_experts\n",
    "\n",
    "        self.n_max_experts = n_max_experts\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(embed_dim, n_gating_hidden, bidirectional=True)\n",
    "\n",
    "        self.fc_out = nn.Linear(n_gating_hidden * 2, n_max_experts)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, seqs, seqs_len, verbose=False):\n",
    "        \n",
    "        # seqs = [seq len, batch_size]\n",
    "        # seqs_len = [batch_size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(seqs))\n",
    "        \n",
    "        # embedded = [seq len, batch_size, embed_dim]\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, seqs_len.to(\"cpu\"))\n",
    "\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "\n",
    "        # outputs = [seq len, batch_size, n_experts * num directions]\n",
    "        # hidden = [n layers * num directions, batch size, n_experts]\n",
    "\n",
    "        hidden = hidden.squeeze(0)\n",
    "\n",
    "        # hidden = [batch_size, n_max_experts]\n",
    "\n",
    "        outputs = outputs[-1]\n",
    "\n",
    "        outputs = self.fc_out(outputs)\n",
    "\n",
    "        # outputs = [batch_size, n_max_experts]\n",
    "        if verbose:\n",
    "            print(\"gating_before_sigmoid\", outputs )\n",
    "        return torch.sigmoid(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gating_ensembler():\n",
    "    gating = Gating_Ensembler(INPUT_DIM, N_GATING_EMBED_DIM,\n",
    "                              N_GATING_HIDDEN_DIM, N_EXPERTS_START,\n",
    "                              N_MAX_EXPERTS, GATE_DROPOUT)\n",
    "    gating.to(device)\n",
    "    gating_optimizer = optim.Adam(gating.parameters(), lr=LEARNING_RATE_GATING)\n",
    "    return gating, gating_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "ztDsiI2WM98Y"
   },
   "outputs": [],
   "source": [
    "class Ensembler(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gating=None,\n",
    "        gating_optimizer=None,\n",
    "        experts=None,\n",
    "        expert_optimizers=None,\n",
    "        status=\"train_gating_initialized_expert\",\n",
    "        expert_decay=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        gating: nn.Module\n",
    "            Gating module\n",
    "        gating_optimizer: optim\n",
    "            optimizer for passed Gating module\n",
    "        expert: list of nn.Module\n",
    "            list of task experts\n",
    "        expert_optimizers: list of optim\n",
    "            list of optimizer for the expert at the same index\n",
    "        \"\"\"\n",
    "        super(Ensembler, self).__init__()\n",
    "\n",
    "        assert (gating is None) == (gating_optimizer is None), \"gating needs gating_optimizer, and vice versa\"\n",
    "        \n",
    "        if gating is None:\n",
    "            gating, gating_optimizer = init_gating_ensembler()\n",
    "        self.gating = gating\n",
    "        self.gating_optimizer = gating_optimizer\n",
    "        \n",
    "        if experts is None:\n",
    "            expert, expert_optimizer = init_expert()\n",
    "            experts = [expert,]\n",
    "            expert_optimizers = [expert_optimizer,]\n",
    "        \n",
    "        assert len(experts) == len(expert_optimizers), \"unequal amount of experts and expert_optimizers\"\n",
    "        \n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        self.expert_optimizers = expert_optimizers\n",
    "        \n",
    "        self.n_train_on_experts = [0 for _ in experts]   # List how often expert has been trained\n",
    "        self.n_active_experts = len(experts)\n",
    "        self.n_max_experts = gating.n_max_experts\n",
    "        \n",
    "        self.status = status\n",
    "        \n",
    "        self.epoch = 0\n",
    "        self.loss_tracker = []\n",
    "        self.allowed_until_check = N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "        \n",
    "        self.expert_decay = expert_decay\n",
    "        if self.expert_decay:\n",
    "            # Initialize Scheduler for expert decay\n",
    "            initScheduler = lambda opt: optim.lr_scheduler.StepLR(opt,\n",
    "                                                                  step_size=STEP_SIZE_DECAY,\n",
    "                                                                  gamma=GAMMA_DECAY)\n",
    "            self.expert_schedulers = [initScheduler(opt) for opt in expert_optimizers]\n",
    "\n",
    "    def forward(self, seqs, seqs_len, trgs, teacher_forcing_ratio=0.5, verbose=False):\n",
    "        #seqs = [seqs len, batch size]\n",
    "        #seqs_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "\n",
    "        vocab_size = self.gating.input_dim\n",
    "        seq_len, batch_size = seqs.shape\n",
    "        \n",
    "        # Decide which expert to use\n",
    "        gatings = self.gating(seqs, seqs_len, verbose)\n",
    "\n",
    "        # gatings = [batch_size, n_max_experts]\n",
    "        \n",
    "        gating_masked = gatings[:,:self.n_active_experts]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"gating_masked: {gating_masked}\")\n",
    "            experts_loss = torch.empty((self.n_active_experts, batch_size))\n",
    "\n",
    "        expert_outputs = torch.empty((self.n_active_experts, seq_len, batch_size, vocab_size))\n",
    "        for e_id in range(self.n_active_experts):\n",
    "            expert_output = self.experts[e_id](seqs, seqs_len, seqs, teacher_forcing_ratio)\n",
    "            # expert_output = [seqs_len, batch_size, vocab_size]\n",
    "\n",
    "            # Weigh every experts output with respective gating weight\n",
    "            for b in range(batch_size):\n",
    "                if verbose:\n",
    "                    experts_loss[e_id,b] = compute_loss(expert_output, seqs, criterion, cutFirstInSequence=True)\n",
    "                #     expert_output[:,b] = expert_output[:,b] * gating_masked[b,e_id]\n",
    "                expert_outputs[e_id,:,b] = expert_output[:,b] * gating_masked[b,e_id]\n",
    "                # expert_outputs[e_id,:,b] = expert_output[:,b]\n",
    "            \n",
    "            # print(\"expert_out\")\n",
    "            # print(expert_output)\n",
    "            # print(\"gating_masked\")\n",
    "            # print(gating_masked)\n",
    "\n",
    "        weighted_outputs = expert_outputs.sum(dim=0)\n",
    "        # weighted_outputs = [seqs_len, batch_size, vocab_size]\n",
    "        if verbose:\n",
    "            print(\"expert_loss\", experts_loss)\n",
    "\n",
    "        return weighted_outputs\n",
    "\n",
    "    def add_expert(self):\n",
    "        # Get new expert\n",
    "        expert, expert_optimizer = init_expert()\n",
    "        self.experts.append(expert)\n",
    "        self.expert_optimizers.append(expert_optimizer)\n",
    "        self.n_active_experts += 1\n",
    "        self.n_train_on_experts.append(0)\n",
    "        \n",
    "        if self.expert_decay:\n",
    "            self.expert_schedulers.append(\n",
    "                optim.lr_scheduler.StepLR(expert_optimizer, \n",
    "                                          step_size=STEP_SIZE_DECAY,\n",
    "                                          gamma=GAMMA_DECAY)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_ensembler_gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensembler_gating(model, iterator, criterion, clip, verbose=False):\n",
    "    assert isinstance(model, Ensembler)\n",
    "    model.gating.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        \n",
    "        #seqs = [seq len, batch size]\n",
    "        #output = [seq len, batch size, vocab sizen]\n",
    "        vocab_size = model.gating.input_dim\n",
    "        seq_len, batch_size = seqs.shape\n",
    "\n",
    "        model.gating_optimizer.zero_grad()\n",
    "        \n",
    "        gating_outputs = model.gating(seqs, seqs_len)\n",
    "\n",
    "        # gating_outputs = [batch_size, n_max_experts]\n",
    "        \n",
    "        gating_masked = gating_outputs[:,:model.n_active_experts]\n",
    "\n",
    "        ## Compute best choice for gating network\n",
    "        # Compute loss for each expert network\n",
    "        expert_outputs = torch.empty((model.n_active_experts, seq_len,\n",
    "                                      batch_size, vocab_size))\n",
    "        for e_id in range(model.n_active_experts):\n",
    "\n",
    "            model.experts[e_id].eval()\n",
    "\n",
    "            expert_output = model.experts[e_id](seqs, seqs_len, seqs)\n",
    "            # expert_output = [seqs_len, batch_size, vocab_size]\n",
    "\n",
    "            # Weigh every experts output with respective gating weight\n",
    "            for b in range(batch_size):\n",
    "                #     expert_output[:,b] = expert_output[:,b] * gating_masked[b,e_id]\n",
    "                expert_outputs[e_id,:,b] = expert_output[:,b] * gating_masked[b,e_id]\n",
    "            \n",
    "        weighted_outputs = expert_outputs.sum(dim=0)\n",
    "        # weighted_outputs = [seqs_len, batch_size, vocab_size]\n",
    "\n",
    "        # Gating Loss just is total loss\n",
    "        gating_loss = compute_loss(weighted_outputs, seqs, criterion,\n",
    "                            cutFirstInSequence=True)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\">> Gating Loss\")\n",
    "            print(gating_loss)\n",
    "\n",
    "        gating_loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        model.gating_optimizer.step()\n",
    "\n",
    "        if verbose:\n",
    "            print(\"-- Masked Gating\")\n",
    "            print(gating_masked)\n",
    "        \n",
    "        epoch_loss += gating_loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_ensembler_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_ensembler_both(model, iterator, criterion, clip):\n",
    "    assert isinstance(model, Ensembler)\n",
    "    \n",
    "    model.eval()\n",
    "    model.experts[model.n_active_experts - 1].train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        \n",
    "        # model.gating_optimizer.zero_grad()\n",
    "        model.expert_optimizers[model.n_active_experts - 1].zero_grad()\n",
    "        \n",
    "        outputs = model(seqs, seqs_len, seqs)\n",
    "        \n",
    "        loss = compute_loss(outputs, seqs, criterion, cutFirstInSequence=True)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # model.gating_optimizer.step()\n",
    "        model.expert_optimizers[model.n_active_experts - 1].step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensembler_expert(model, iterator, train_id, criterion, clip):\n",
    "    assert isinstance(model, Ensembler)\n",
    "    \n",
    "    model.experts[train_id].train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        \n",
    "        # model.gating_optimizer.zero_grad()\n",
    "        model.expert_optimizers[train_id].zero_grad()\n",
    "        \n",
    "        outputs = model.experts[train_id](seqs, seqs_len, seqs)\n",
    "        \n",
    "        loss = compute_loss(outputs, seqs, criterion, cutFirstInSequence=True)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        model.expert_optimizers[train_id].step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensembler(model, optimizer, iterator, criterion, clip, verbose=False):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(seqs, seqs_len, seqs, verbose=verbose)\n",
    "        \n",
    "        loss = compute_loss(outputs, seqs, criterion, cutFirstInSequence=True)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_ensembler(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if \"gating\" in name:\n",
    "            if \"weight\" in name:\n",
    "                nn.init.uniform_(param.data, a=0.0, b=1.0)\n",
    "            else:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "        else:\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "            else:\n",
    "                nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)\n",
    "\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembler(\n",
      "  (gating): Gating_Ensembler(\n",
      "    (embedding): Embedding(8, 9)\n",
      "    (rnn): GRU(9, 15, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=30, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(19, 9, bidirectional=True)\n",
      "        (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "          (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(37, 9)\n",
      "        (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "model = Ensembler()\n",
    "m_optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "print(model.apply(init_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_backup = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_expert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.508172423811629e-06, -0.00041902376688085496, -0.00015881884610280395, -2.4202134227380157e-05, 0.0003325603320263326, 0.000978443888016045, -0.0004651243216358125, 0.0009709399309940636], [-0.0001899793860502541, -0.000222160539124161, 0.00027672317810356617, -0.0001702979498077184, 0.00025136719341389835, 3.7281206459738314e-06, 1.7927406588569283e-05, 0.0007821590406820178], [-0.00021046228357590735, 0.00024208836839534342, 0.0005469521274790168, 0.000171741092344746, 0.0004685086023528129, 0.0005818973295390606, -0.00011901764082722366, -2.6262918254360557e-05], [-0.00018430400814395398, -0.00022479146718978882, 0.00027763901744037867, -0.00017314677825197577, 0.0002612153475638479, 4.811336111743003e-06, 2.1315645426511765e-05, 0.0007766126655042171], [-0.00020764520741067827, 0.00024078571004793048, 0.0005473484634421766, 0.0001703549933154136, 0.0004734025860670954, 0.0005824442487210035, -0.00011734259896911681, -2.9060785891488194e-05], [-0.00018290600564796478, -0.00022543626255355775, 0.0002778064226731658, -0.00017382139049004763, 0.0002636473800521344, 5.087240424472839e-06, 2.214417327195406e-05, 0.000775201537180692], [-0.0002069516049232334, 0.00024046641192398965, 0.0005474168574437499, 0.00017002702224999666, 0.00047461106441915035, 0.0005825837142765522, -0.00011693310807459056, -2.97726655844599e-05], [-0.00018256196926813573, -0.00022559429635293782, 0.00027783302357420325, -0.000173980908584781, 0.0002642478502821177, 5.157671694178134e-06, 2.2346677724272013e-05, 0.0007748423959128559]]\n",
      "tensor([[7],\n",
      "        [3],\n",
      "        [6],\n",
      "        [5],\n",
      "        [4],\n",
      "        [6],\n",
      "        [5],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "for s, sl in train_dls[0]:\n",
    "    print((model.experts[1](s, sl, s, 0)[1:])[:,0].tolist())\n",
    "    print(s[1:])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gating_before_relu tensor([[ 1.3537, -0.9929,  0.0215]])\n",
      "gating_masked: tensor([[0.7947, 0.2703]])\n",
      "expert_loss tensor([[0.0929],\n",
      "        [0.6914]])\n",
      "pred = [7, 3, 6, 5, 4, 6, 5, 2] \n",
      "trg  = [7, 3, 6, 5, 4, 6, 5, 2]\n",
      "-\n",
      "gating_before_relu tensor([[ 1.5565, -1.1279,  0.0224]])\n",
      "gating_masked: tensor([[0.8259, 0.2445]])\n",
      "expert_loss tensor([[0.1154],\n",
      "        [0.5721]])\n",
      "pred = [7, 6, 5, 4, 2] \n",
      "trg  = [7, 6, 5, 4, 2]\n",
      "-\n",
      "gating_before_relu tensor([[ 1.1979, -0.8526,  0.0144]])\n",
      "gating_masked: tensor([[0.7682, 0.2989]])\n",
      "expert_loss tensor([[0.1415],\n",
      "        [0.7368]])\n",
      "pred = [7, 6, 4, 2] \n",
      "trg  = [7, 6, 4, 2]\n",
      "-\n",
      "gating_before_relu tensor([[ 1.5534, -1.1237,  0.0221]])\n",
      "gating_masked: tensor([[0.8254, 0.2453]])\n",
      "expert_loss tensor([[0.1328],\n",
      "        [0.5809]])\n",
      "pred = [7, 3, 6, 4, 2] \n",
      "trg  = [7, 3, 6, 4, 2]\n",
      "-\n",
      "gating_before_relu tensor([[ 1.8999, -1.2998,  0.0283]])\n",
      "gating_masked: tensor([[0.8699, 0.2142]])\n",
      "expert_loss tensor([[0.1045],\n",
      "        [0.6851]])\n",
      "pred = [7, 6, 4, 6, 5, 2] \n",
      "trg  = [7, 6, 4, 6, 5, 2]\n",
      "-\n",
      "gating_before_relu tensor([[ 1.0491, -0.8141,  0.0179]])\n",
      "gating_masked: tensor([[0.7406, 0.3070]])\n",
      "expert_loss tensor([[0.1153],\n",
      "        [0.6239]])\n",
      "pred = [7, 3, 6, 4, 6, 2] \n",
      "trg  = [7, 3, 6, 4, 6, 2]\n",
      "-\n",
      "gating_before_relu tensor([[ 1.5244, -1.0906,  0.0235]])\n",
      "gating_masked: tensor([[0.8212, 0.2515]])\n",
      "expert_loss tensor([[0.0969],\n",
      "        [0.5304]])\n",
      "pred = [7, 3, 6, 4, 6, 5, 2] \n",
      "trg  = [7, 3, 6, 4, 6, 5, 2]\n",
      "-\n",
      "gating_before_relu tensor([[ 1.4568, -1.0521,  0.0227]])\n",
      "gating_masked: tensor([[0.8110, 0.2588]])\n",
      "expert_loss tensor([[0.0989],\n",
      "        [0.4440]])\n",
      "pred = [7, 6, 5, 4, 6, 5, 2] \n",
      "trg  = [7, 6, 5, 4, 6, 5, 2]\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "visual_eval(model, train_dls[0], cutEndToken=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Acc 1.0% | Gr acc 1.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "train up, test down\n",
      "Task 0: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy(model, train_dls)\n",
    "print(\"train up, test down\")\n",
    "accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Acc 1.0% | Gr acc 1.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy(model.experts[0], train_dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gating_before_relu tensor([[-2.8803,  2.8071, -0.0069]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0531, 0.9431]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2834],\n",
      "        [0.3674]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-3.2180,  3.2752, -0.0049]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0385, 0.9636]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2701],\n",
      "        [0.2669]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.9362,  2.9575, -0.0040]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0504, 0.9506]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.6912],\n",
      "        [0.7696]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.8273,  2.8278, -0.0048]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0559, 0.9442]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.7673],\n",
      "        [0.5674]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.8531,  3.0069, -0.0062]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0545, 0.9529]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.5668],\n",
      "        [0.6961]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.5374,  2.6025, -0.0072]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0733, 0.9310]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.5885],\n",
      "        [0.6965]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.7467,  3.0782, -0.0073]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0603, 0.9560]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.3908],\n",
      "        [0.6033]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.3426,  2.6440, -0.0047]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0877, 0.9336]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.8733],\n",
      "        [0.7101]], grad_fn=<CopySlices>)\n",
      "0.5682560727000237\n",
      "gating_before_relu tensor([[-2.1851,  2.5655, -0.0063]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.1011, 0.9286]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.3635],\n",
      "        [0.7295]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.2343,  2.8584, -0.0033]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0967, 0.9457]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2419],\n",
      "        [0.4849]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.0045,  2.7368, -0.0030]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.1187, 0.9392]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.5218],\n",
      "        [0.9676]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-1.6925,  2.2450, -0.0040]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.1554, 0.9042]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.6696],\n",
      "        [0.5721]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-1.4309,  2.3166, -0.0041]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.1930, 0.9102]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2925],\n",
      "        [0.2803]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-1.2046,  2.3297, -0.0037]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.2307, 0.9113]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.3677],\n",
      "        [0.6012]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-1.1347e+00,  2.3919e+00, -1.4505e-04]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.2433, 0.9162]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2100],\n",
      "        [0.4992]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-8.8122e-01,  2.3645e+00, -1.7280e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.2929, 0.9141]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1890],\n",
      "        [0.6425]], grad_fn=<CopySlices>)\n",
      "0.49113180488348007\n",
      "gating_before_relu tensor([[-7.2881e-01,  2.0784e+00, -3.2354e-04]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.3255, 0.8888]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1772],\n",
      "        [0.3921]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-3.4440e-01,  2.0593e+00, -1.1747e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.4147, 0.8869]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.3006],\n",
      "        [0.7049]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.5110e-01,  2.0951e+00,  1.6691e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.4376, 0.8904]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1997],\n",
      "        [0.6493]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-6.0284e-03,  1.9817e+00,  7.2929e-04]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.4985, 0.8789]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1957],\n",
      "        [0.4550]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 0.3568,  1.9462, -0.0021]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.5883, 0.8750]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.4015],\n",
      "        [0.9136]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[3.4678e-01, 1.8087e+00, 3.8315e-04]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.5858, 0.8592]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.4377],\n",
      "        [0.6772]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 1.0478e+00,  2.1423e+00, -1.1698e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.7404, 0.8949]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1748],\n",
      "        [0.8947]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[9.5378e-01, 1.7798e+00, 5.7311e-04]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.7219, 0.8557]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1809],\n",
      "        [0.3087]], grad_fn=<CopySlices>)\n",
      "0.3428385481238365\n",
      "gating_before_relu tensor([[0.8217, 1.1666, 0.0035]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.6946, 0.7625]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1888],\n",
      "        [0.7070]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 2.1702,  1.6768, -0.0039]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.8975, 0.8425]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1728],\n",
      "        [0.3039]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 2.0957e+00,  1.7332e+00, -1.1982e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.8905, 0.8498]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1313],\n",
      "        [0.6689]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 2.3055,  1.1178, -0.0044]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9093, 0.7536]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2625],\n",
      "        [0.7401]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.1345,  0.9742, -0.0079]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9583, 0.7260]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1709],\n",
      "        [0.4048]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.5878,  1.2512, -0.0051]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9731, 0.7775]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1900],\n",
      "        [0.7602]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.4201,  1.1684, -0.0055]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9683, 0.7629]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1385],\n",
      "        [0.3242]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.0775,  0.7181, -0.0045]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9560, 0.6722]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2430],\n",
      "        [0.6304]], grad_fn=<CopySlices>)\n",
      "0.20110375434160233\n",
      "gating_before_relu tensor([[ 2.9470e+00,  6.3874e-01, -9.4138e-04]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9501, 0.6545]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1353],\n",
      "        [0.4221]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.5710,  0.3203, -0.0037]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9726, 0.5794]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1691],\n",
      "        [0.3096]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.3134,  0.1256, -0.0105]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9868, 0.5314]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1652],\n",
      "        [0.7693]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.3010e+00,  3.2702e-01, -2.3065e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9645, 0.5810]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1697],\n",
      "        [0.5888]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.3775,  0.4142, -0.0085]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9876, 0.6021]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1397],\n",
      "        [0.7431]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.9318,  0.1616, -0.0052]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9808, 0.5403]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1870],\n",
      "        [0.2777]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.1598, -0.5044, -0.0099]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9846, 0.3765]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1601],\n",
      "        [0.4203]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.7081,  0.1388, -0.0099]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9911, 0.5347]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1650],\n",
      "        [0.6726]], grad_fn=<CopySlices>)\n",
      "0.15688258409500122\n",
      "gating_before_relu tensor([[ 3.9808, -0.4242, -0.0054]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9817, 0.3955]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1476],\n",
      "        [0.8689]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.7996, -0.3806, -0.0100]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9918, 0.4060]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.3116],\n",
      "        [0.6119]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.3954, -0.2169, -0.0049]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9878, 0.4460]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1521],\n",
      "        [0.2826]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.8863, -0.6735, -0.0134]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9972, 0.3377]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1286],\n",
      "        [0.6042]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.3624, -0.9349, -0.0179]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9953, 0.2819]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1181],\n",
      "        [0.6653]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.0272e+00, -4.3498e-01,  5.3743e-04]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9538, 0.3929]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1571],\n",
      "        [0.4951]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.7524, -0.5961, -0.0136]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9968, 0.3552]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1181],\n",
      "        [0.3581]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8598, -0.9543, -0.0142]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9923, 0.2780]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1611],\n",
      "        [0.8116]], grad_fn=<CopySlices>)\n",
      "0.15238194167613983\n",
      "gating_before_relu tensor([[ 4.8372, -0.6463, -0.0067]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9921, 0.3438]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1139],\n",
      "        [0.3070]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8928, -1.0277, -0.0136]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9926, 0.2635]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1647],\n",
      "        [0.7107]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4027, -0.8453, -0.0114]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9955, 0.3004]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1281],\n",
      "        [0.9591]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.9422, -0.6725, -0.0117]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9929, 0.3379]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1116],\n",
      "        [0.7515]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.7669, -0.9021, -0.0152]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9969, 0.2886]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1185],\n",
      "        [0.4883]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.4282, -0.9574, -0.0076]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9882, 0.2774]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1017],\n",
      "        [0.6672]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.2421e+00, -5.0256e-01, -1.4962e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9858, 0.3769]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1003],\n",
      "        [0.3662]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4785, -0.2970, -0.0087]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9958, 0.4263]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1804],\n",
      "        [0.5933]], grad_fn=<CopySlices>)\n",
      "0.12723933160305023\n",
      "gating_before_relu tensor([[ 5.6101, -0.8207, -0.0106]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9964, 0.3056]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1303],\n",
      "        [0.3773]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.7864, -0.9902, -0.0122]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9969, 0.2709]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0987],\n",
      "        [0.5954]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.7247, -0.7913, -0.0048]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9912, 0.3119]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1319],\n",
      "        [0.9183]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.5325, -1.1846, -0.0119]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9961, 0.2342]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1119],\n",
      "        [0.7143]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.6682e+00, -1.2028e-01,  1.9920e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9751, 0.4700]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1387],\n",
      "        [0.5548]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4066, -1.2279, -0.0120]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9955, 0.2266]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1097],\n",
      "        [0.3920]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.2503, -1.3537, -0.0136]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9948, 0.2053]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1052],\n",
      "        [0.3631]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.0512, -1.6280, -0.0186]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9977, 0.1641]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1008],\n",
      "        [0.6289]], grad_fn=<CopySlices>)\n",
      "0.11735443025827408\n",
      "gating_before_relu tensor([[ 5.5197, -0.3026, -0.0103]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9960, 0.4249]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0966],\n",
      "        [0.9131]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8691, -1.6335, -0.0122]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9924, 0.1634]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1143],\n",
      "        [0.3652]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.4047e+00, -1.2325e+00, -2.7984e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9679, 0.2257]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1979],\n",
      "        [0.5706]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.1269, -1.9127, -0.0204]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9978, 0.1287]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1021],\n",
      "        [0.3284]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.7837, -1.4804, -0.0095]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9917, 0.1854]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0962],\n",
      "        [0.7189]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.0208, -1.8715, -0.0168]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9976, 0.1334]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0959],\n",
      "        [0.5846]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8713, -1.6144, -0.0086]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9924, 0.1660]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1147],\n",
      "        [0.9038]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.6091, -1.7570, -0.0149]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9963, 0.1472]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1447],\n",
      "        [0.8630]], grad_fn=<CopySlices>)\n",
      "0.11937637627124786\n",
      "gating_before_relu tensor([[ 4.0150e+00, -4.3933e-01, -2.8834e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9823, 0.3919]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0978],\n",
      "        [0.2730]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.3942, -1.9291, -0.0132]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9955, 0.1269]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1020],\n",
      "        [0.3879]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8935, -1.9420, -0.0107]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9926, 0.1254]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1240],\n",
      "        [0.4038]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.4006, -1.8298, -0.0096]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9879, 0.1383]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1015],\n",
      "        [0.4522]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.6472, -2.1136, -0.0205]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9965, 0.1078]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1025],\n",
      "        [0.7544]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.6900, -2.1106, -0.0170]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9966, 0.1081]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0904],\n",
      "        [0.6723]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4903, -2.0535, -0.0177]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9959, 0.1137]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1273],\n",
      "        [0.4805]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.6204, -1.7929, -0.0150]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9964, 0.1427]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1520],\n",
      "        [0.3475]], grad_fn=<CopySlices>)\n",
      "0.10887506604194641\n",
      "gating_before_relu tensor([[ 4.8902, -1.9732, -0.0092]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9925, 0.1220]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1177],\n",
      "        [0.6760]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.6659, -1.9476, -0.0141]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9907, 0.1248]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0952],\n",
      "        [0.7221]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.3869, -1.9410, -0.0168]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9983, 0.1255]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0944],\n",
      "        [0.3171]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.5905, -2.0551, -0.0181]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9963, 0.1135]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2283],\n",
      "        [0.7493]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.3466, -1.9887, -0.0140]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9953, 0.1204]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1099],\n",
      "        [0.5703]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.2610, -2.0704, -0.0171]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9981, 0.1120]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1042],\n",
      "        [0.4146]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8683, -1.8911, -0.0120]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9924, 0.1311]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1410],\n",
      "        [0.5482]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8821, -1.9857, -0.0132]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9925, 0.1207]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0938],\n",
      "        [0.3281]], grad_fn=<CopySlices>)\n",
      "0.12340371310710907\n",
      "gating_before_relu tensor([[ 5.7049, -2.0258, -0.0140]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9967, 0.1165]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1027],\n",
      "        [0.3901]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.9093, -2.1354, -0.0185]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9973, 0.1057]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2103],\n",
      "        [0.6216]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8734, -1.7630, -0.0104]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9924, 0.1464]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1038],\n",
      "        [0.4297]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.2601, -1.7241, -0.0071]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9861, 0.1513]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1120],\n",
      "        [0.7103]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4747, -1.6799, -0.0117]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9958, 0.1571]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0970],\n",
      "        [0.2786]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.8363, -2.0369, -0.0151]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9971, 0.1154]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1545],\n",
      "        [0.6722]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.1577, -1.3526, -0.0069]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9846, 0.2054]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1387],\n",
      "        [0.5406]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.8899, -2.1616, -0.0137]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9972, 0.1032]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1254],\n",
      "        [0.4305]], grad_fn=<CopySlices>)\n",
      "0.1300085484981537\n",
      "gating_before_relu tensor([[ 5.8122, -2.1787, -0.0175]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9970, 0.1017]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1275],\n",
      "        [0.5175]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.5104e+00, -4.8284e-01, -3.6801e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9891, 0.3816]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1475],\n",
      "        [0.2445]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.9104, -2.1110, -0.0147]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9973, 0.1080]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0964],\n",
      "        [0.7215]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.4744e+00, -9.7075e-01, -4.4670e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9887, 0.2747]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1047],\n",
      "        [0.3222]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4580, -2.0903, -0.0115]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9958, 0.1100]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0833],\n",
      "        [0.3798]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.9202e+00, -9.4392e-01, -3.2549e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9805, 0.2801]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1249],\n",
      "        [0.7018]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.5679, -2.0358, -0.0158]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9962, 0.1155]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1007],\n",
      "        [0.6918]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.6233, -2.0834, -0.0181]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9964, 0.1107]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1096],\n",
      "        [0.3725]], grad_fn=<CopySlices>)\n",
      "0.10891195386648178\n",
      "gating_before_relu tensor([[ 6.3783, -2.1003, -0.0169]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9983, 0.1091]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1078],\n",
      "        [0.4325]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.5472, -2.2130, -0.0178]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9986, 0.0986]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1276],\n",
      "        [0.4442]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.5784, -1.7973, -0.0090]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9898, 0.1422]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1040],\n",
      "        [0.6390]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.2876, -2.0597, -0.0134]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9950, 0.1131]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0948],\n",
      "        [0.5889]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8707, -2.0082, -0.0111]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9924, 0.1183]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1200],\n",
      "        [0.7572]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.5865, -2.1226, -0.0154]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9963, 0.1069]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1118],\n",
      "        [0.4081]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.8871, -1.9478, -0.0142]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9972, 0.1248]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1030],\n",
      "        [0.7047]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.1900, -1.9555, -0.0112]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9945, 0.1240]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1045],\n",
      "        [0.8730]], grad_fn=<CopySlices>)\n",
      "0.10898962616920471\n",
      "gating_before_relu tensor([[ 5.6283, -2.1225, -0.0146]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9964, 0.1069]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0902],\n",
      "        [0.4069]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.9432, -1.5345, -0.0062]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9810, 0.1773]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1230],\n",
      "        [0.5389]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.5266, -2.1160, -0.0140]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9960, 0.1076]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1065],\n",
      "        [0.4408]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.3798, -2.0649, -0.0162]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9983, 0.1126]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1131],\n",
      "        [0.5456]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.4678, -1.7749, -0.0055]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9698, 0.1449]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0847],\n",
      "        [0.6666]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.6865, -2.0792, -0.0135]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9966, 0.1111]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0884],\n",
      "        [0.8131]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.2607, -1.2293, -0.0084]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9948, 0.2263]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1252],\n",
      "        [0.4275]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.3414, -1.9268, -0.0088]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9952, 0.1271]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1059],\n",
      "        [0.3135]], grad_fn=<CopySlices>)\n",
      "0.10481567680835724\n",
      "gating_before_relu tensor([[ 5.3203, -2.1454, -0.0128]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9951, 0.1048]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1178],\n",
      "        [0.4481]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.1691, -2.0340, -0.0140]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9943, 0.1157]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0736],\n",
      "        [0.6726]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.7277, -2.0166, -0.0057]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9912, 0.1175]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1222],\n",
      "        [0.6592]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4268, -2.0541, -0.0119]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9956, 0.1136]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0912],\n",
      "        [0.7418]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.2668, -2.1001, -0.0119]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9949, 0.1091]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1270],\n",
      "        [0.6202]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.2770e+00, -1.3069e+00, -2.4708e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9863, 0.2130]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1051],\n",
      "        [0.2654]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.3183, -2.4359, -0.0176]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9982, 0.0805]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0928],\n",
      "        [0.3757]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.3225, -1.9076, -0.0163]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9982, 0.1293]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1054],\n",
      "        [0.4984]], grad_fn=<CopySlices>)\n",
      "0.10430794209241867\n",
      "gating_before_relu tensor([[ 5.9653, -2.3388, -0.0162]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9974, 0.0880]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1504],\n",
      "        [0.7407]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.0999, -1.9738, -0.0090]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9939, 0.1220]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1034],\n",
      "        [0.4247]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.5672, -2.4973, -0.0168]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9986, 0.0760]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1071],\n",
      "        [0.3651]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.7082, -2.1570, -0.0163]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9967, 0.1037]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0896],\n",
      "        [0.7222]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.4118, -2.3532, -0.0174]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9984, 0.0868]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0988],\n",
      "        [0.6141]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.1170, -2.1155, -0.0146]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9978, 0.1076]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1202],\n",
      "        [0.5060]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.9597, -1.6855, -0.0085]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9930, 0.1564]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0955],\n",
      "        [0.6996]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.9963, -2.2361, -0.0116]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9933, 0.0966]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0976],\n",
      "        [0.3798]], grad_fn=<CopySlices>)\n",
      "0.10654933005571365\n",
      "gating_before_relu tensor([[ 5.4211, -2.2857, -0.0163]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9956, 0.0923]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0853],\n",
      "        [0.3621]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.5705, -2.2467, -0.0108]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9962, 0.0956]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1183],\n",
      "        [0.5434]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.6544, -2.2266, -0.0133]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9965, 0.0974]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1173],\n",
      "        [0.5849]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.7937, -2.3670, -0.0160]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9970, 0.0857]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0851],\n",
      "        [0.6538]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4660, -2.1800, -0.0112]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9958, 0.1016]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0870],\n",
      "        [0.2786]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.9553, -2.3741, -0.0130]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9974, 0.0852]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1050],\n",
      "        [0.5791]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.6036, -1.9152, -0.0056]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9735, 0.1284]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0860],\n",
      "        [0.6645]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4579, -2.1051, -0.0134]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9958, 0.1086]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0969],\n",
      "        [0.4386]], grad_fn=<CopySlices>)\n",
      "0.09807850420475006\n",
      "gating_before_relu tensor([[ 6.0137, -2.4012, -0.0148]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9976, 0.0831]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0972],\n",
      "        [0.7445]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.4148, -2.2248, -0.0165]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9984, 0.0975]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0858],\n",
      "        [0.2812]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.9518, -2.1956, -0.0098]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9930, 0.1001]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1018],\n",
      "        [0.4027]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.0322, -2.4296, -0.0160]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9976, 0.0809]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0989],\n",
      "        [0.2928]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.7349, -2.3788, -0.0132]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9968, 0.0848]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1142],\n",
      "        [0.6184]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.4275, -2.4694, -0.0169]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9984, 0.0780]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0976],\n",
      "        [0.4950]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.9684, -2.1594, -0.0065]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9814, 0.1035]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0764],\n",
      "        [0.5498]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.9642, -2.5048, -0.0137]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9974, 0.0755]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1127],\n",
      "        [0.3727]], grad_fn=<CopySlices>)\n",
      "0.09878010302782059\n",
      "gating_before_relu tensor([[ 5.4276, -2.5226, -0.0157]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9956, 0.0743]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1060],\n",
      "        [0.7565]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.9657, -2.1877, -0.0081]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9814, 0.1009]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0753],\n",
      "        [0.3372]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.0549, -2.5996, -0.0157]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9977, 0.0692]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0896],\n",
      "        [0.4811]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.6823, -2.3437, -0.0084]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9908, 0.0876]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1269],\n",
      "        [0.5763]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.9247, -2.5884, -0.0149]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9973, 0.0699]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0896],\n",
      "        [0.7571]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.2451, -2.1245, -0.0094]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9948, 0.1067]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1059],\n",
      "        [0.5705]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.7418, -2.0439, -0.0118]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9968, 0.1147]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0891],\n",
      "        [0.4872]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.2929, -2.6697, -0.0149]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9982, 0.0648]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1104],\n",
      "        [0.4871]], grad_fn=<CopySlices>)\n",
      "0.09925767779350281\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    print(train_ensembler(model, m_optimizer, train_dls[0], criterion, clip=1, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6819222569465637\n",
      "0.5159699842333794\n",
      "0.4478273540735245\n",
      "0.44442009925842285\n",
      "0.4017461687326431\n",
      "0.4108700156211853\n",
      "0.36904872953891754\n",
      "0.3939468562602997\n",
      "0.380849227309227\n",
      "0.3801833763718605\n",
      "0.32542800158262253\n",
      "0.3442196324467659\n",
      "0.2935721278190613\n",
      "0.35802025347948074\n",
      "0.3127453699707985\n",
      "0.35975559800863266\n",
      "0.3624192997813225\n",
      "0.3340729847550392\n",
      "0.3143194168806076\n",
      "0.31856175512075424\n",
      "0.290366567671299\n",
      "0.33430249243974686\n",
      "0.3475809320807457\n",
      "0.35768380761146545\n",
      "0.32783298194408417\n",
      "0.3088560923933983\n",
      "0.27582738548517227\n",
      "0.2882274463772774\n",
      "0.3500097244977951\n",
      "0.27082621306180954\n",
      "0.2905246168375015\n",
      "0.2647992670536041\n",
      "0.25673429667949677\n",
      "0.2832217290997505\n",
      "0.25846124440431595\n",
      "0.2937106564640999\n",
      "0.24560310691595078\n",
      "0.21725226193666458\n",
      "0.2346200793981552\n",
      "0.24897794425487518\n",
      "0.22636029869318008\n",
      "0.2215658202767372\n",
      "0.21642332524061203\n",
      "0.22373760491609573\n",
      "0.20710919052362442\n",
      "0.2318643182516098\n",
      "0.29033371806144714\n",
      "0.2234899252653122\n",
      "0.21376769244670868\n",
      "0.20557651668787003\n",
      "0.2603313699364662\n",
      "0.25770843774080276\n",
      "0.19829197227954865\n",
      "0.19657956063747406\n",
      "0.23009366542100906\n",
      "0.23028745502233505\n",
      "0.19156234711408615\n",
      "0.194068543612957\n",
      "0.24114757031202316\n",
      "0.18336236476898193\n",
      "0.20552147924900055\n",
      "0.18832410871982574\n",
      "0.24301724135875702\n",
      "0.18124248832464218\n",
      "0.17453563958406448\n",
      "0.2280053123831749\n",
      "0.21842745691537857\n",
      "0.17402775585651398\n",
      "0.18187490850687027\n",
      "0.21305087953805923\n",
      "0.19077467173337936\n",
      "0.2044442892074585\n",
      "0.1522967591881752\n",
      "0.1890232190489769\n",
      "0.14997000992298126\n",
      "0.14673472940921783\n",
      "0.17841757833957672\n",
      "0.17327819764614105\n",
      "0.18703081458806992\n",
      "0.18066978454589844\n",
      "0.2540165185928345\n",
      "0.19356969743967056\n",
      "0.15471993386745453\n",
      "0.189193956553936\n",
      "0.17900065332651138\n",
      "0.15990224480628967\n",
      "0.1817990243434906\n",
      "0.1485789194703102\n",
      "0.15646616369485855\n",
      "0.15941256284713745\n",
      "0.14891498535871506\n",
      "0.13769111782312393\n",
      "0.15419796854257584\n",
      "0.19168980419635773\n",
      "0.13267996162176132\n",
      "0.15432817488908768\n",
      "0.13190093636512756\n",
      "0.17842616140842438\n",
      "0.1523570716381073\n",
      "0.22105004638433456\n",
      "0.17641431838274002\n",
      "0.1648062989115715\n",
      "0.16122521460056305\n",
      "0.1476520374417305\n",
      "0.12822704762220383\n",
      "0.12085522711277008\n",
      "0.15077093988656998\n",
      "0.12596354633569717\n",
      "0.12806012481451035\n",
      "0.12584073841571808\n",
      "0.11973564326763153\n",
      "0.13707377016544342\n",
      "0.12780658155679703\n",
      "0.12644672393798828\n",
      "0.15050291270017624\n",
      "0.13269007205963135\n",
      "0.1205056682229042\n",
      "0.1421618089079857\n",
      "0.11885038018226624\n",
      "0.13633455336093903\n",
      "0.12476804107427597\n",
      "0.17574070394039154\n",
      "0.1280074566602707\n",
      "0.1590150147676468\n",
      "0.11913776397705078\n",
      "0.11645939946174622\n",
      "0.14410822093486786\n",
      "0.12144742161035538\n",
      "0.11631747335195541\n",
      "0.11219434440135956\n"
     ]
    }
   ],
   "source": [
    "train_id = 0\n",
    "for _ in range(130):\n",
    "    print(train_ensembler_expert(model, train_dls[0], train_id, criterion, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_expert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6047074869275093\n",
      "0.4525083675980568\n",
      "0.41188517212867737\n",
      "0.3863498419523239\n",
      "0.40757525712251663\n",
      "0.37751537561416626\n",
      "0.38592784851789474\n",
      "0.35213592648506165\n",
      "0.347373366355896\n",
      "0.33041082322597504\n",
      "0.33090417832136154\n",
      "0.3478125035762787\n",
      "0.30645691603422165\n",
      "0.3227307200431824\n",
      "0.2872186452150345\n",
      "0.2924410551786423\n",
      "0.3570890426635742\n",
      "0.31722015142440796\n",
      "0.3560579642653465\n",
      "0.3351268768310547\n",
      "0.3220672607421875\n",
      "0.2739083617925644\n",
      "0.2896508276462555\n",
      "0.35112224519252777\n",
      "0.29314666241407394\n",
      "0.30075741559267044\n",
      "0.28139810264110565\n",
      "0.2736208587884903\n",
      "0.2606027275323868\n",
      "0.3058577924966812\n",
      "0.26708005368709564\n",
      "0.2969105839729309\n",
      "0.3008725270628929\n",
      "0.24462435394525528\n",
      "0.2775457799434662\n",
      "0.27789808064699173\n",
      "0.32739755511283875\n",
      "0.2805725708603859\n",
      "0.2496674880385399\n",
      "0.2798337861895561\n",
      "0.2792147099971771\n",
      "0.28302282840013504\n",
      "0.30447782576084137\n",
      "0.25157085061073303\n",
      "0.23309019953012466\n",
      "0.2473481521010399\n",
      "0.24102695286273956\n",
      "0.2451634630560875\n",
      "0.2204611748456955\n",
      "0.2948429808020592\n",
      "0.23440247029066086\n",
      "0.24506614357233047\n",
      "0.2135242521762848\n",
      "0.27413423359394073\n",
      "0.2258775606751442\n",
      "0.21091344207525253\n",
      "0.2159041240811348\n",
      "0.21692225337028503\n",
      "0.2871958017349243\n",
      "0.2256965935230255\n",
      "0.21657780557870865\n",
      "0.1938825398683548\n",
      "0.20536161214113235\n",
      "0.1733112782239914\n",
      "0.208919920027256\n",
      "0.1945476233959198\n",
      "0.21343356370925903\n",
      "0.180329829454422\n",
      "0.17605005204677582\n",
      "0.17278148978948593\n",
      "0.1825346052646637\n",
      "0.19215259701013565\n",
      "0.17223940044641495\n",
      "0.186446875333786\n",
      "0.1975068897008896\n",
      "0.167712040245533\n",
      "0.18060285598039627\n",
      "0.17173895984888077\n",
      "0.19365082681179047\n",
      "0.1457042619585991\n",
      "0.1513429880142212\n",
      "0.15268734842538834\n",
      "0.15046487003564835\n",
      "0.1243046224117279\n",
      "0.1859157532453537\n",
      "0.1279527246952057\n",
      "0.1380903273820877\n",
      "0.1375075802206993\n",
      "0.14353765547275543\n",
      "0.15004614740610123\n",
      "0.12130044400691986\n",
      "0.12802797555923462\n",
      "0.1309652030467987\n",
      "0.11807587742805481\n",
      "0.14408136159181595\n",
      "0.13316359370946884\n",
      "0.150610513985157\n",
      "0.1284170299768448\n",
      "0.15144933760166168\n",
      "0.13879839330911636\n",
      "0.11879581958055496\n",
      "0.12150575965642929\n",
      "0.11645175516605377\n",
      "0.1601027473807335\n",
      "0.12986896187067032\n",
      "0.12831342220306396\n",
      "0.13970103859901428\n",
      "0.12347128987312317\n",
      "0.14781255275011063\n",
      "0.11918802559375763\n",
      "0.12662296742200851\n",
      "0.10951507091522217\n",
      "0.10373636335134506\n",
      "0.10222604125738144\n",
      "0.10630770772695541\n",
      "0.09814052283763885\n",
      "0.10288447141647339\n",
      "0.10063834488391876\n",
      "0.10184124857187271\n",
      "0.09595887362957001\n",
      "0.09773872047662735\n",
      "0.10060756653547287\n",
      "0.10395079851150513\n",
      "0.09868147224187851\n",
      "0.10205966234207153\n",
      "0.10005279630422592\n",
      "0.09298700839281082\n",
      "0.0952887088060379\n",
      "0.0946991965174675\n",
      "0.09883996844291687\n"
     ]
    }
   ],
   "source": [
    "train_id = 1\n",
    "for _ in range(130):\n",
    "    print(train_ensembler_expert(model, train_dls[1], train_id, criterion, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 1.0% | Gr acc 1.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy(model.experts[1], train_dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Gating Loss\n",
      "tensor(0.4611, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2177, 0.0018]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.5026, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2379, 0.0298]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.4410, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2804, 0.0831]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.3156, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3703, 0.2269]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.2597, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3285, 0.3424]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.1952, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3177, 0.5010]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2785, 0.8670]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.1595, 0.9603]], grad_fn=<SliceBackward>)\n",
      "loss: 0.29437927156686783\n",
      "\n",
      ">> Gating Loss\n",
      "tensor(0.0666, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0425, 1.4590]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.0928,  1.5980]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0907, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.1921,  1.6714]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0706, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.3137,  2.3131]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0910, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.3090,  2.0946]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.3618,  2.6810]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.3650,  3.1779]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.1171, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.3101,  3.3351]], grad_fn=<SliceBackward>)\n",
      "loss: 0.08999010920524597\n",
      "\n",
      ">> Gating Loss\n",
      "tensor(0.1128, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.2386,  3.5002]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0694, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.2102,  3.4089]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0708, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.1384,  3.5113]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0894, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0050, 4.2805]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.1488,  2.4901]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.0660,  3.0221]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2013, 4.2764]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0632, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2115, 4.2603]], grad_fn=<SliceBackward>)\n",
      "loss: 0.08283793181180954\n",
      "\n",
      ">> Gating Loss\n",
      "tensor(0.0631, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2477, 4.1452]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.1100, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3315, 4.4296]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0884, 3.3466]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.0052,  2.8615]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0670, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0651, 3.1657]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0719, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3593, 4.7786]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3644, 4.4734]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.4588, 5.3684]], grad_fn=<SliceBackward>)\n",
      "loss: 0.08241797983646393\n",
      "\n",
      ">> Gating Loss\n",
      "tensor(0.0809, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3727, 4.9117]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2877, 4.6521]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3911, 5.3859]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0701, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0252, 3.6559]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.0759,  3.1983]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0481, 3.9903]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0611, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.0931,  3.2485]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.1304, 5.0337]], grad_fn=<SliceBackward>)\n",
      "loss: 0.08200984448194504\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(f\"loss: {train_ensembler_gating(model, train_dls[1], criterion, 1, verbose=True)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Acc 1.0% | Gr acc 1.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy(model, train_dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_ensembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ensembler(\n",
    "    n_tasks_total,\n",
    "    model,\n",
    "    task_id,\n",
    "    epochs,\n",
    "    step_size_evaluation,\n",
    "    criterion,\n",
    "    clip,\n",
    "    repetition=None\n",
    "):\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((n_tasks_total, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((n_tasks_total, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    model.allowed_until_check = N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "    \n",
    "    # Train model depending on its status\n",
    "    for epoch in range(epochs):\n",
    "        # First Epoch log performance BEFORE training\n",
    "        if epoch == 0:\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, epoch, criterion)\n",
    "\n",
    "        if model.status == \"train_gating_initialized_expert\":\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_ensembler_gating(model, train_dls[task_id],\n",
    "                                               criterion, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Log hits\n",
    "            loss_tracker[epoch] = evaluate_extra(model, train_dls[task_id], allOrNoneLoss)\n",
    "\n",
    "            # Check for improvement in loss\n",
    "            if epoch > model.allowed_until_check:\n",
    "                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)\n",
    "                    and \n",
    "                    (valid_loss > PERFORMANCE_TRESHHOLD)\n",
    "                ):\n",
    "                    # Case of no improvement:\n",
    "                    # Switch to train the expert and gating\n",
    "\n",
    "                    model.status = \"train_gating_train_expert\"\n",
    "                    model.allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                    print(\"-----------------------------------\")\n",
    "                    print(\"------Switch to training both------\")\n",
    "                    print(\"-----------------------------------\")\n",
    "\n",
    "            \n",
    "        if model.status == \"train_gating_train_expert\":\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_ensembler_both(model, train_dls[task_id],\n",
    "                                               criterion, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "        if model.status == \"train_gating_uninitialized_expert\":\n",
    "            assert len(model.experts) > 0, \"Need at least one expert\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_ensembler_gating(model, train_dls[task_id],\n",
    "                                               criterion, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Log loss\n",
    "            loss_tracker[epoch] = valid_loss\n",
    "\n",
    "            # Check for improvement in loss\n",
    "            if epoch > N_EPOCHS_UNTIL_NEW_EXPERT:\n",
    "                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)\n",
    "                    and \n",
    "                    (valid_loss > PERFORMANCE_TRESHHOLD)\n",
    "                   ):\n",
    "                    # Case of no improvement:\n",
    "                    # Initiate new expert and train gating and new expert on it\n",
    "                    model.add_expert()\n",
    "\n",
    "                    model.status = \"train_gating_initialized_expert\"\n",
    "                    model.allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                    print(\"-----------------------------------\")\n",
    "                    print(\"-----Added Expert-train Gating-----\")\n",
    "                    print(\"-----------------------------------\")\n",
    "\n",
    "            \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % step_size_evaluation == 0:\n",
    "            idx = epoch//step_size_evaluation\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, epoch, criterion)\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if repetition is not None:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | R{repetition} T{task_id}')\n",
    "        else:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | T{task_id}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "        model.epoch += 1\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show_expert2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_expert2(model, iterator):\n",
    "    model.gating.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            seqs, seqs_len = batch\n",
    "\n",
    "            batch_size = seqs.shape[1]\n",
    "\n",
    "            gating_outputs = model.gating(seqs, seqs_len)\n",
    "\n",
    "            gating_masked = gating_outputs[:,:model.n_active_experts]\n",
    "\n",
    "            for b in range(batch_size):\n",
    "                print(f\"{gating_masked[b]} - {seqs[:,b]}\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_ensembler():\n",
    "    # Initialize DynaMoE\n",
    "    model = Ensembler()\n",
    "    print(model.apply(init_weights))\n",
    "    \n",
    "    # Cosine loss works better for small datasets, thus used for experts\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)\n",
    "    model_optimizer = optim.Adam(model2.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    return (model, criterion, model_otpimizer)\n",
    "\n",
    "def repeat_ensembler(n_tasks_total, n_task_epochs, task_id, step_size_evaluation, repetition, pass_on_variables):\n",
    "    model, criterion, model_optimizer = pass_on_variables\n",
    "    hist_loss, hist_hits = fit_ensembler(\n",
    "        n_tasks_total,\n",
    "        model,\n",
    "        task_id,\n",
    "        n_task_epochs,\n",
    "        step_size_evaluation,\n",
    "        criterion,\n",
    "        repetition=repetition\n",
    "    )\n",
    "    return hist_loss, hist_hits, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Ensembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "id": "CvFmPpKQozmz"
   },
   "outputs": [],
   "source": [
    "N_EXPERTS_START = 1\n",
    "N_MAX_EXPERTS = 3\n",
    "GATE_DROPOUT = 0.5\n",
    "N_GATING_HIDDEN_DIM = 10\n",
    "N_GATING_EMBED_DIM = 10\n",
    "SCHEDULE = not_interleaved\n",
    "\n",
    "# If the amount of missed hits is bigger then PERFORMANCE_TRESHHOLD\n",
    "# and it stays within ALLOWED_ERROR_VARIANCE for\n",
    "# N_EPOCHS_UNTIL_NEW_EXPERT epochs, then a new\n",
    "# expert is initialized\n",
    "N_EPOCHS_UNTIL_NEW_EXPERT = 30\n",
    "ALLOWED_ERROR_VARIANCE = 0.1\n",
    "PERFORMANCE_TRESHHOLD = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "id": "dUUt4knHYfDj"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRoGUVZcfgMg",
    "outputId": "2c57ceea-2add-4c03-f0b3-5a9252f522d0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@ Repetition   0 @@@@@@@@@\n",
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(19, 9, bidirectional=True)\n",
      "        (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "          (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(37, 9)\n",
      "        (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: Ensembler-1.s0.t0.e400\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.143 | Train PPL:   3.137\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.186 | Train PPL:   3.274\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.101 | Train PPL:   3.008\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.165 | Train PPL:   3.206\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.096 | Train PPL:   2.992\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.090 | Train PPL:   2.973\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.106 | Train PPL:   3.023\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.175 | Train PPL:   3.239\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.066 | Train PPL:   2.905\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.110 | Train PPL:   3.036\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.082 | Train PPL:   2.951\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.956\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.129 | Train PPL:   3.091\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.124 | Train PPL:   3.079\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.100 | Train PPL:   3.005\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.139 | Train PPL:   3.125\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.144 | Train PPL:   3.138\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.143 | Train PPL:   3.137\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.121 | Train PPL:   3.067\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.163 | Train PPL:   3.200\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.133 | Train PPL:   3.104\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.071 | Train PPL:   2.918\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.113 | Train PPL:   3.043\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.073 | Train PPL:   2.923\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.111 | Train PPL:   3.036\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.066 | Train PPL:   2.903\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.073 | Train PPL:   2.925\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.070 | Train PPL:   2.916\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.115 | Train PPL:   3.050\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.080 | Train PPL:   2.944\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.120 | Train PPL:   3.064\n",
      "\t Val. Loss: 1.137 |  Val. PPL:   3.118\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.678 | Train PPL:   1.970\n",
      "\t Val. Loss: 0.531 |  Val. PPL:   1.701\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.483 | Train PPL:   1.622\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.544\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.434 | Train PPL:   1.543\n",
      "\t Val. Loss: 0.414 |  Val. PPL:   1.513\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.475\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.500\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.404 |  Val. PPL:   1.498\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.490\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.482\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.482\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.515\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.506\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.370 | Train PPL:   1.448\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.478\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.472\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.416\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.468\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.451\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.399\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.383\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.316\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.317\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.247\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.182\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 101 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 102 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 103 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 104 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 105 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 106 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 107 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 108 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 109 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 110 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.125\n",
      "Epoch: 111 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.164\n",
      "Epoch: 112 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 113 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 114 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 115 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 116 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 117 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 118 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 119 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 120 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 121 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 122 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 123 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 124 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 125 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 126 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 127 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 128 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 129 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 130 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 131 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 132 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 133 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 134 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 135 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 136 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 137 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 138 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 139 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 140 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 141 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 142 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 143 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 144 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 145 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 146 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 147 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 148 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 149 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 150 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 151 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 152 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 153 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 154 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 155 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 156 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 157 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 158 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 159 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 160 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 161 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 162 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 163 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 164 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 165 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 166 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 167 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 168 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 169 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 170 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 171 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 172 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 173 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 174 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 175 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 176 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 177 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 180 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 181 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 182 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 184 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 186 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 187 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 189 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 199 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 200 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 201 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 202 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 203 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 204 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 205 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 206 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 207 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 208 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 209 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 210 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 211 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 212 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 213 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 214 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 215 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 216 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 217 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 218 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 219 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 220 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 221 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 222 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 223 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 224 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 225 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 226 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 227 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 228 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 229 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 230 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 231 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 232 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 233 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 234 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 235 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 236 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 237 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 238 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 239 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 240 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 241 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 242 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 243 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 244 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 245 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 246 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 247 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 248 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 249 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 250 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 251 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 252 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 253 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 254 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 255 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 256 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 257 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 258 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 259 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 260 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 261 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 262 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 263 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 264 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 265 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 266 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 267 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 268 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 269 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 270 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 271 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 272 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 273 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 274 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 275 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 276 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 277 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 278 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 279 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 280 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 281 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 282 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 283 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 284 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 285 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 286 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 287 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 288 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 289 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 290 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 291 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 292 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 293 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 294 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 295 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 296 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 297 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 298 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 299 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 300 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 301 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 302 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 303 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 305 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 308 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 315 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 316 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 317 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 318 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 321 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 326 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 328 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 330 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 331 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 332 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 333 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 335 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 336 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 337 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 340 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 341 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 342 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 343 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 344 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 347 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 348 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 349 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 350 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 352 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 353 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 354 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 355 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 356 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 357 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 358 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 359 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 360 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 361 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 362 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 363 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 364 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 365 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 366 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 367 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 368 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 369 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 370 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 371 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 372 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 373 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 374 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 375 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 376 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 377 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 378 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 379 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 380 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 381 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 382 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 383 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 384 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 385 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 386 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 387 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 388 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 389 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 390 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 391 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 392 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 393 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 394 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 395 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 396 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 397 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 398 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 399 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 400 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "\n",
      "SCHEDULE: Ensembler-1.s1.t1.e400\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.553 | Train PPL:   1.739\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.550 | Train PPL:   1.733\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.556 | Train PPL:   1.744\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.548 | Train PPL:   1.730\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.561 | Train PPL:   1.753\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.724\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.546 | Train PPL:   1.726\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.719\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.555 | Train PPL:   1.742\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.555 | Train PPL:   1.742\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.563 | Train PPL:   1.756\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.544 | Train PPL:   1.723\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.554 | Train PPL:   1.740\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.736\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.566 | Train PPL:   1.761\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.561 | Train PPL:   1.752\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.548 | Train PPL:   1.729\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.547 | Train PPL:   1.729\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.557 | Train PPL:   1.745\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.553 | Train PPL:   1.738\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.718\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.555 | Train PPL:   1.742\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.547 | Train PPL:   1.728\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.556 | Train PPL:   1.744\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.553 | Train PPL:   1.739\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.725\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.552 | Train PPL:   1.737\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.720\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.718\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.735\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "-----------------------------------\n",
      "-----Added Expert-train Gating-----\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.559 | Train PPL:   1.749\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.544 | Train PPL:   1.724\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.543 | Train PPL:   1.721\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.550 | Train PPL:   1.734\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.561 | Train PPL:   1.752\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.554 | Train PPL:   1.740\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.549 | Train PPL:   1.731\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.725\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.558 | Train PPL:   1.748\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.558 | Train PPL:   1.748\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.554 | Train PPL:   1.740\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.725\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.725\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.561 | Train PPL:   1.753\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.547 | Train PPL:   1.728\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.554 | Train PPL:   1.741\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.547 | Train PPL:   1.728\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.543 | Train PPL:   1.720\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.549 | Train PPL:   1.732\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.554 | Train PPL:   1.740\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.560 | Train PPL:   1.751\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.552 | Train PPL:   1.736\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.557 | Train PPL:   1.746\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.566 | Train PPL:   1.762\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.735\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.563 | Train PPL:   1.756\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.560 | Train PPL:   1.751\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.734\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.546 | Train PPL:   1.727\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.560 | Train PPL:   1.751\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.554 | Train PPL:   1.741\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.634 | Train PPL:   1.884\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.478 | Train PPL:   1.613\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.539\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.527\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.380 | Train PPL:   1.462\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.467\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.533\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.550\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.493\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.463\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.373 | Train PPL:   1.452\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.463\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.307\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.434\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.397\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.337\n",
      "Epoch: 101 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 102 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 103 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 104 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 105 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 106 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 107 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.312\n",
      "Epoch: 108 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 109 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 110 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 111 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 112 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 113 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 114 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 115 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 116 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 117 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 118 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 119 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 120 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 121 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 122 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 123 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 124 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 125 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 126 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 127 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 128 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 129 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 130 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 131 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
      "Epoch: 132 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 133 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 134 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 135 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 136 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.249\n",
      "Epoch: 137 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.250\n",
      "Epoch: 138 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 139 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 140 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 141 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 142 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 143 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 144 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 145 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.259\n",
      "Epoch: 146 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 147 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 148 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 149 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 150 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 151 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.215\n",
      "Epoch: 152 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 153 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 154 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 155 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 156 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 157 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 158 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 159 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 160 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 161 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 162 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 163 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 164 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 165 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 166 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 167 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 168 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 169 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 170 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 171 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 172 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 173 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 174 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 175 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 176 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 177 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.146\n",
      "Epoch: 178 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 179 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.179\n",
      "Epoch: 180 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 181 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 182 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 183 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 184 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 185 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 186 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 187 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 188 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 189 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 190 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 191 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 192 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 193 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 194 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 195 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 196 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 197 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 198 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 199 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 200 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 201 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 202 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 203 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 204 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 205 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 206 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 207 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 208 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 209 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 210 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 211 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 212 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 213 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 214 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 215 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.115\n",
      "Epoch: 216 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 217 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 218 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 219 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 220 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 221 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 222 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 223 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 224 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 225 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 226 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 227 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 228 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 229 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 230 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 231 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 232 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 233 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 234 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 235 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 236 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 237 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 238 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 239 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 240 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 241 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 242 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 243 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 244 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 245 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 246 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 247 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 248 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 249 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 250 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 251 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 252 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 253 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 254 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 255 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 256 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 257 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 258 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 259 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 260 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 261 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 262 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 263 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.391 | Train PPL:   1.478\n",
      "\t Val. Loss: 0.495 |  Val. PPL:   1.641\n",
      "Epoch: 264 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.579\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
      "Epoch: 265 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 266 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 267 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 268 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 269 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 270 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 271 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 272 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 273 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 274 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 275 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 276 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 277 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 278 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 279 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 280 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 281 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 282 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 283 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 284 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 285 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 286 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 287 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 288 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 289 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 290 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 291 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 292 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 293 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 294 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 295 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 296 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 297 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 298 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 299 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 300 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 301 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 302 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 303 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 304 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 305 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 306 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 307 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 308 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 309 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 310 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 311 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 312 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 313 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 314 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 315 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 316 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 317 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 318 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 319 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 320 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 321 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 322 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 323 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 324 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 325 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 326 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 327 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 328 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 329 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 330 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 331 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 332 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 333 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 334 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 335 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 336 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 337 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 338 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 339 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 340 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 341 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 342 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 343 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 344 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 345 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 346 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 347 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 348 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 349 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 350 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 351 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 352 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 353 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 354 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 355 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 356 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 357 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 358 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 359 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 360 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 361 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 362 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 363 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 364 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 365 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 366 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 367 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 368 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 369 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 370 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 371 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 372 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 373 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 374 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 375 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 376 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 378 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 379 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 380 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 381 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 382 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 384 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 385 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 386 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 387 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 389 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 390 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 392 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 393 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 394 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 395 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 397 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 398 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 399 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 400 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "SCHEDULE: Ensembler-1.s2.t2.e400\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.399\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.308 |  Val. PPL:   1.361\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.139\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.139\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 101 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 102 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 103 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 104 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 105 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 106 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 107 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 108 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 109 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 110 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 111 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 112 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 113 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 114 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 115 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 116 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 117 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 118 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 119 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 120 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 121 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 122 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 123 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 124 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 125 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 126 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 127 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 128 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 129 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 130 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 131 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 132 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 133 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 134 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 135 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 136 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 137 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 138 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 139 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 140 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 141 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 142 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 143 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 144 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 145 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 146 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 147 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 148 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 149 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 150 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 151 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 152 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 153 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 155 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 162 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 179 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 186 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 189 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 201 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 202 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 203 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 204 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 205 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 206 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 207 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 208 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 209 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 210 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 211 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 212 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 213 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 214 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 215 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 216 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 217 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 218 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 219 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 220 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 221 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 222 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 223 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 224 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 225 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 226 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 227 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 228 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 229 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 230 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 231 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 232 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 233 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 234 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 235 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 236 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 237 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 238 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 239 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 240 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 241 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 242 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 243 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 244 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 245 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 246 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 247 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 248 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 249 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 250 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 251 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 252 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 253 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 254 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 255 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 256 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 257 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 258 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 259 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 260 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 261 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 262 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 263 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 264 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 265 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 266 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 267 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 268 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 269 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 270 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 271 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 272 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 273 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 274 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 275 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 276 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 277 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 278 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 279 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 280 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 281 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 282 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 283 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 284 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 285 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 286 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 287 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 288 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 289 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 290 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 291 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 292 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 293 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 294 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 295 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 296 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 297 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 298 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 299 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 300 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 301 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 302 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 303 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 304 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 305 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 306 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 307 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 308 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 309 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 310 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 311 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 312 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 313 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 314 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 315 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 316 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 317 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 318 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 319 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 320 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 321 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 322 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 323 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 324 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 325 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 326 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 327 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 328 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 329 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 330 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 331 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 332 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 333 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 334 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 335 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 336 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 337 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 338 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 339 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 340 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 341 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 342 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 343 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 344 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 345 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 346 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 347 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 348 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 349 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 350 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 351 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 352 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 353 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 354 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 355 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 356 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 357 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 358 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 359 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 360 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 361 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 362 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 363 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 364 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 365 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 366 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 367 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 368 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 369 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 370 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 371 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 372 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 373 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 374 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 375 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 376 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 377 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 378 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 379 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 380 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 381 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 382 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 383 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 384 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 385 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 386 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 387 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 388 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 389 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 390 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 391 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 392 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 393 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 394 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 395 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 396 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 397 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 398 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 399 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 400 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "@@@@@@@@@ Repetition   1 @@@@@@@@@\n",
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(19, 9, bidirectional=True)\n",
      "        (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "          (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(37, 9)\n",
      "        (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: Ensembler-1.s0.t0.e400\n",
      "Epoch: 01 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.986 | Train PPL:   2.682\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 02 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 1.002 | Train PPL:   2.725\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 03 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.986 | Train PPL:   2.680\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 04 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.953 | Train PPL:   2.594\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 05 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.962 | Train PPL:   2.618\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 06 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.967 | Train PPL:   2.630\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 07 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.959 | Train PPL:   2.608\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 08 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.983 | Train PPL:   2.671\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 09 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.967 | Train PPL:   2.629\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 10 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.984 | Train PPL:   2.676\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 11 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.970 | Train PPL:   2.638\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 12 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.969 | Train PPL:   2.635\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 13 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.956 | Train PPL:   2.602\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 14 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 1.012 | Train PPL:   2.752\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 15 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.943 | Train PPL:   2.569\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 16 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.992 | Train PPL:   2.697\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 17 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.974 | Train PPL:   2.649\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 18 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.987 | Train PPL:   2.684\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 19 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.951 | Train PPL:   2.588\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 20 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.941 | Train PPL:   2.563\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 21 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.977 | Train PPL:   2.655\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 22 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.960 | Train PPL:   2.612\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 23 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 1.023 | Train PPL:   2.782\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 24 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.981 | Train PPL:   2.667\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 25 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.976 | Train PPL:   2.654\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 26 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.977 | Train PPL:   2.658\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 27 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.944 | Train PPL:   2.569\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 28 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.958 | Train PPL:   2.606\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 29 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.973 | Train PPL:   2.645\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 30 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.980 | Train PPL:   2.666\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "Epoch: 31 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.956 | Train PPL:   2.601\n",
      "\t Val. Loss: 0.973 |  Val. PPL:   2.645\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.628 | Train PPL:   1.874\n",
      "\t Val. Loss: 0.522 |  Val. PPL:   1.686\n",
      "Epoch: 33 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.569\n",
      "\t Val. Loss: 0.470 |  Val. PPL:   1.600\n",
      "Epoch: 34 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.450 | Train PPL:   1.568\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 35 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 36 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.494\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 37 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.490\n",
      "Epoch: 38 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.456\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 39 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 40 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 41 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 42 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.416\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 43 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.474\n",
      "Epoch: 44 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 45 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.502\n",
      "Epoch: 46 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 47 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 48 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 49 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 50 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 51 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.383\n",
      "Epoch: 52 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 53 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 54 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 55 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 56 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 57 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 58 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 59 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 60 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.318 |  Val. PPL:   1.374\n",
      "Epoch: 61 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
      "Epoch: 62 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 63 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 64 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.281\n",
      "Epoch: 65 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 66 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 67 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 68 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 69 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 70 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 71 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
      "Epoch: 72 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 73 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 74 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.254\n",
      "Epoch: 75 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 76 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 77 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 78 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 79 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 80 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.248\n",
      "Epoch: 81 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 82 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 83 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 84 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 85 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 86 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 87 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 88 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 89 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 90 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 91 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 92 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 93 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 94 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 95 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 96 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 97 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 98 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 99 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 100 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 101 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 102 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 103 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 104 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 105 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 106 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 107 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 108 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 109 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 110 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 111 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.185\n",
      "Epoch: 112 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 113 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 114 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 115 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 116 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.215\n",
      "Epoch: 117 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 118 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 119 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 120 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 121 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 122 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 123 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 124 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 125 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 126 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.134\n",
      "Epoch: 127 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 128 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 129 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 130 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 131 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 132 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 133 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 134 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 135 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 136 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 137 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 138 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 139 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 140 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 141 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 142 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 143 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 144 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 145 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 146 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 147 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 148 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 149 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 150 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 151 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 152 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 153 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 154 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 155 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 156 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 157 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 158 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 159 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 160 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 161 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 162 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 163 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 164 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 165 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 166 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 167 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 168 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 169 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 170 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 171 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 172 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 173 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 174 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 175 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 176 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 177 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 178 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 179 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 180 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 181 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 182 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 183 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 184 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 185 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 186 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 187 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 188 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 189 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 190 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 191 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 192 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 193 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 194 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 195 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 196 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 197 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 198 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 199 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 200 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 201 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 202 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 203 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 204 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 205 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 206 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 207 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 208 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 209 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 210 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 211 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 212 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 213 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 214 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 215 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 216 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 217 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 218 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 219 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 220 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 221 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 222 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 223 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 224 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 225 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 226 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 227 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 228 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 229 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 230 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 231 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 232 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 233 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 234 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 235 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 236 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 237 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 238 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 239 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.115\n",
      "Epoch: 240 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 241 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 242 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 243 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 244 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 245 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 246 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 247 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 248 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 249 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 250 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 251 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 252 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 253 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 254 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 255 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 256 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 257 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 258 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 259 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 260 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 261 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 262 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 263 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 264 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 265 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 266 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 267 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 268 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 269 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 270 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 271 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 272 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 273 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 274 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 275 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 276 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 277 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 278 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 279 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 280 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 281 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 282 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 283 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 284 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 285 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 286 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 287 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 288 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 289 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 290 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 291 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 292 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 293 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 294 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 295 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 296 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 297 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 298 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 299 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 300 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 301 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 302 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 303 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 304 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 305 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 306 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 307 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 308 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 309 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 310 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 311 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 312 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 313 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 314 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 315 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 316 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 317 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 318 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 319 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 320 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 321 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 322 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 323 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 324 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 325 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 326 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 327 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 328 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 329 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 330 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 331 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 332 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 333 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 334 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 335 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 336 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 337 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 339 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 340 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 341 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 342 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 343 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 344 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 345 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 347 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 348 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 349 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 350 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 352 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 353 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 354 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 355 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 356 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 357 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 358 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 359 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 360 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 361 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 362 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 363 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 364 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 365 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 369 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 374 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 375 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 376 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 379 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 385 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 390 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 392 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s | R1 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "SCHEDULE: Ensembler-1.s1.t1.e400\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.597 | Train PPL:   1.817\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 02 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.700 | Train PPL:   2.015\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 03 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.710 | Train PPL:   2.035\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 04 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.632 | Train PPL:   1.882\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 05 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.600 | Train PPL:   1.822\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 06 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.628 | Train PPL:   1.873\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 07 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.778 | Train PPL:   2.178\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 08 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.709 | Train PPL:   2.033\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 09 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.622 | Train PPL:   1.863\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 10 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.613 | Train PPL:   1.845\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 11 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.605 | Train PPL:   1.832\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 12 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.738 | Train PPL:   2.091\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 13 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.697 | Train PPL:   2.007\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 14 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.742 | Train PPL:   2.100\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 15 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.661 | Train PPL:   1.936\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 16 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.650 | Train PPL:   1.915\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 17 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.761 | Train PPL:   2.140\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 18 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.666 | Train PPL:   1.946\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 19 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.625 | Train PPL:   1.868\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 20 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.647 | Train PPL:   1.909\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 21 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.738 | Train PPL:   2.092\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 22 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.711 | Train PPL:   2.036\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 23 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.726 | Train PPL:   2.066\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 24 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.607 | Train PPL:   1.835\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 25 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.696 | Train PPL:   2.006\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 26 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.686 | Train PPL:   1.987\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 27 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.654 | Train PPL:   1.923\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 28 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.761 | Train PPL:   2.140\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 29 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.621 | Train PPL:   1.861\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 30 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.713 | Train PPL:   2.041\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 31 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.635 | Train PPL:   1.886\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "-----------------------------------\n",
      "-----Added Expert-train Gating-----\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.698 | Train PPL:   2.010\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 33 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.721 | Train PPL:   2.056\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 34 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.646 | Train PPL:   1.908\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 35 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.657 | Train PPL:   1.930\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 36 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.710 | Train PPL:   2.034\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 37 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.658 | Train PPL:   1.931\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 38 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.647 | Train PPL:   1.910\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 39 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.791 | Train PPL:   2.206\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 40 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.651 | Train PPL:   1.917\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 41 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.735 | Train PPL:   2.086\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 42 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.739 | Train PPL:   2.093\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 43 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.732 | Train PPL:   2.079\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 44 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.745 | Train PPL:   2.107\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 45 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.684 | Train PPL:   1.982\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 46 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.586 | Train PPL:   1.797\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 47 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.665 | Train PPL:   1.944\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 48 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.722 | Train PPL:   2.058\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 49 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.619 | Train PPL:   1.856\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 50 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.597 | Train PPL:   1.817\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 51 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.588 | Train PPL:   1.800\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 52 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.701 | Train PPL:   2.015\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 53 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.659 | Train PPL:   1.934\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 54 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.675 | Train PPL:   1.963\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 55 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.608 | Train PPL:   1.836\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 56 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.639 | Train PPL:   1.895\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 57 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.712 | Train PPL:   2.039\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 58 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.669 | Train PPL:   1.952\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 59 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.735 | Train PPL:   2.085\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 60 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.605 | Train PPL:   1.830\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 61 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.680 | Train PPL:   1.973\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 62 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.656 | Train PPL:   1.927\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 63 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.614 | Train PPL:   1.848\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 64 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.465 | Train PPL:   1.593\n",
      "\t Val. Loss: 0.725 |  Val. PPL:   2.064\n",
      "Epoch: 65 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.424 |  Val. PPL:   1.528\n",
      "Epoch: 66 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.414 | Train PPL:   1.513\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
      "Epoch: 67 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.434\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 68 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.509\n",
      "Epoch: 69 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.387 | Train PPL:   1.473\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.521\n",
      "Epoch: 70 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.519\n",
      "Epoch: 71 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.506\n",
      "Epoch: 72 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.355 | Train PPL:   1.426\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 73 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.465\n",
      "Epoch: 74 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 75 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.437\n",
      "Epoch: 76 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 77 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.424\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 78 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 79 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
      "Epoch: 80 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.411\n",
      "Epoch: 81 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 82 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 83 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 84 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.403 |  Val. PPL:   1.497\n",
      "Epoch: 85 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 86 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 87 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.312\n",
      "Epoch: 88 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.348\n",
      "Epoch: 89 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.368\n",
      "Epoch: 90 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 91 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 92 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 93 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.368\n",
      "Epoch: 94 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.425\n",
      "Epoch: 95 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.274\n",
      "Epoch: 96 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 97 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
      "Epoch: 98 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 99 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 100 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 101 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 102 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 103 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 104 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 105 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 106 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 107 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 108 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
      "Epoch: 109 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
      "Epoch: 110 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.292\n",
      "Epoch: 111 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
      "Epoch: 112 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
      "Epoch: 113 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 114 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 115 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 116 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
      "Epoch: 117 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 118 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 119 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.254\n",
      "Epoch: 120 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 121 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 122 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 123 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 124 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 125 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 126 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 127 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 128 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 129 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 130 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.182\n",
      "Epoch: 131 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.276\n",
      "Epoch: 132 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 133 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 134 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 135 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.175\n",
      "Epoch: 136 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 137 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 138 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 139 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 140 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 141 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 142 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 143 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 144 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.175\n",
      "Epoch: 145 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 146 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 147 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 148 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 149 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 150 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 151 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 152 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 153 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 154 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 155 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 156 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 157 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 158 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "Epoch: 159 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 160 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.125\n",
      "Epoch: 161 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 162 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 163 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 164 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 165 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 166 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 167 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 168 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 169 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 170 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 171 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 172 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 173 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 174 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 175 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 176 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 177 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 178 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 179 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 180 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 181 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 182 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 183 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 184 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 185 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 186 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 187 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 188 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 189 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 190 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 191 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 192 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 193 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 194 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 195 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 196 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 197 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 198 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 199 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 200 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 201 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 202 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 203 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 204 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 205 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 206 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 207 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 208 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 209 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 210 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 211 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 212 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 213 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 214 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 215 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 216 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 217 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 218 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 219 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 220 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 221 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 222 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 223 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 224 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 225 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 226 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 227 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 228 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 229 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 230 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 231 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 232 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 233 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 234 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 235 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 236 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 237 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 238 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 239 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 240 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 241 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 242 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 243 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 244 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 245 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 246 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 247 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 248 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 249 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 250 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 251 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 252 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 253 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 254 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 255 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 256 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 257 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 258 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 259 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 260 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 261 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 262 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 263 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 264 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 265 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 266 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 267 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 268 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 269 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 270 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 271 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 272 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 273 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 274 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 275 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 276 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 277 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 278 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 279 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 280 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 281 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 282 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 283 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 284 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 285 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 286 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 287 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 288 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 289 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 290 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 291 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 292 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 293 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 294 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 295 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 296 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 297 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 298 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 299 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 300 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 301 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 302 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 303 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 304 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 305 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 306 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 308 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 309 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 313 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 314 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 315 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 316 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 317 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 318 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 319 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 320 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 321 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 322 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 323 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 324 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 325 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 326 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 327 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 328 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 329 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 330 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 331 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 332 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 333 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 334 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 335 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 336 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 337 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 338 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 339 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 340 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 341 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 342 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 343 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 344 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 345 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 346 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 347 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 348 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 349 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 350 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 351 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 352 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 353 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 354 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 355 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 356 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 357 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 358 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 359 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 360 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 361 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 362 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 363 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 364 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 365 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 366 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 367 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 368 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 369 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 370 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 371 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 372 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 373 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 374 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 375 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 376 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 377 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 378 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 379 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 380 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 381 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 382 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 383 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 384 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 385 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 386 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 387 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 388 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 389 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 390 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 391 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 392 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 394 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 395 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 397 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s | R1 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "SCHEDULE: Ensembler-1.s2.t2.e400\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.355\n",
      "Epoch: 02 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.496\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 03 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 04 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.406 | Train PPL:   1.502\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.355\n",
      "Epoch: 05 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.425 | Train PPL:   1.530\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 06 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 07 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 08 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 09 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 10 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 11 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 12 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 13 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 14 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 15 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 16 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 17 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 18 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 19 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 20 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 21 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 22 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 23 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 24 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 25 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 26 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 27 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 28 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 29 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 30 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 31 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 32 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 33 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 34 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 35 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 36 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 37 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 38 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 39 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 40 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 41 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 42 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 43 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 44 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 45 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 46 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 47 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 48 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 49 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 50 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 51 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 52 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 53 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 54 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 55 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 56 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 57 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 58 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 59 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 60 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 61 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 62 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 63 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 64 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 65 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 66 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 67 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 68 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 69 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 70 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 71 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 72 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 73 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 74 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 75 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 76 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 77 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 78 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 79 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 80 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 81 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 82 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 83 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 84 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 85 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 86 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 87 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 88 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 89 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 90 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 91 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 92 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 93 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 94 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 95 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 96 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 97 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 98 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 99 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 100 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 101 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 102 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 103 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 104 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 105 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 106 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 107 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 108 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 109 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 110 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 111 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 112 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 113 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 114 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 115 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 116 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 117 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 118 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 119 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 120 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 121 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 122 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 123 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 124 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 125 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 126 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 127 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 128 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 129 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 130 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 131 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 132 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 133 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 134 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 135 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 136 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 137 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 138 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 139 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 140 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 141 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 142 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 143 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 144 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 145 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 146 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 147 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 148 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 149 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 150 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 151 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 152 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 153 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 154 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 155 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 156 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 157 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 158 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 159 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 160 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 161 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 162 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 164 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 165 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 166 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 167 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 168 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 169 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 170 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 171 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 201 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 202 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 203 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 204 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 205 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 206 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 207 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 208 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 209 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 210 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 211 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 212 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 213 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 214 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 215 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 216 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 217 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 218 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 219 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 220 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 221 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 222 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 223 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 224 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 225 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 226 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 227 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 228 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 229 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 230 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 231 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 232 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 233 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 234 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 235 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 236 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 237 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 238 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 239 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 240 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 241 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 242 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 243 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 244 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 245 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 246 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 247 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 248 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 249 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 250 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 251 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 252 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 253 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 254 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 255 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 256 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 257 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 258 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 259 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 260 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 261 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 262 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 263 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 264 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 265 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 266 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 267 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 268 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 269 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 270 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 271 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 272 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 273 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 274 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 275 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 276 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 277 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 278 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 279 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 280 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 281 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 282 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 283 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 284 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 285 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 286 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 287 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 288 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 289 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 290 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 291 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 292 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 293 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 294 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 295 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 296 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 297 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 298 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 299 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 300 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 301 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 302 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 303 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 305 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 308 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 315 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 316 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 318 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 321 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 326 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 328 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 330 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 331 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 332 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 333 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 335 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 336 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 337 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 340 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 341 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 342 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 343 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 344 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 347 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 348 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 349 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 350 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 352 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 353 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 354 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 355 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 356 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 360 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 362 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 363 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 369 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 374 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 375 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 376 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 379 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 385 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 390 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 392 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s | R1 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "@@@@@@@@@ Repetition   2 @@@@@@@@@\n",
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(19, 9, bidirectional=True)\n",
      "        (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "          (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(37, 9)\n",
      "        (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: Ensembler-1.s0.t0.e400\n",
      "Epoch: 01 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.854 | Train PPL:   2.349\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 02 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.832 | Train PPL:   2.299\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 03 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.855 | Train PPL:   2.351\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 04 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.830 | Train PPL:   2.292\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 05 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.870 | Train PPL:   2.386\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 06 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.831 | Train PPL:   2.297\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 07 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.862 | Train PPL:   2.369\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 08 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.884 | Train PPL:   2.421\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 09 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.814 | Train PPL:   2.257\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 10 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.881 | Train PPL:   2.414\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 11 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.854 | Train PPL:   2.349\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 12 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.867 | Train PPL:   2.380\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 13 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.800 | Train PPL:   2.227\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 14 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.918 | Train PPL:   2.504\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 15 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.887 | Train PPL:   2.429\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 16 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.897 | Train PPL:   2.453\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 17 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.910 | Train PPL:   2.484\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 18 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.890 | Train PPL:   2.436\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 19 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.853 | Train PPL:   2.346\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 20 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.879 | Train PPL:   2.409\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 21 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.857 | Train PPL:   2.356\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 22 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.839 | Train PPL:   2.315\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 23 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.844 | Train PPL:   2.325\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 24 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.890 | Train PPL:   2.435\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 25 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.870 | Train PPL:   2.386\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 26 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.888 | Train PPL:   2.431\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 27 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.918 | Train PPL:   2.503\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 28 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.871 | Train PPL:   2.389\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 29 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.862 | Train PPL:   2.367\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 30 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.847 | Train PPL:   2.333\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "Epoch: 31 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.903 | Train PPL:   2.468\n",
      "\t Val. Loss: 0.879 |  Val. PPL:   2.410\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.703 | Train PPL:   2.020\n",
      "\t Val. Loss: 0.564 |  Val. PPL:   1.758\n",
      "Epoch: 33 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.526 | Train PPL:   1.693\n",
      "\t Val. Loss: 0.477 |  Val. PPL:   1.611\n",
      "Epoch: 34 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 35 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.509\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 36 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.500\n",
      "\t Val. Loss: 0.424 |  Val. PPL:   1.528\n",
      "Epoch: 37 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.427\n",
      "\t Val. Loss: 0.431 |  Val. PPL:   1.539\n",
      "Epoch: 38 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.494\n",
      "Epoch: 39 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.483\n",
      "Epoch: 40 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 41 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 42 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.482\n",
      "Epoch: 43 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.502\n",
      "Epoch: 44 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.499\n",
      "Epoch: 45 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.490\n",
      "Epoch: 46 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 47 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.476\n",
      "Epoch: 48 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 49 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.460\n",
      "Epoch: 50 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 51 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
      "Epoch: 52 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 53 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 54 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
      "Epoch: 55 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 56 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 57 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.321\n",
      "Epoch: 58 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 59 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 60 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 61 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 62 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 63 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 64 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 65 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 66 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 67 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 68 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 69 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 70 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 71 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.232\n",
      "Epoch: 72 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 73 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 74 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 75 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 76 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 77 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 78 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 79 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 80 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 81 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.203\n",
      "Epoch: 82 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 83 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 84 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 85 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 86 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 87 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 88 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 89 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.149\n",
      "Epoch: 90 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 91 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 92 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 93 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 94 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 95 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 96 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 97 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 98 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 99 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 100 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 101 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.143\n",
      "Epoch: 102 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 103 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 104 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 105 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.154\n",
      "Epoch: 106 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 107 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 108 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 109 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 110 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 111 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 112 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 113 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 114 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 115 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 116 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 117 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 118 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 119 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 120 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 121 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 122 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 123 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 124 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 125 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 126 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 127 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 128 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 129 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 130 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 131 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 132 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 133 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 134 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 135 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 136 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 137 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 138 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 139 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 140 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 141 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 142 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 143 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 144 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 145 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 146 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 147 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 148 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 149 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 150 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 151 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 152 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 153 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 154 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 155 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 156 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 157 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 158 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 159 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 160 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 161 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 162 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 163 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 164 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 165 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 166 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 167 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 168 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 169 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 170 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 171 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 172 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 173 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 174 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 175 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 176 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 177 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 178 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 179 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 180 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 181 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 182 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 183 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 184 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 185 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 186 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 187 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 188 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 189 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 190 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 191 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 192 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 193 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 194 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 195 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 196 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 197 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 198 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 199 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 200 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 201 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 202 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 203 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 204 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 205 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 206 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 207 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 208 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 209 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 210 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 211 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 212 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 213 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 214 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 215 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 216 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 217 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 218 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 219 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 220 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 221 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 222 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 223 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 224 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 225 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 226 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 227 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 228 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 229 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 230 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 231 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 232 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 233 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 234 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 235 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 236 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 237 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 238 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 239 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 240 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 241 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 242 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 243 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 244 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 245 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 246 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 247 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 248 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 249 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 250 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 251 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 252 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 253 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 254 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 255 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 256 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 257 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 258 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 259 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 260 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 261 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 262 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 263 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 264 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 265 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 266 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 267 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 268 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 269 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 270 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 271 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 272 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 273 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 274 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 275 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 276 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 277 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 278 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 279 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 280 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 281 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 282 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 283 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 284 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 285 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 286 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 287 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 288 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 289 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 290 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 291 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 292 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 293 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 294 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 295 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 296 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 297 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 298 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 299 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 300 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 301 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 302 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 303 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 305 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 308 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 315 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 316 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 318 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 320 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 321 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 322 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 323 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 324 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 325 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 326 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 328 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 330 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 331 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 332 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 333 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 335 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 336 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 337 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 338 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 339 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 340 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 341 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 342 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 343 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 344 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 345 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 347 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 348 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 349 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 350 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 351 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 352 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 353 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 354 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 355 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 356 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 360 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 362 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 363 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 369 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 370 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 371 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 372 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 373 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 374 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 375 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 376 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 379 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 385 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 390 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 392 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 396 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 397 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 399 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s | R2 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "\n",
      "SCHEDULE: Ensembler-1.s1.t1.e400\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.434 | Train PPL:   1.543\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 02 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.459 | Train PPL:   1.583\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 03 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.450 | Train PPL:   1.569\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 04 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.418 | Train PPL:   1.518\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 05 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.488 | Train PPL:   1.629\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 06 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.449 | Train PPL:   1.567\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 07 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.507 | Train PPL:   1.661\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 08 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 09 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.484 | Train PPL:   1.622\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 10 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 11 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.476 | Train PPL:   1.610\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 12 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.437 | Train PPL:   1.548\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 13 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.464 | Train PPL:   1.590\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 14 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.487 | Train PPL:   1.627\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 15 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.487 | Train PPL:   1.628\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 16 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.483 | Train PPL:   1.621\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 17 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.489 | Train PPL:   1.631\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 18 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.471 | Train PPL:   1.602\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 19 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.471 | Train PPL:   1.602\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 20 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.564\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 21 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.462 | Train PPL:   1.587\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 22 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 23 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.603\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 24 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 25 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.436 | Train PPL:   1.546\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 26 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.579\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 27 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.473 | Train PPL:   1.604\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 28 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.482 | Train PPL:   1.619\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 29 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.446 | Train PPL:   1.562\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 30 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.487 | Train PPL:   1.627\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 31 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.564\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "-----------------------------------\n",
      "-----Added Expert-train Gating-----\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.437 | Train PPL:   1.549\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 33 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.558\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 34 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.471 | Train PPL:   1.602\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 35 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.469 | Train PPL:   1.598\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 36 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.478 | Train PPL:   1.613\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 37 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.484 | Train PPL:   1.623\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 38 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.461 | Train PPL:   1.586\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 39 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.577\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 40 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.449 | Train PPL:   1.566\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 41 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.577\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 42 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 43 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.450 | Train PPL:   1.568\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 44 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.476 | Train PPL:   1.610\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 45 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.525\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 46 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.426 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 47 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.430 | Train PPL:   1.537\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 48 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.448 | Train PPL:   1.565\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 49 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.463 | Train PPL:   1.589\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 50 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.458 | Train PPL:   1.582\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 51 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.448 | Train PPL:   1.565\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 52 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 53 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.499 | Train PPL:   1.647\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 54 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.495 | Train PPL:   1.641\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 55 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.558\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 56 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.436 | Train PPL:   1.546\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 57 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.578\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 58 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.487 | Train PPL:   1.627\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 59 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.446 | Train PPL:   1.562\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 60 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.438 | Train PPL:   1.550\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 61 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.458 | Train PPL:   1.581\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 62 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.453 | Train PPL:   1.573\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 63 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.664 | Train PPL:   1.942\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 64 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.502 | Train PPL:   1.651\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 65 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 66 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.502\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 67 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.466\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.487\n",
      "Epoch: 68 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 69 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.355 | Train PPL:   1.427\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 70 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 71 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.543\n",
      "Epoch: 72 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 73 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.540\n",
      "Epoch: 74 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.552\n",
      "Epoch: 75 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
      "Epoch: 76 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 77 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 78 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 79 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
      "Epoch: 80 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 81 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 82 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
      "\t Val. Loss: 0.315 |  Val. PPL:   1.371\n",
      "Epoch: 83 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 84 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 85 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 86 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 87 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 88 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 89 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.316\n",
      "Epoch: 90 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 91 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
      "Epoch: 92 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.305\n",
      "Epoch: 93 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 94 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
      "Epoch: 95 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
      "Epoch: 96 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 97 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 98 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.259\n",
      "Epoch: 99 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 100 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 101 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 102 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.247\n",
      "Epoch: 103 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.244\n",
      "Epoch: 104 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 105 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 106 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 107 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 108 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 109 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.234\n",
      "Epoch: 110 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 111 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.194\n",
      "Epoch: 112 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 113 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 114 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 115 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 116 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 117 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 118 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 119 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 120 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 121 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.151\n",
      "Epoch: 122 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 123 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 124 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 125 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 126 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 127 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 128 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.145\n",
      "Epoch: 129 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 130 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 131 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 132 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 133 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 134 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 135 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 136 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 137 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 138 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 139 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 140 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 141 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 142 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 143 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 144 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 145 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 146 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 147 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 148 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 149 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 150 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 151 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 152 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 153 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 154 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 155 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 156 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 157 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 158 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 159 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 160 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 161 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 162 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 163 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 164 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 165 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 166 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 167 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 168 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 169 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 170 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 171 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 172 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 173 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 174 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 175 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 176 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 177 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 178 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 179 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 180 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 181 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 182 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 183 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 184 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 185 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 186 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 187 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 188 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 189 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 190 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 191 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 192 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 193 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 194 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 195 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 196 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 197 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 198 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 199 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 200 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 201 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 202 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 203 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 204 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 205 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 206 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 207 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 208 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 209 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 210 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 211 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 212 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 213 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 214 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 215 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 216 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 217 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 218 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 219 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 220 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 221 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 222 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 223 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 224 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 225 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 226 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 227 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 228 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 229 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 230 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 231 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 232 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 233 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 234 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 235 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 236 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 237 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 238 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 239 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 240 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 241 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 242 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 243 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 244 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 245 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 246 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 247 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 248 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 249 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 250 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 251 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 252 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 253 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 254 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 255 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 256 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 257 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 258 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 259 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 260 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 261 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 262 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 263 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 264 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 265 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 266 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 267 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 268 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 269 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 270 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 271 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 272 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 273 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 274 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 275 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 276 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 277 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 278 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 279 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 280 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 281 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 282 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 283 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 284 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 285 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 286 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 287 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 288 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 289 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 290 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 291 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 292 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 293 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 294 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 295 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 296 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 297 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 298 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 299 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 300 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 301 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 302 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 303 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 305 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 308 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 314 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 315 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 316 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 318 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 321 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 326 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 328 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 330 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 331 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 332 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 333 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 335 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 336 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 337 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 340 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 341 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 342 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 343 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 344 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 346 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 347 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 348 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 349 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 350 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 351 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 352 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 353 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 354 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 355 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 356 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 358 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 360 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 362 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 363 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 368 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 369 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 370 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 371 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 372 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 373 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 374 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 375 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 376 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 378 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 379 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 385 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 390 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 392 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s | R2 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "SCHEDULE: Ensembler-1.s2.t2.e400\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.479\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
      "Epoch: 02 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
      "Epoch: 03 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 04 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 05 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 06 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 07 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 08 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 09 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 10 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 11 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 12 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 13 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 14 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 15 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 16 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 17 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 18 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 19 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 20 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 21 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 22 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 23 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 24 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 25 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 26 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 27 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 28 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 29 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 30 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 31 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 32 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 33 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 34 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 35 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 36 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 37 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 38 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 39 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 40 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 41 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 42 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 43 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 44 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 45 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 46 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 47 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 48 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 49 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 50 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 51 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 52 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 53 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 54 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 55 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 56 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 57 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 58 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 59 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 60 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 61 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 62 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 63 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 64 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 65 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 66 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 67 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 68 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 69 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 70 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 71 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 72 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 73 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 74 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 75 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 76 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 77 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 78 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 79 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 80 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 81 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 82 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 83 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 84 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 85 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 86 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 87 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 88 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 89 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 90 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 91 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 92 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 93 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 94 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 95 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 96 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 97 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 98 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 99 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 100 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 101 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 102 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 103 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 104 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 105 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 106 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 107 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 108 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 109 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 110 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 111 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 112 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 113 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 114 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 115 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 116 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 117 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 118 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 119 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 120 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 121 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 122 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 123 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 124 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 125 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 126 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 127 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 128 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 129 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 130 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 131 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 132 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 133 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 134 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 135 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 136 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 137 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 138 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 139 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 140 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 141 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 142 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 143 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 144 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 145 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 146 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 147 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 148 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 149 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 150 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 151 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 152 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 153 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 154 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 155 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 156 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 157 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 158 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 159 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 160 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 161 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 162 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 164 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 165 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 166 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 167 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 168 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 169 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 170 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 171 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 201 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 202 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 203 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 204 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 205 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 206 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 207 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 208 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 209 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 210 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 211 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 212 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 213 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 214 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 215 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 216 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 217 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 218 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 219 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 220 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 221 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 222 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 223 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 224 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 225 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 226 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 227 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 228 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 229 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 230 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 231 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 232 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 233 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 234 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 235 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 236 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 237 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 238 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 239 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 240 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 241 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 242 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 243 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 244 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 245 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 246 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 247 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 248 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 249 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 250 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 251 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 252 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 253 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 254 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 255 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 256 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 257 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 258 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 259 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 260 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 261 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 262 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 263 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 264 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 265 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 266 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 267 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 268 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 269 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 270 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 271 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 272 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 273 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 274 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 275 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 276 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 277 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 278 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 279 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 280 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 281 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 282 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 283 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 284 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 285 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 286 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 287 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 288 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 289 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 290 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 291 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 292 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 293 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 294 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 295 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 296 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 297 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 298 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 299 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 300 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 301 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 302 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 303 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 305 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 308 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 315 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 316 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 318 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 321 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 326 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 328 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 330 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 331 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 332 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 333 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 335 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 336 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 337 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 340 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 341 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 342 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 343 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 344 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 347 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 348 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 349 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 350 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 352 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 353 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 354 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 355 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 356 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 360 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 362 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 363 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 369 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 374 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 375 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 376 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 379 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 385 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 390 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 392 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s | R2 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "@@@@@@@@@ Repetition   3 @@@@@@@@@\n",
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(19, 9, bidirectional=True)\n",
      "        (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "          (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(37, 9)\n",
      "        (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: Ensembler-1.s0.t0.e400\n",
      "Epoch: 01 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.032 | Train PPL:   2.807\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 02 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.092 | Train PPL:   2.980\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 03 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.036 | Train PPL:   2.817\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 04 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.076 | Train PPL:   2.933\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 05 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.936\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 06 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.065 | Train PPL:   2.900\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 07 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.023 | Train PPL:   2.782\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 08 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.074 | Train PPL:   2.927\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 09 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.136 | Train PPL:   3.113\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 10 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.057 | Train PPL:   2.877\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 11 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.936\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 12 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.060 | Train PPL:   2.887\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 13 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.103 | Train PPL:   3.013\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 14 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.086 | Train PPL:   2.961\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 15 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.070 | Train PPL:   2.916\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 16 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.992 | Train PPL:   2.697\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 17 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.018 | Train PPL:   2.769\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 18 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.034 | Train PPL:   2.813\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 19 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.081 | Train PPL:   2.949\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 20 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.089 | Train PPL:   2.973\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 21 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.036 | Train PPL:   2.818\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 22 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.090 | Train PPL:   2.975\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 23 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.032 | Train PPL:   2.808\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 24 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.023 | Train PPL:   2.781\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 25 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.107 | Train PPL:   3.025\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 26 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.059 | Train PPL:   2.883\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 27 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.052 | Train PPL:   2.864\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 28 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.064 | Train PPL:   2.899\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 29 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.057 | Train PPL:   2.877\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 30 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.115 | Train PPL:   3.049\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "Epoch: 31 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 1.068 | Train PPL:   2.909\n",
      "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.662 | Train PPL:   1.939\n",
      "\t Val. Loss: 0.539 |  Val. PPL:   1.714\n",
      "Epoch: 33 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.506 | Train PPL:   1.659\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.556\n",
      "Epoch: 34 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 35 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.452 |  Val. PPL:   1.572\n",
      "Epoch: 36 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.487\n",
      "\t Val. Loss: 0.403 |  Val. PPL:   1.496\n",
      "Epoch: 37 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.440\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 38 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.443 |  Val. PPL:   1.557\n",
      "Epoch: 39 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.355 | Train PPL:   1.426\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.506\n",
      "Epoch: 40 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.431 |  Val. PPL:   1.538\n",
      "Epoch: 41 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.540\n",
      "Epoch: 42 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.512\n",
      "Epoch: 43 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.465\n",
      "Epoch: 44 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 45 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.441\n",
      "Epoch: 46 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.466\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.430\n",
      "Epoch: 47 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 48 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 49 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 50 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 51 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 52 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.457\n",
      "Epoch: 53 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.460\n",
      "Epoch: 54 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.437\n",
      "Epoch: 55 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 56 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 57 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 58 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 59 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 60 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 61 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.417\n",
      "Epoch: 62 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.269\n",
      "Epoch: 63 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
      "Epoch: 64 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 65 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 66 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 67 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 68 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 69 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 70 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 71 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 72 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 73 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 74 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 75 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 76 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 77 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 78 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 79 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 80 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 81 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 82 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 83 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 84 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 85 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 86 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 87 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 88 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.185\n",
      "Epoch: 89 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 90 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 91 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 92 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 93 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 94 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 95 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 96 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 97 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "Epoch: 98 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 99 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 100 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.168\n",
      "Epoch: 101 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 102 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 103 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 104 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 105 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 106 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 107 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.151\n",
      "Epoch: 108 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 109 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 110 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 111 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 112 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 113 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 114 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 115 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 116 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 117 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 118 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 119 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 120 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 121 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 122 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 123 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 124 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 125 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 126 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 127 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 128 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 129 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 130 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 131 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 132 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 133 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 134 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 135 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 136 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 137 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 138 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 139 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 140 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 141 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 142 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 143 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 144 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 145 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 146 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 147 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 148 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 149 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 150 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 151 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 152 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 153 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 154 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 155 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 156 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 157 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 158 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 159 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 160 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 161 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 162 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 163 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 164 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 165 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 166 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 167 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 168 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 169 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 170 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 171 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 172 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 173 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 174 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 175 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 176 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 177 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 179 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 180 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 186 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 201 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 202 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 203 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 204 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 205 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 206 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 207 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 208 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 209 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 210 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 211 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 212 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 213 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 214 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 215 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 216 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 217 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 218 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 219 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 220 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 221 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 222 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 223 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 224 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 225 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 226 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 227 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 228 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 229 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 230 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 231 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 232 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 233 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 234 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 235 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 236 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 237 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 238 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 239 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 240 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 241 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 242 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 243 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 244 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 245 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 246 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 247 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 248 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 249 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 250 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 251 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 252 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 253 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 254 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 255 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 256 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 257 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 258 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 259 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 260 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 261 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 262 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 263 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 264 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 265 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 266 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 267 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 268 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 269 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 270 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 271 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 272 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 273 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 274 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 275 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 276 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 277 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 278 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 279 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 280 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 281 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 282 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 283 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 284 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 285 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 286 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 287 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 288 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 289 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 290 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 291 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 292 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 293 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 294 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 295 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 296 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 297 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 298 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 299 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 300 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 301 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 302 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 303 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 305 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 306 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 308 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 309 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 310 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 315 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 316 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 317 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 318 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 319 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 320 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 321 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 322 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 323 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 324 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 325 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 326 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 327 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 328 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 329 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 330 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 331 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 332 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 333 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 334 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 335 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 336 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 337 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 338 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 339 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 340 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 341 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 342 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 343 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 344 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 345 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 346 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 347 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 348 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 349 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 350 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 351 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 352 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 353 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 354 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 355 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 356 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 357 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 358 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 359 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 360 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 361 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 362 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 363 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 364 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 365 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 366 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 367 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 368 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 369 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 370 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 371 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 372 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 373 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 374 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 375 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 376 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 379 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 380 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 385 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 390 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 392 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s | R3 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "SCHEDULE: Ensembler-1.s1.t1.e400\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.522 | Train PPL:   1.686\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 02 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.519 | Train PPL:   1.680\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 03 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.724\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 04 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.535 | Train PPL:   1.708\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 05 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.515 | Train PPL:   1.673\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 06 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.519 | Train PPL:   1.680\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 07 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.725\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 08 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.669\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 09 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.712\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 10 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.525 | Train PPL:   1.690\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 11 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.513 | Train PPL:   1.670\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 12 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.517 | Train PPL:   1.677\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 13 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.712\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 14 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.537 | Train PPL:   1.711\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 15 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.524 | Train PPL:   1.689\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 16 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.523 | Train PPL:   1.687\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 17 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.513 | Train PPL:   1.671\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 18 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.519 | Train PPL:   1.680\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 19 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.669\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 20 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.718\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 21 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.531 | Train PPL:   1.701\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 22 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.513 | Train PPL:   1.670\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 23 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.543 | Train PPL:   1.722\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 24 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.528 | Train PPL:   1.695\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 25 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.520 | Train PPL:   1.681\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 26 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.515 | Train PPL:   1.674\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 27 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.514 | Train PPL:   1.673\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 28 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.522 | Train PPL:   1.686\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 29 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.544 | Train PPL:   1.722\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 30 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.530 | Train PPL:   1.699\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 31 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.534 | Train PPL:   1.705\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "-----------------------------------\n",
      "-----Added Expert-train Gating-----\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 33 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.507 | Train PPL:   1.661\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 34 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.520 | Train PPL:   1.682\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 35 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.524 | Train PPL:   1.689\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 36 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.517 | Train PPL:   1.677\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 37 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.518 | Train PPL:   1.679\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 38 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.529 | Train PPL:   1.697\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 39 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.526 | Train PPL:   1.692\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 40 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.518 | Train PPL:   1.679\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 41 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.522 | Train PPL:   1.686\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 42 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.530 | Train PPL:   1.699\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 43 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.520 | Train PPL:   1.682\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 44 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.514 | Train PPL:   1.672\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 45 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.516 | Train PPL:   1.676\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 46 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.527 | Train PPL:   1.695\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 47 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.715\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 48 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.544 | Train PPL:   1.723\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 49 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.515 | Train PPL:   1.674\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 50 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 51 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.521 | Train PPL:   1.683\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 52 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.521 | Train PPL:   1.684\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 53 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.518 | Train PPL:   1.679\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 54 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.524 | Train PPL:   1.688\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 55 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.528 | Train PPL:   1.695\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 56 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.537 | Train PPL:   1.711\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 57 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.513 | Train PPL:   1.670\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 58 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.535 | Train PPL:   1.708\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 59 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.548 | Train PPL:   1.730\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 60 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.516 | Train PPL:   1.675\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 61 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.521 | Train PPL:   1.684\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 62 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.713\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 63 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.629 | Train PPL:   1.876\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 64 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.501 | Train PPL:   1.651\n",
      "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
      "Epoch: 65 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.419 | Train PPL:   1.520\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 66 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.573\n",
      "Epoch: 67 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
      "Epoch: 68 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.476\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 69 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.426 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 70 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n",
      "Epoch: 71 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.472\n",
      "Epoch: 72 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.482\n",
      "Epoch: 73 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.370 | Train PPL:   1.448\n",
      "\t Val. Loss: 0.404 |  Val. PPL:   1.497\n",
      "Epoch: 74 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 75 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.404 |  Val. PPL:   1.498\n",
      "Epoch: 76 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.488\n",
      "Epoch: 77 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.496\n",
      "Epoch: 78 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 79 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.397\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.475\n",
      "Epoch: 80 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 81 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 82 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.430\n",
      "Epoch: 83 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.370 | Train PPL:   1.448\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
      "Epoch: 84 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
      "Epoch: 85 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.363\n",
      "Epoch: 86 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 87 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 88 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 89 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 90 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 91 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.278\n",
      "Epoch: 92 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.278\n",
      "Epoch: 93 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.264\n",
      "Epoch: 94 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 95 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 96 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 97 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 98 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
      "Epoch: 99 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.252\n",
      "Epoch: 100 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 101 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 102 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 103 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 104 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 105 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 106 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 107 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 108 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 109 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 110 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 111 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 112 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 113 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 114 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 115 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 116 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.168\n",
      "Epoch: 117 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.188\n",
      "Epoch: 118 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 119 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 120 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 121 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 122 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 123 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 124 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 125 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 126 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 127 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 128 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 129 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 130 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 131 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 132 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.168\n",
      "Epoch: 133 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 134 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 135 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 136 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 137 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.168\n",
      "Epoch: 138 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 139 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 140 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 141 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 142 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.164\n",
      "Epoch: 143 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 144 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 145 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 146 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 147 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 148 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 149 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 150 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 151 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 152 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 153 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 154 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 155 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 156 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 157 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 158 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 159 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 160 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 161 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 162 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 163 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 164 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 165 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 166 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 167 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 168 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 169 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 170 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 171 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 172 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 173 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 174 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 175 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 176 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 177 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 178 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 179 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 180 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 181 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 182 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 183 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 184 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 185 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 186 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 187 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 188 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 189 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 190 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 191 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 192 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 193 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 194 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 195 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 196 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 197 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 198 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 199 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 200 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 201 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 202 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 203 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 204 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 205 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 206 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 207 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 208 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 209 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 210 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 211 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 212 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 213 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 214 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 215 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 216 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 217 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 218 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 219 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 220 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 221 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 222 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 223 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 224 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 225 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 226 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 227 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 228 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 229 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 230 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 231 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 232 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 233 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 234 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 235 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 236 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 237 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 238 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 239 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 240 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 241 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 242 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 243 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 244 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 245 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 246 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 247 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 248 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 249 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 250 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 251 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 252 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 253 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 254 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 255 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 256 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 257 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 258 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 259 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 260 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 261 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 262 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 263 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 264 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 265 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 266 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 267 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 268 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 269 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 270 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 271 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 272 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 273 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 274 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 275 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 276 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 277 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 278 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 279 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 280 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 281 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 282 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 283 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 284 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 285 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 286 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 287 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 288 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 289 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 290 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 291 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 292 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 293 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 294 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 295 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 296 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 297 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 298 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 299 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 300 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 301 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 302 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 303 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 305 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 308 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 314 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 315 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 316 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 318 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 321 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 326 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 328 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 330 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 331 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 332 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 333 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 335 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 336 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 337 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 340 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 341 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 342 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 343 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 344 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 347 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 348 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 349 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 350 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 351 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 352 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 353 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 354 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 355 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 356 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 360 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 362 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 363 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 364 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 369 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 374 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 375 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 376 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 378 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 379 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 380 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 381 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 382 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 383 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 384 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 385 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 387 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 390 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 392 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s | R3 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "\n",
      "SCHEDULE: Ensembler-1.s2.t2.e400\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.355 | Train PPL:   1.426\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 02 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.311 |  Val. PPL:   1.364\n",
      "Epoch: 03 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.311 |  Val. PPL:   1.364\n",
      "Epoch: 04 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.311 |  Val. PPL:   1.364\n",
      "Epoch: 05 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 06 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 07 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 08 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 09 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 10 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 11 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 12 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 13 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 14 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 15 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 16 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 17 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 18 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 19 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 20 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 21 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 22 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 23 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 24 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 25 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 26 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 27 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 28 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 29 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 30 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 31 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 32 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 33 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 34 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 35 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 36 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 37 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 38 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 39 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 40 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 41 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 42 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 43 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 44 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 45 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 46 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 47 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 48 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 49 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 50 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 51 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 52 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 53 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 54 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 55 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 56 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 57 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 58 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 59 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 60 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 61 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 62 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 63 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 64 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 65 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 66 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 67 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 68 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 69 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 70 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 71 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 72 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 73 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 74 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 75 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 76 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 77 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 78 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 79 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 80 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 81 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 82 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 83 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 84 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 85 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 86 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 87 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 88 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 89 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 90 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 91 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 92 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 93 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 94 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 95 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 96 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 97 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 98 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 99 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 100 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 101 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 102 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 103 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 104 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 105 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 106 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 107 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 108 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 109 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 110 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 111 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 112 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 113 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 114 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 115 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 116 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 117 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 118 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 119 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 120 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 121 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 122 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 123 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 124 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 125 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 126 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 127 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 128 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 129 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 130 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 131 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 132 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 133 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 134 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 135 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 136 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 137 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 138 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 139 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 140 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 141 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 142 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 143 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 144 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 145 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 146 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 147 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 148 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 149 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 150 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 151 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 152 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 153 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 154 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 155 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 156 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 157 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 158 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 159 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 160 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 161 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 162 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 164 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 165 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 166 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 167 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 168 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 169 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 170 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 171 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 201 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 202 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 203 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 204 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 205 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 206 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 207 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 208 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 209 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 210 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 211 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 212 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 213 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 214 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 215 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 216 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 217 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 218 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 219 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 220 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 221 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 222 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 223 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 224 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 225 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 226 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 227 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 228 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 229 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 230 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 231 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 232 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 233 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 234 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 235 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 236 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 237 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 238 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 239 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 240 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 241 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 242 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 243 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 244 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 245 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 246 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 247 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 248 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 249 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 250 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 251 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 252 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 253 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 254 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 255 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 256 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 257 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 258 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 259 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 260 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 261 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 262 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 263 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 264 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 265 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 266 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 267 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 268 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 269 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 270 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 271 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 272 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 273 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 274 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 275 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 276 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 277 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 278 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 279 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 280 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 281 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 282 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 283 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 284 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 285 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 286 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 287 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 288 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 289 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 290 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 291 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 292 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 293 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 294 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 295 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 296 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 297 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 298 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 299 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 300 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 301 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 302 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 303 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 305 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 308 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 315 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 316 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 318 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 321 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 326 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 328 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 330 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 331 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 332 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 333 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 335 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 336 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 337 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 340 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 341 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 342 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 343 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 344 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 347 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 348 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 349 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 350 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 352 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 353 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 354 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 355 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 356 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 360 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 362 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 363 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 369 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 374 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 375 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 376 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 379 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 385 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 390 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 392 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s | R3 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "@@@@@@@@@ Repetition   4 @@@@@@@@@\n",
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(19, 9, bidirectional=True)\n",
      "        (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "          (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(37, 9)\n",
      "        (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: Ensembler-1.s0.t0.e400\n",
      "Epoch: 01 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.975 | Train PPL:   2.652\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 02 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.964 | Train PPL:   2.623\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 03 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.977 | Train PPL:   2.657\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 04 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.973 | Train PPL:   2.646\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 05 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.975 | Train PPL:   2.652\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 06 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.961 | Train PPL:   2.613\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 07 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.954 | Train PPL:   2.595\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 08 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.954 | Train PPL:   2.597\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 09 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.962 | Train PPL:   2.616\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 10 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.988 | Train PPL:   2.685\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 11 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.981 | Train PPL:   2.666\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 12 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.956 | Train PPL:   2.601\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 13 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.984 | Train PPL:   2.674\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 14 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 1.016 | Train PPL:   2.763\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 15 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.973 | Train PPL:   2.647\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 16 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 1.005 | Train PPL:   2.731\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 17 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.997 | Train PPL:   2.709\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 18 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.976 | Train PPL:   2.653\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 19 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.971 | Train PPL:   2.642\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 20 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.962 | Train PPL:   2.618\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 21 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.986 | Train PPL:   2.680\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 22 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.960 | Train PPL:   2.612\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 23 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.974 | Train PPL:   2.647\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 24 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.924 | Train PPL:   2.519\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 25 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.962 | Train PPL:   2.617\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 26 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.961 | Train PPL:   2.615\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 27 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.981 | Train PPL:   2.666\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 28 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.982 | Train PPL:   2.670\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 29 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.976 | Train PPL:   2.653\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 30 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 1.039 | Train PPL:   2.827\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "Epoch: 31 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.956 | Train PPL:   2.601\n",
      "\t Val. Loss: 0.997 |  Val. PPL:   2.711\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.658 | Train PPL:   1.930\n",
      "\t Val. Loss: 0.539 |  Val. PPL:   1.714\n",
      "Epoch: 33 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.490 | Train PPL:   1.633\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.552\n",
      "Epoch: 34 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.448 | Train PPL:   1.566\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.537\n",
      "Epoch: 35 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 36 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.391 | Train PPL:   1.478\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 37 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.466\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.463\n",
      "Epoch: 38 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.506\n",
      "Epoch: 39 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 40 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.499\n",
      "Epoch: 41 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 42 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.526\n",
      "Epoch: 43 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 44 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.440\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 45 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.433\n",
      "Epoch: 46 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.411\n",
      "Epoch: 47 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 48 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n",
      "Epoch: 49 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 50 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 51 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.393\n",
      "Epoch: 52 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.320\n",
      "Epoch: 53 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
      "\t Val. Loss: 0.320 |  Val. PPL:   1.377\n",
      "Epoch: 54 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.457\n",
      "Epoch: 55 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 56 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 57 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
      "Epoch: 58 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 59 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 60 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 61 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 62 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 63 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 64 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.226\n",
      "Epoch: 65 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 66 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 67 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 68 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 69 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 70 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 71 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 72 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 73 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 74 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 75 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 76 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 77 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 78 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 79 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 80 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 81 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.188\n",
      "Epoch: 82 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 83 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 84 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 85 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 86 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 87 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.182\n",
      "Epoch: 88 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 89 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 90 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 91 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 92 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 93 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 94 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 95 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 96 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 97 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 98 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 99 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 100 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 101 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 102 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 103 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 104 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 105 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 106 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 107 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 108 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.154\n",
      "Epoch: 109 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 110 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 111 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 112 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 113 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 114 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 115 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 116 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 117 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 118 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 119 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 120 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 121 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 122 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 123 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 124 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 125 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 126 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 127 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 128 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 129 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 130 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 131 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 132 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 133 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 134 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 135 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 136 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 137 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 138 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 139 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 140 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 141 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 142 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 143 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 144 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 145 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 146 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 147 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 148 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 149 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 150 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 151 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 152 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 153 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 154 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 155 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 156 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 157 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 158 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 159 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 160 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 161 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 162 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 163 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 164 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 165 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 166 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 167 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 168 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 169 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 170 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 171 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 172 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 173 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 174 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 175 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 176 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 177 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 180 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 181 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 183 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 184 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 186 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 187 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 189 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 190 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 191 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 192 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 193 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 196 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 200 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 201 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 202 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 203 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 204 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 205 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 206 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 207 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 208 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 209 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 210 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 211 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 212 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 213 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 214 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 215 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 216 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 217 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 218 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 219 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 220 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 221 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 222 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 223 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 224 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 225 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 226 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 227 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 228 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 229 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 230 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 231 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 232 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 233 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 234 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 235 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 236 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 237 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 238 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 239 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 240 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 241 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 242 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 243 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 244 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 245 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 246 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 247 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 248 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 249 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 250 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 251 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 252 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 253 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 254 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 255 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 256 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 257 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 258 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 259 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 260 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 261 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 262 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 263 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 264 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 265 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 266 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 267 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 268 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 269 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 270 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 271 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 272 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 273 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 274 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 275 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 276 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 277 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 278 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 279 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 280 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 281 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 282 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 283 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 284 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 285 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 286 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 287 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 288 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 289 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 290 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 291 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 292 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 293 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 294 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 295 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 296 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 297 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 298 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 299 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 300 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 301 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 302 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 303 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 304 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 305 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 308 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 313 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 315 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 316 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 318 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 319 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 321 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 326 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 328 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 330 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 331 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 332 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 333 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 334 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 335 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 336 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 337 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 338 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 340 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 341 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 342 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 343 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 344 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 346 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 347 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 348 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 349 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 350 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 351 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 352 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 353 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 354 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 355 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 356 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 357 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 358 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 359 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 360 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 361 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 362 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 363 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 366 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 369 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 374 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 375 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 376 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 377 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 378 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 379 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 380 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 381 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 382 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 383 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 384 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 385 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 386 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 387 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 388 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 389 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 390 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 391 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 392 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 393 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 394 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 395 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 396 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s | R4 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "SCHEDULE: Ensembler-1.s1.t1.e400\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.718 | Train PPL:   2.050\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 02 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.666 | Train PPL:   1.946\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 03 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.683 | Train PPL:   1.980\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 04 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.645 | Train PPL:   1.906\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 05 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.593 | Train PPL:   1.810\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 06 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.616 | Train PPL:   1.852\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 07 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.705 | Train PPL:   2.024\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 08 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.680 | Train PPL:   1.973\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 09 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.593 | Train PPL:   1.809\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 10 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.646 | Train PPL:   1.908\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 11 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.605 | Train PPL:   1.832\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 12 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.614 | Train PPL:   1.847\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 13 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.619 | Train PPL:   1.856\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 14 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.652 | Train PPL:   1.919\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 15 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.609 | Train PPL:   1.839\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 16 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.636 | Train PPL:   1.890\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 17 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.648 | Train PPL:   1.912\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 18 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.665 | Train PPL:   1.945\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 19 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.644 | Train PPL:   1.905\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 20 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.603 | Train PPL:   1.828\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 21 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.555 | Train PPL:   1.741\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 22 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.732 | Train PPL:   2.079\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 23 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.659 | Train PPL:   1.932\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 24 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.691 | Train PPL:   1.996\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 25 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.620 | Train PPL:   1.859\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 26 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.634 | Train PPL:   1.886\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 27 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.614 | Train PPL:   1.848\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 28 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.614 | Train PPL:   1.848\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 29 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.630 | Train PPL:   1.877\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 30 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.561 | Train PPL:   1.753\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 31 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.597 | Train PPL:   1.817\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "-----------------------------------\n",
      "-----Added Expert-train Gating-----\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.588 | Train PPL:   1.800\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 33 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.635 | Train PPL:   1.888\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 34 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.652 | Train PPL:   1.920\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 35 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.626 | Train PPL:   1.871\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 36 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.584 | Train PPL:   1.793\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 37 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.573 | Train PPL:   1.773\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 38 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.686 | Train PPL:   1.987\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 39 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.672 | Train PPL:   1.958\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 40 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.708 | Train PPL:   2.030\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 41 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.640 | Train PPL:   1.896\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 42 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.634 | Train PPL:   1.886\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 43 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.652 | Train PPL:   1.920\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 44 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.705 | Train PPL:   2.024\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 45 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.670 | Train PPL:   1.954\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 46 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.667 | Train PPL:   1.948\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 47 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.606 | Train PPL:   1.832\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 48 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.603 | Train PPL:   1.827\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 49 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.717 | Train PPL:   2.049\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 50 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.637 | Train PPL:   1.891\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 51 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.706 | Train PPL:   2.025\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 52 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.645 | Train PPL:   1.906\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 53 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.587 | Train PPL:   1.798\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 54 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.644 | Train PPL:   1.904\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 55 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.575 | Train PPL:   1.777\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 56 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.682 | Train PPL:   1.977\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 57 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.625 | Train PPL:   1.867\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 58 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.706 | Train PPL:   2.026\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 59 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.603 | Train PPL:   1.828\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 60 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.698 | Train PPL:   2.011\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 61 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.644 | Train PPL:   1.903\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 62 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.637 | Train PPL:   1.891\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 63 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.660 | Train PPL:   1.934\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 64 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.484 | Train PPL:   1.622\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   1.999\n",
      "Epoch: 65 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.432 | Train PPL:   1.541\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.506\n",
      "Epoch: 66 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.508\n",
      "Epoch: 67 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.415 |  Val. PPL:   1.514\n",
      "Epoch: 68 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.401 | Train PPL:   1.493\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 69 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.384 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.466\n",
      "Epoch: 70 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.387 | Train PPL:   1.473\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 71 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 72 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.460\n",
      "Epoch: 73 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.447\n",
      "Epoch: 74 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 75 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.404 |  Val. PPL:   1.499\n",
      "Epoch: 76 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 77 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 78 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.463\n",
      "Epoch: 79 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 80 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 81 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 82 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.337\n",
      "Epoch: 83 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 84 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 85 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 86 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 87 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 88 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 89 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 90 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 91 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 92 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 93 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 94 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 95 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 96 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 97 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
      "Epoch: 98 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.242\n",
      "Epoch: 99 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 100 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 101 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 102 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 103 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 104 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 105 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 106 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 107 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 108 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 109 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 110 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 111 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.215\n",
      "Epoch: 112 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 113 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 114 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 115 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 116 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.194\n",
      "Epoch: 117 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 118 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 119 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 120 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 121 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 122 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 123 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 124 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 125 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 126 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 127 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 128 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 129 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 130 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 131 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 132 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 133 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 134 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 135 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 136 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 137 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 138 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 139 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 140 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 141 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 142 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 143 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 144 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 145 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 146 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 147 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 148 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 149 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 150 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 151 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 152 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 153 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 154 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 155 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 156 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 157 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 158 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 159 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 160 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 161 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 162 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 163 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 164 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 165 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 166 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 167 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 168 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 169 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 170 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 171 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 172 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 173 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 174 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 175 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 176 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 177 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 178 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 179 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 180 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 182 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 186 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 187 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 188 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 189 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 190 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 191 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 192 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 193 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 194 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 195 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 196 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 197 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 198 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 199 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 200 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 201 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 202 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 203 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 204 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 205 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 206 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 207 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 208 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 209 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 210 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 211 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 212 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 213 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 214 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 215 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 216 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 217 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 218 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 219 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 220 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 221 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 222 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 223 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 224 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 225 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 226 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 227 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 228 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 229 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 230 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 231 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 232 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 233 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 234 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 235 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 236 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 237 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 238 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 239 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 240 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 241 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 242 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 243 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 244 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 245 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 246 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 247 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 248 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 249 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 250 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 251 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 252 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 253 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 254 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 255 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 256 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 257 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 258 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 259 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 260 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 261 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 262 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 263 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 264 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 265 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 266 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 267 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 268 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 269 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 270 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 271 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 272 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 273 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 274 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 275 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 276 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 277 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 278 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 279 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 280 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 281 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 282 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 283 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 284 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 285 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 286 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 287 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 288 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 289 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 290 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 291 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 292 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 293 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 294 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 295 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 296 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 297 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 298 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 299 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 300 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 301 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 302 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 303 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 305 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 308 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 315 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 316 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 317 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 318 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 321 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 324 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 326 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 327 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 328 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 330 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 331 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 332 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 333 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 335 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 336 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 337 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 340 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 341 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 342 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 343 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 344 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 345 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 347 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 348 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 349 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 350 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 352 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 353 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 354 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 355 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 356 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 357 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 360 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 361 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 362 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 363 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 364 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 366 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 367 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 368 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 369 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 370 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 371 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 372 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 373 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 374 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 375 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 376 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 377 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 378 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 379 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 380 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 381 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 382 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 383 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 384 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 385 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 386 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 387 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 388 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 389 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 390 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 391 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 392 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 393 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 394 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 395 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 396 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 397 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 398 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 399 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 400 | Time: 0m 0s | R4 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "\n",
      "SCHEDULE: Ensembler-1.s2.t2.e400\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.448 | Train PPL:   1.566\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 02 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 03 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.414 | Train PPL:   1.513\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 04 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 05 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 06 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 07 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 08 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 09 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 10 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 11 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 12 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 13 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 14 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 15 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 16 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 17 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 18 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 19 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 20 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 21 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 22 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 23 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 24 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 25 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 26 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 27 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 28 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 29 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 30 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 31 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 32 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 33 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 34 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 35 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 36 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 37 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 38 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 39 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 40 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 41 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 42 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 43 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 44 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 45 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 46 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 47 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 48 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 49 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 50 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 51 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 52 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 53 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 54 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 55 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 56 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 57 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 58 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 59 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 60 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 61 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 62 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 63 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 64 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 65 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 66 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 67 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 68 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 69 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 70 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 71 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 72 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 73 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 74 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 75 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 76 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 77 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 78 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 79 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 80 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 81 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 82 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 83 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 84 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 85 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 86 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 87 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 88 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 89 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 90 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 91 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 93 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 94 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 95 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 96 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 97 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 98 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 99 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 100 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 101 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 102 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 103 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 104 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 105 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 106 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 107 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 108 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 109 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 110 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 111 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 112 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 113 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 114 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 115 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 116 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 117 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 118 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 119 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 120 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 121 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 122 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 123 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 124 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 125 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 126 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 127 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 128 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 129 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 130 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 131 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 132 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 133 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 134 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 135 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 136 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 137 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 138 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 139 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 140 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 141 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 142 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 143 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 144 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 145 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 146 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 147 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 148 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 149 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 150 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 151 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 152 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 153 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 155 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 162 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 179 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 186 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 189 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 201 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 202 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 203 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 204 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 205 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 206 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 207 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 208 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 209 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 210 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 211 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 212 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 213 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 214 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 215 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 216 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 217 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 218 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 219 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 220 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 221 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 222 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 223 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 224 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 225 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 226 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 227 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 228 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 229 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 230 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 231 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 232 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 233 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 234 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 235 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 236 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 237 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 238 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 239 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 240 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 241 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 242 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 243 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 244 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 245 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 246 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 247 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 248 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 249 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 250 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 251 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 252 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 253 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 254 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 255 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 256 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 257 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 258 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 259 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 260 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 261 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 262 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 263 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 264 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 265 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 266 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 267 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 268 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 269 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 270 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 271 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 272 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 273 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 274 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 275 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 276 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 277 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 278 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 279 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 280 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 281 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 282 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 283 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 284 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 285 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 286 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 287 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 288 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 289 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 290 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 291 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 292 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 293 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 294 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 295 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 296 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 297 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 298 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 299 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 300 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 301 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 302 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 303 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 304 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 305 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 306 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 307 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 308 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 309 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 310 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 311 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 312 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 313 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 314 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 315 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 316 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 317 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 318 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 319 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 320 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 321 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 322 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 323 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 324 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 325 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 326 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 327 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 328 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 329 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 330 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 331 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 332 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 333 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 334 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 335 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 336 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 337 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 338 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 339 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 340 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 341 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 342 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 343 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 344 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 345 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 346 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 347 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 348 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 349 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 350 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 351 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 352 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 353 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 354 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 355 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 356 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 357 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 358 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 359 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 360 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 361 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 362 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 363 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 364 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 365 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 366 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 367 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 368 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 369 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 370 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 371 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 372 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 373 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 374 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 375 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 376 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 377 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 378 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 379 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 380 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 381 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 382 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 383 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 384 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 385 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 386 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 387 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 388 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 389 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 390 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 391 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 392 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 393 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 394 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 395 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 396 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 397 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 398 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 399 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 400 | Time: 0m 0s | R4 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = 5\n",
    "hist_all_losses_F, hist_all_hitsss_F, models_F = experiment(\n",
    "    \"Ensembler-1\",\n",
    "    n_repetitions,\n",
    "    SCHEDULE,\n",
    "    init_dynamoe,\n",
    "    repeat_dynamoe,\n",
    "    STEP_SIZE_EVALUATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_F, SCHEDULE, STEP_SIZE_EVALUATION, datasets=(0,1), figsize=(12,7), save=True, path=PLOTSFOLDER+\"/showtyler21.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_F[2].n_active_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.3983,  5.1391, -1.3611]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "for seqs, seqs_len in train_dls[1]:\n",
    "    print(models_F[2].gating(seqs, seqs_len))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22.0830, -0.2584]) - tensor([1, 3, 7, 5, 4, 5, 2])\n",
      "tensor([21.2003, -0.2421]) - tensor([1, 3, 5, 4, 5, 6, 2])\n",
      "tensor([20.5874, -0.2038]) - tensor([1, 3, 5, 6, 4, 2])\n",
      "tensor([21.1779, -0.2220]) - tensor([1, 3, 5, 4, 2])\n",
      "tensor([21.1373, -0.2487]) - tensor([1, 3, 7, 5, 6, 4, 5, 6, 2])\n",
      "tensor([22.0578, -0.2568]) - tensor([1, 3, 7, 5, 4, 2])\n",
      "tensor([22.0680, -0.2647]) - tensor([1, 3, 7, 5, 4, 5, 6, 2])\n",
      "tensor([20.6297, -0.2455]) - tensor([1, 3, 5, 6, 4, 5, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "show_expert2(model, train_dls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "TCvCl-WOjvYN"
   },
   "outputs": [],
   "source": [
    "expert, expert_optimizer = init_expert()\n",
    "\n",
    "gating, gating_optimizer = init_gating()\n",
    "\n",
    "model2 = Ensembler(gating, gating_optimizer, [expert,], [expert_optimizer,])\n",
    "\n",
    "model_optimizer = optim.Adam(model2.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09185882657766342\n",
      "0.09303067624568939\n",
      "0.09172848239541054\n",
      "0.09473882988095284\n",
      "0.09154968708753586\n",
      "0.09308598190546036\n",
      "0.09350227937102318\n",
      "0.09345363080501556\n",
      "0.09185237810015678\n",
      "0.09177808463573456\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(train_ensembler_gating(model, train_dls[2], criterion, CLIP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [3],\n",
      "        [7],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "for seqs, seqs_len in train_dls[1]:\n",
    "    print(model2.gating(seqs, seqs_len))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.2788e+01,  4.3714e-02, -1.4698e-02]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [7],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2]])\n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [5],\n",
      "        [7],\n",
      "        [4],\n",
      "        [2]])\n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [5],\n",
      "        [7],\n",
      "        [4],\n",
      "        [2]])\n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [7],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2]])\n",
      "tensor([[1],\n",
      "        [3],\n",
      "        [7],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "for seqs, seqs_len in train_dls[0]:\n",
    "    print(model.gating(seqs, seqs_len))\n",
    "    print(model.experts[0](seqs, seqs_len, seqs).argmax(dim=2))\n",
    "    print(model.experts[1](seqs, seqs_len, seqs).argmax(dim=2))\n",
    "    print(model(seqs, seqs_len, seqs).argmax(dim=2))\n",
    "    print((model.experts[0](seqs, seqs_len, seqs) * model.gating(seqs, seqs_len)[0][0] + model.experts[1](seqs, seqs_len, seqs) * model.gating(seqs, seqs_len)[0][1]).argmax(dim=2) )\n",
    "    print(seqs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_active_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16881714761257172\n",
      "0.15141833573579788\n",
      "0.15692844986915588\n",
      "0.15707916021347046\n",
      "0.15791257470846176\n",
      "0.18426920473575592\n",
      "0.15594960004091263\n",
      "0.2099481225013733\n",
      "0.19191593676805496\n",
      "0.16780265420675278\n",
      "0.16559413820505142\n",
      "0.14507802575826645\n",
      "0.14517706632614136\n",
      "0.15876172482967377\n",
      "0.18021392822265625\n",
      "0.14854232966899872\n",
      "0.17011234909296036\n",
      "0.19770054519176483\n",
      "0.15062608569860458\n",
      "0.16328270733356476\n",
      "0.1630496382713318\n",
      "0.15208538621664047\n",
      "0.1585157811641693\n",
      "0.1476331725716591\n",
      "0.19418861716985703\n",
      "0.14303864538669586\n",
      "0.15178367495536804\n",
      "0.13727358728647232\n",
      "0.14693152904510498\n",
      "0.16229414194822311\n",
      "0.16068880259990692\n",
      "0.13847041130065918\n",
      "0.17334415018558502\n",
      "0.15148300677537918\n",
      "0.16502273082733154\n",
      "0.14539223909378052\n",
      "0.12727899849414825\n",
      "0.1345638856291771\n",
      "0.14078588038682938\n",
      "0.16145791113376617\n",
      "0.1524231731891632\n",
      "0.1419893577694893\n",
      "0.12537699937820435\n",
      "0.14602867513895035\n",
      "0.14317089319229126\n",
      "0.16259494423866272\n",
      "0.17060024291276932\n",
      "0.1443256139755249\n",
      "0.13479702174663544\n",
      "0.15025698393583298\n",
      "0.14225276559591293\n",
      "0.16034740954637527\n",
      "0.12438227236270905\n",
      "0.12518545240163803\n",
      "0.14440368115901947\n",
      "0.13735894113779068\n",
      "0.19062084704637527\n",
      "0.13652129471302032\n",
      "0.17650730162858963\n",
      "0.14275554567575455\n",
      "0.16762355715036392\n",
      "0.14181502908468246\n",
      "0.1351715624332428\n",
      "0.12215232849121094\n",
      "0.1263386607170105\n",
      "0.11908707767724991\n",
      "0.12115544825792313\n",
      "0.1282174363732338\n",
      "0.1253407672047615\n",
      "0.12408994138240814\n",
      "0.14472167938947678\n",
      "0.128596231341362\n",
      "0.12068396806716919\n",
      "0.12849827855825424\n",
      "0.1212557777762413\n",
      "0.1384652704000473\n",
      "0.172116719186306\n",
      "0.11886386573314667\n",
      "0.11503040790557861\n",
      "0.1102377399802208\n",
      "0.11419399827718735\n",
      "0.13094515353441238\n",
      "0.11548421531915665\n",
      "0.12197885662317276\n",
      "0.11363319307565689\n",
      "0.11196141690015793\n",
      "0.11577392369508743\n",
      "0.11174105852842331\n",
      "0.1156032532453537\n",
      "0.1134130209684372\n",
      "0.12646576017141342\n",
      "0.12065742164850235\n",
      "0.11694453656673431\n",
      "0.13224681466817856\n",
      "0.10931586474180222\n",
      "0.13207688927650452\n",
      "0.123417429625988\n",
      "0.10937980562448502\n",
      "0.12781494110822678\n",
      "0.11629518866539001\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    print(train_dynamoe2_both(model2, train_dls[1], criterion, CLIP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "pJE4efjNcUCM"
   },
   "outputs": [],
   "source": [
    "model2.add_expert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVuFGE3vj6K-",
    "outputId": "80edc0df-4dcb-4f7c-f48f-6e818ad0a2c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [5],\n",
      "        [3],\n",
      "        [6],\n",
      "        [7],\n",
      "        [4],\n",
      "        [6],\n",
      "        [7],\n",
      "        [2]])\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 1.6035e-04, -3.0651e-03, -3.0967e+00,  5.0364e+00, -2.0193e+00,\n",
      "           1.1399e+01, -1.7679e+00,  4.7675e+00]],\n",
      "\n",
      "        [[-1.7267e-02, -1.7725e-02, -2.5892e+00,  8.6882e+00, -3.9250e-02,\n",
      "           2.2423e+00,  2.9604e+00,  1.6000e+00]],\n",
      "\n",
      "        [[-6.3461e-03, -2.2191e-02, -3.4556e+00,  2.0296e-01,  1.4208e-01,\n",
      "          -6.5787e-02,  1.4856e+01,  2.9764e+00]],\n",
      "\n",
      "        [[-1.5827e-02, -6.2566e-03, -2.2252e+00,  1.3526e+00,  6.0539e+00,\n",
      "          -3.2105e-01,  1.5760e+00,  7.5781e+00]],\n",
      "\n",
      "        [[-1.3362e-02,  1.7968e-02,  3.8569e+00, -1.9823e+00,  8.1386e+00,\n",
      "           1.5963e+00, -1.1647e-02,  1.3524e+00]],\n",
      "\n",
      "        [[-5.1290e-03, -1.0336e-02,  3.7318e+00,  1.4661e+00, -1.7547e+00,\n",
      "          -1.2773e+00,  1.0091e+01,  2.9304e+00]],\n",
      "\n",
      "        [[-6.9810e-03,  1.6720e-02,  7.0399e+00, -3.2432e-01, -3.1907e+00,\n",
      "           1.1463e-01,  1.1009e+00,  1.0808e+01]],\n",
      "\n",
      "        [[-1.7517e-02,  9.3911e-03,  1.1679e+01, -1.5853e+00, -6.2468e-01,\n",
      "           1.4161e+00, -1.5484e+00,  6.6679e+00]]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for seqs, seqs_len in train_dls[0]:\n",
    "    print(seqs)\n",
    "    print(model2(seqs, seqs_len, seqs))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer H: Adaptive ensembling - Ansembling\n",
    "\n",
    "1. Compute Model loss\n",
    "2. Compute Training decision with Gating\n",
    "3. Based on Model loss \"question\" training decision\n",
    "4. Train chosen expert, at the same time compute expert losses\n",
    "5. Use ensembled expert losses to train Gating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_ansembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ansembler(model, iterator, criterion, clip, verbose=False):\n",
    "\n",
    "    model.gating.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        #seqs = [seq len, batch size]\n",
    "        #output = [seq len, batch size, vocab sizen]\n",
    "\n",
    "        seq_len, batch_size = seqs.shape\n",
    "        vocab_size = model.gating.input_dim\n",
    "        \n",
    "        # 1. Compute model loss\n",
    "        model_outputs = model.forward(seqs, seqs_len, seqs)\n",
    "        \n",
    "        model_loss = compute_loss(model_outputs, seqs,\n",
    "                                  criterion,\n",
    "                                  cutFirstInSequence=True)\n",
    "        \n",
    "        \n",
    "        # 2. Compute training choices (@TODO: more efficient, integrate return in first forward pass)\n",
    "        model.gating_optimizer.zero_grad()\n",
    "        gating_outputs = model.gating(seqs, seqs_len)\n",
    "        # gating_outputs = [batch_size, n_max_experts]\n",
    "        \n",
    "        gating_masked = gating_outputs[:,:model.n_active_experts]\n",
    "        \n",
    "        training_choices = torch.argmax(gating_masked, dim=1)\n",
    "        # training_choices = [batch_size]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"training_choices\", training_choices)\n",
    "        # 3. \"Question\" choices\n",
    "        # Roll dice depending on error for every batch content\n",
    "        training_override = torch.empty(batch_size)\n",
    "        for b in range(batch_size):\n",
    "            rand = random.random()\n",
    "\n",
    "            if verbose:\n",
    "                print(\"model_loss\", model_loss)\n",
    "            \n",
    "            # @TODO: get rid of arbitrary scalar...\n",
    "            if (rand < model_loss * MULT_LOSS_SCALAR_QUESTIONING\n",
    "                and model.n_active_experts > 1):\n",
    "                randchoice = random.randint(0, model.n_active_experts - 2)\n",
    "                # make sure different choice than original choice\n",
    "                training_override[b] = randchoice\n",
    "                if randchoice >= training_choices[b]:\n",
    "                    training_override[b] += 1\n",
    "            else:\n",
    "                training_override[b] = training_choices[b]\n",
    "        if verbose:\n",
    "            print(\"training_override\", training_override)\n",
    "        \n",
    "        # 4. Train expert on resulting choice\n",
    "        # Collect expert outputs at the same time\n",
    "        loss_experts = torch.empty((batch_size, model.n_active_experts))\n",
    "        # loss_experts = [batch_size, n_active_experts]\n",
    "        \n",
    "        expert_outputs = torch.empty((model.n_active_experts, seq_len,\n",
    "                                      batch_size, vocab_size))\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            \n",
    "            train_id = int(training_override[b])\n",
    "            \n",
    "            for e_id in range(model.n_active_experts):\n",
    "                \n",
    "                seq_curr = seqs[:,b].unsqueeze(1)\n",
    "                seq_len_curr = seqs_len[b].unsqueeze(0)\n",
    "            \n",
    "                if train_id == e_id:\n",
    "                    model.experts[e_id].train()\n",
    "                    model.expert_optimizers[e_id].zero_grad()\n",
    "\n",
    "                    # Get model prediction\n",
    "                    expert_output = model.experts[e_id](seq_curr,\n",
    "                                                        seq_len_curr,\n",
    "                                                        seq_curr)\n",
    "\n",
    "                    loss = compute_loss(expert_output, seq_curr,\n",
    "                                        criterion,\n",
    "                                        cutFirstInSequence=True)\n",
    "\n",
    "                    loss.backward()\n",
    "                    model.expert_optimizers[e_id].step()\n",
    "\n",
    "                else:\n",
    "                    model.experts[e_id].eval()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "\n",
    "                        # Get model prediction\n",
    "                        expert_output = model.experts[e_id](seq_curr,\n",
    "                                                            seq_len_curr,\n",
    "                                                            seq_curr)\n",
    "\n",
    "                # Log expert_outputs multitplied with gating\n",
    "                expert_outputs[e_id,:,b] = (expert_output[:,b].detach()\n",
    "                                            * gating_masked[b,e_id])\n",
    "            \n",
    "            model.n_train_on_experts[train_id] += 1\n",
    "            model.expert_schedulers[train_id].step()\n",
    "\n",
    "        # 5. Train Gating\n",
    "        # Compute ensembled output\n",
    "        weighted_outputs = expert_outputs.sum(dim=0)\n",
    "        # weighted_outputs = [seqs_len, batch_size, vocab_size]\n",
    "        \n",
    "        # Gating Loss just is total loss\n",
    "        total_loss = compute_loss(weighted_outputs, seqs, criterion,\n",
    "                                  cutFirstInSequence=True)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\">> Total Loss(used for gating)\")\n",
    "            print(total_loss)\n",
    "\n",
    "        total_loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        model.gating_optimizer.step()\n",
    "        \n",
    "        epoch_loss += total_loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replay_ansembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_ansembler(model, task_id, expert_criterion, clip, verbose=False):\n",
    "    for other_id in range(task_id):\n",
    "        train_ensembler_gating(model, train_dls[other_id],\n",
    "                               expert_criterion,\n",
    "                               clip, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_ansembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ansembler(\n",
    "    n_tasks_total,\n",
    "    model,\n",
    "    task_id,\n",
    "    n_task_epochs,\n",
    "    step_size_evaluation,\n",
    "    expert_criterion,\n",
    "    clip=1,\n",
    "    repetition=None,\n",
    "    verbose=False\n",
    "):\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((n_tasks_total, 3, n_task_epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((n_tasks_total, 3, n_task_epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "    \n",
    "    for epoch in range(n_task_epochs):\n",
    "        # First Epoch log performance BEFORE training\n",
    "        if epoch == 0:\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, 0, expert_criterion)\n",
    "        \n",
    "        # If training does not lead anywhere, init new expert\n",
    "        if (model.epoch > model.allowed_until_check\n",
    "            and model.n_active_experts < model.n_max_experts\n",
    "           ):\n",
    "            init_expert = False\n",
    "            # Lookahead check whether data from another task is coming in\n",
    "            loss_ahead = evaluate_extra(model, valid_dls[task_id],\n",
    "                                        allOrNoneLoss)\n",
    "            last_loss = model.loss_tracker[-1]\n",
    "            if ((loss_ahead - PERFORMANCE_DIFFERENCE_NEW_TASK > last_loss)):\n",
    "                init_expert = True\n",
    "            \n",
    "            # Check whether learning is stagnating\n",
    "            # (difference to N_EPOCHS_UNTIL_NEW_EXPERT is small enough)\n",
    "            if (((model.loss_tracker[model.epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                     last_loss) < ALLOWED_ERROR_VARIANCE)\n",
    "                and \n",
    "                (model.valid_loss > PERFORMANCE_TRESHHOLD_START)\n",
    "               ):\n",
    "                init_expert = True\n",
    "                        \n",
    "            if init_expert:\n",
    "                # Add new expert\n",
    "                model.status = \"train_gating_uninitialized_expert\"\n",
    "                model.allowed_until_check = model.epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                model.add_expert()\n",
    "\n",
    "                print(\"-----------------------------------\")\n",
    "                print(\"--------- Init new Expert ---------\")\n",
    "                print(\"-----------------------------------\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train model\n",
    "        train_loss = train_ansembler(model, train_dls[task_id],\n",
    "                                     expert_criterion, clip, verbose)\n",
    "\n",
    "        model.valid_loss = evaluate(model, valid_dls[task_id],\n",
    "                                    expert_criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Log hits\n",
    "        model.loss_tracker.append(evaluate_extra(model, valid_dls[task_id],\n",
    "                                                 allOrNoneLoss))\n",
    "            \n",
    "#         if model.valid_loss < best_valid_loss:\n",
    "#             best_valid_loss = model.valid_loss\n",
    "#             torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        # Log performance AFTER training\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0 and epoch != 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, idx, expert_criterion)\n",
    "            \n",
    "        if epoch % STEP_SIZE_REPLAY == 0 and epoch != 0:\n",
    "            print(\"-Replay\")\n",
    "            replay_ansembler(model, task_id, expert_criterion, clip, verbose)\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if repetition is not None:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | R{repetition} T{task_id}')\n",
    "        else:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | T{task_id}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {model.valid_loss:.3f} |  Val. PPL: {math.exp(model.valid_loss):7.3f}')\n",
    "        \n",
    "        model.epoch += 1\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_ansembler():\n",
    "    # Initialize Ensembler\n",
    "    model = Ensembler(expert_decay=True)\n",
    "    print(model.apply(init_weights))\n",
    "\n",
    "    # Cosine loss works better for small datasets, thus used for experts\n",
    "    expert_criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)\n",
    "\n",
    "    return (model, expert_criterion)\n",
    "\n",
    "def repeat_ansembler(n_tasks_total, n_task_epochs, task_id, step_size_evaluation, repetition, pass_on_variables):\n",
    "    model, expert_criterion = pass_on_variables\n",
    "    \n",
    "    hist_loss, hist_hits = fit_ansembler(\n",
    "        n_tasks_total,\n",
    "        model,\n",
    "        task_id,\n",
    "        n_task_epochs,\n",
    "        step_size_evaluation,\n",
    "        expert_criterion,\n",
    "        repetition=repetition,\n",
    "        verbose=False\n",
    "    )\n",
    "    return hist_loss, hist_hits, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Ansembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "CvFmPpKQozmz"
   },
   "outputs": [],
   "source": [
    "N_EXPERTS_START = 1\n",
    "N_MAX_EXPERTS = 3\n",
    "GATE_DROPOUT = 0.5\n",
    "N_GATING_HIDDEN_DIM = 15\n",
    "N_GATING_EMBED_DIM = 9\n",
    "LEARNING_RATE_GATING = 0.005\n",
    "SCHEDULE = not_interleaved\n",
    "\n",
    "# If the amount of missed hits is bigger then PERFORMANCE_TRESHHOLD_START\n",
    "# and it stays within ALLOWED_ERROR_VARIANCE for\n",
    "# N_EPOCHS_UNTIL_NEW_EXPERT epochs, then a new\n",
    "# expert is initialized\n",
    "N_EPOCHS_UNTIL_NEW_EXPERT = 30\n",
    "ALLOWED_ERROR_VARIANCE = 0.05\n",
    "PERFORMANCE_TRESHHOLD_START = 0.4\n",
    "\n",
    "# Difference between replicated sequence accuracy of training sequence\n",
    "# before and training sequence coming in to decide to consolidate for\n",
    "# a new expert.\n",
    "PERFORMANCE_DIFFERENCE_NEW_TASK = 0.3 # 0.5 for bad example\n",
    "\n",
    "STEP_SIZE_REPLAY = 5\n",
    "STEP_SIZE_DECAY = 20\n",
    "GAMMA_DECAY = 0.95\n",
    "LEARNING_RATE = 0.05\n",
    "# need seperate learning rates for gating and experts\n",
    "\n",
    "TEST_ALL_TASKS = 1\n",
    "\n",
    "# When training choices are questioned, a random number\n",
    "# has to be less than the model loss times this factor\n",
    "MULT_LOSS_SCALAR_QUESTIONING = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "dUUt4knHYfDj"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRoGUVZcfgMg",
    "outputId": "2c57ceea-2add-4c03-f0b3-5a9252f522d0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@ Repetition   0 @@@@@@@@@\n",
      "Ensembler(\n",
      "  (gating): Gating_Ensembler(\n",
      "    (embedding): Embedding(8, 9)\n",
      "    (rnn): GRU(9, 15, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=30, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(19, 9, bidirectional=True)\n",
      "        (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "          (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(37, 9)\n",
      "        (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: Ansembler-1.s0.t0.e400\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.603 | Train PPL:   1.828\n",
      "\t Val. Loss: 0.502 |  Val. PPL:   1.651\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.483 | Train PPL:   1.620\n",
      "\t Val. Loss: 0.460 |  Val. PPL:   1.583\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.436 | Train PPL:   1.546\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.493\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.401 | Train PPL:   1.494\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "-Replay\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.482\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "-Replay\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.421\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "-Replay\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "-Replay\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "-Replay\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "-Replay\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "-Replay\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.232\n",
      "-Replay\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "-Replay\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.257\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "-Replay\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.217\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "-Replay\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.185\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "-Replay\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "-Replay\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "-Replay\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "-Replay\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "-Replay\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "-Replay\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "-Replay\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "-Replay\n",
      "Epoch: 101 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 102 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 103 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 104 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 105 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "-Replay\n",
      "Epoch: 106 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 107 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 108 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 109 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 110 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "-Replay\n",
      "Epoch: 111 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 112 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 113 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 114 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 115 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 116 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 117 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 118 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 119 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 120 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 121 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 122 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 123 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 124 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 125 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 126 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 127 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 128 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 129 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 130 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 131 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 132 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 133 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 134 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 135 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 136 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 137 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 138 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 139 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 140 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 141 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 142 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 143 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 144 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 145 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 146 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 147 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 148 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 149 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 150 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 151 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 152 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 153 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 154 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 155 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 156 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 157 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 158 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 159 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 160 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 161 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 162 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 163 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 164 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 165 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 166 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 167 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 168 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 169 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 170 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 171 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 172 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 173 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 174 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 175 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 176 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 177 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 178 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 179 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 181 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 183 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 184 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 185 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 186 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 187 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 188 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 189 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 190 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 191 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 192 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 193 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 194 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 196 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 197 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 198 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 199 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 200 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 201 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 202 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 203 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 204 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 205 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 206 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 207 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 208 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 209 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 210 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 211 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 212 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 213 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 214 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 215 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 216 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 217 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 218 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 219 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 220 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 221 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 222 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 223 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 224 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 225 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 226 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 227 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 228 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 229 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 230 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 231 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 232 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 233 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 234 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 235 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 236 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 237 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 238 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 239 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 240 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 241 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 242 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 243 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 244 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 245 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 246 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 247 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 248 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 249 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 250 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 251 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 252 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 253 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 254 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 255 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 256 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 257 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 258 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 259 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 260 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 261 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 262 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 263 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 264 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 265 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 266 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 267 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 268 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 269 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 270 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 271 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 272 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 273 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 274 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 275 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 276 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 277 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 278 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 279 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 280 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 281 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 282 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 283 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 284 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 285 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 286 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 287 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 288 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 289 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 290 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 291 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 292 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 293 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 294 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 295 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 296 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 297 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 298 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 299 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 300 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 301 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 302 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 303 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 304 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 305 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 306 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 307 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 308 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 309 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 310 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 311 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 312 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 313 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 314 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 315 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 316 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 317 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 318 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 319 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 320 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 321 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 322 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 323 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 324 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 325 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 326 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 327 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 328 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 329 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 330 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 331 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 332 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 333 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 334 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 335 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 336 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 337 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 338 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 339 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 340 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 341 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 342 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 343 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 344 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 345 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 346 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 347 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 348 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 349 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 350 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 351 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 352 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 353 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 354 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 355 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 356 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 357 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 358 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 359 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 360 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 361 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 362 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 363 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 364 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 365 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 366 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 367 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 368 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 369 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 370 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 371 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 372 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 373 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 374 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 375 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 376 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 377 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 378 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 379 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 380 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 381 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 382 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 383 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 384 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 385 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 386 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 387 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 388 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 389 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 390 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 391 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 392 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 393 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 394 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 395 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 396 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 397 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 398 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 399 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 400 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "\n",
      "SCHEDULE: Ansembler-1.s1.t1.e400\n",
      "-----------------------------------\n",
      "--------- Init new Expert ---------\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.538\n",
      "\t Val. Loss: 0.647 |  Val. PPL:   1.910\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.462 | Train PPL:   1.587\n",
      "\t Val. Loss: 0.448 |  Val. PPL:   1.564\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.414 | Train PPL:   1.513\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.580\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.488\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.521\n",
      "-Replay\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
      "\t Val. Loss: 0.454 |  Val. PPL:   1.575\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.503\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.463\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "-Replay\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.429 |  Val. PPL:   1.535\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.543\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.515\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "-Replay\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.488\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.403 |  Val. PPL:   1.496\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.462\n",
      "-Replay\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.483\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.424\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "-Replay\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.389\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.393\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.340\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.317\n",
      "-Replay\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "-Replay\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.232\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "-Replay\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "-Replay\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "-Replay\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "-Replay\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "-Replay\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "-Replay\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "-Replay\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "-Replay\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-Replay\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "-Replay\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.414\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.134\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.125\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "-Replay\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-Replay\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-Replay\n",
      "Epoch: 101 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 102 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 103 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 104 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 105 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 106 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 107 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 108 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 109 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 110 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "-Replay\n",
      "Epoch: 111 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 112 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 113 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 114 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 115 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "-Replay\n",
      "Epoch: 116 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 117 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 118 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 119 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 120 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "-Replay\n",
      "Epoch: 121 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 122 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 123 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 124 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 125 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "-Replay\n",
      "Epoch: 126 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 127 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 128 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 129 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 130 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 131 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 132 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 133 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 134 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 135 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 136 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 137 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 138 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 139 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 140 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 141 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 142 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 143 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 144 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 145 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 146 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 147 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 148 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 149 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 150 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 151 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 152 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 153 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 154 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 155 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 156 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 157 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 158 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 159 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 160 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 161 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 162 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 163 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 164 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 165 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 166 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 167 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 168 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 169 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 170 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 171 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 172 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 173 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 174 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 175 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 176 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 177 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 178 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 179 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 181 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 183 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 184 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 185 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 186 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 187 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 188 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 189 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 190 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 191 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 192 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 193 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 194 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 196 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 197 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 198 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 199 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 200 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 201 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 202 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 203 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 204 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 205 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 206 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 207 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 208 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 209 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 210 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 211 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 212 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 213 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 214 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 215 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 216 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 217 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 218 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 219 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 220 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 221 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 222 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 223 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 224 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 225 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 226 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 227 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 228 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 229 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 230 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 231 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 232 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 233 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 234 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 235 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 236 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 237 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 238 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 239 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 240 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "-Replay\n",
      "Epoch: 241 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 242 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 243 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 244 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 245 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 246 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 247 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 248 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 249 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 250 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 251 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 252 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 253 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 254 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 255 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 256 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 257 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 258 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 259 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 260 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 261 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 262 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 263 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 264 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 265 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 266 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 267 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 268 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 269 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 270 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 271 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 272 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 273 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 274 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 275 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 276 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 277 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 278 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 279 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 280 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 281 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 282 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 283 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 284 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 285 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 286 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 287 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 288 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 289 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 290 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 291 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 292 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 293 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 294 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 295 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 296 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 297 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 298 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 299 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 300 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 301 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 302 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 303 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 304 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 305 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 306 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 307 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 308 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 309 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 310 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 311 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 312 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 313 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 314 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 315 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 316 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 317 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 318 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 319 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 320 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 321 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 322 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 323 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 324 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 325 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 326 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 327 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 328 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 329 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 330 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 331 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 332 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 333 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 334 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 335 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 336 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 337 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 338 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 339 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 340 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 341 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 342 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 343 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 344 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 345 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 346 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 347 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 348 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 349 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 350 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 351 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 352 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 353 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 354 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 355 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 356 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 357 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 358 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 359 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 360 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 361 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 362 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 363 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 364 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 365 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 366 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 367 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 368 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 369 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 370 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 371 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 372 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 373 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 374 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 375 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 376 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 377 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 378 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 379 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 380 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 381 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 382 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 383 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 384 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 385 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 386 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 387 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 388 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 389 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 390 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 391 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 392 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 393 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 394 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 395 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 396 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 397 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 398 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 399 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 400 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "\n",
      "SCHEDULE: Ansembler-1.s2.t2.e400\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 101 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 102 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 103 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 104 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 105 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 106 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 107 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 108 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 109 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 110 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 111 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 112 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 113 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 114 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 115 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 116 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 117 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 118 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 119 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 120 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 121 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 122 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 123 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 124 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 125 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 126 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 127 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 128 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 129 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 130 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 131 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 132 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 133 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 134 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 135 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 136 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 137 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 138 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 139 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 140 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 141 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 143 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 144 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 145 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 146 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 147 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 148 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 150 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 151 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 152 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 154 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 156 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 157 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 158 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 159 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 160 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 161 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 163 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 164 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 166 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 168 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 169 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 170 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 171 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 173 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 174 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 175 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 176 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 177 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 180 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 181 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 182 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 184 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 186 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 187 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 191 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 196 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 197 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 199 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 200 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 201 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 202 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 203 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 204 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 205 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 206 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 207 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 208 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 209 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 210 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 211 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 212 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 213 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 214 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 215 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 216 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 217 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 218 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 219 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 220 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 221 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 222 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 223 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 224 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 225 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 226 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 227 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 228 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 229 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 230 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 231 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 232 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 233 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 234 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 235 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 236 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 237 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 238 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 239 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 240 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 241 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 242 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 243 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 244 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 245 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 246 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 247 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 248 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 249 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 250 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 251 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 252 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 253 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 254 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 255 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 256 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 257 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 258 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 259 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 260 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 261 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 262 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 263 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 264 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 265 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 266 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 267 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 268 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 269 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 270 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 271 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 272 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 273 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 274 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 275 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 276 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 277 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 278 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 279 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 280 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 281 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 282 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 283 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 284 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 285 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 286 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 287 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 288 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 289 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 290 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 291 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 292 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 293 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 294 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 295 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 296 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 297 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 298 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 299 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 300 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 301 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 302 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 303 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 304 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 305 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 306 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 307 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 308 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 309 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 310 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 311 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 312 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 313 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 314 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 315 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 316 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 317 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 318 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 319 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 320 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 321 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 322 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 323 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 324 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 325 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 326 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 327 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 328 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 329 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 330 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 331 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 332 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 333 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 334 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 335 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 336 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 337 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 338 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 339 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 340 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 341 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 342 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 343 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 344 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 345 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 346 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 347 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 348 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 349 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 350 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 351 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 352 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 353 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 354 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 355 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 356 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 357 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 358 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 359 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 360 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 361 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 362 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 363 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 364 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 365 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 366 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 367 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 368 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 369 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 370 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 371 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 372 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 373 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 374 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 375 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 376 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 377 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 378 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 379 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 380 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 381 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 382 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 383 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 384 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 385 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 386 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 387 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 388 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 389 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 390 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 391 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 392 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 393 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 394 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 395 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "-Replay\n",
      "Epoch: 396 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 397 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 398 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 399 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 400 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = 1\n",
    "hist_all_losses_H, hist_all_hitsss_H, models_H = experiment(\n",
    "    \"Ansembler-1\",\n",
    "    n_repetitions,\n",
    "    SCHEDULE,\n",
    "    init_ansembler,\n",
    "    repeat_ansembler,\n",
    "    STEP_SIZE_EVALUATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_H, SCHEDULE, STEP_SIZE_EVALUATION, datasets=(0,1), figsize=(12,7), save=True, path=PLOTSFOLDER+\"/showTyler4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_H[2].n_active_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0033, 0.9998]) - tensor([1, 5, 4, 6, 3, 4, 6, 2])\n",
      "tensor([0.0032, 0.9998]) - tensor([1, 5, 7, 4, 3, 4, 6, 2])\n",
      "tensor([0.0032, 0.9998]) - tensor([1, 5, 4, 3, 4, 6, 2])\n",
      "tensor([0.0029, 0.9998]) - tensor([1, 5, 4, 3, 2])\n",
      "tensor([0.0031, 0.9998]) - tensor([1, 5, 4, 6, 3, 2])\n",
      "tensor([0.0029, 0.9998]) - tensor([1, 5, 7, 4, 3, 2])\n",
      "tensor([0.0034, 0.9998]) - tensor([1, 5, 7, 4, 6, 3, 4, 6, 2])\n",
      "tensor([0.0031, 0.9998]) - tensor([1, 5, 7, 4, 3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "show_expert2(models_H[2], train_dls[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "agl_autoencoder.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "fa0c181e15994ab92b77e0a9eeb7815659805982e17e67ed50eb685ba3cdee26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
